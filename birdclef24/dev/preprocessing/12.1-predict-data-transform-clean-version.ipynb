{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = Path(\"../../data/predict\")\n",
    "pred_files = pred_dir.glob(\"*.ogg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regarding the data of a single audio, some audio information needs to be paid attention to, such as audio duration and number of channels.\n",
    "\n",
    "\n",
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio: torch.Tensor, segment_length:int):\n",
    "\n",
    "    '''\n",
    "    split raw audio tensor into multiple clips with 5 seconds long.\n",
    "\n",
    "    Parameters:\n",
    "        audio: the raw audio tensor\n",
    "        segment_length: the audio length of each 5 seconds\n",
    "\n",
    "    return:\n",
    "        parts: list includes all clips\n",
    "        end_time_list: the list of all clips' end time in seconds\n",
    "    '''\n",
    "\n",
    "    length_audio = audio.shape[1]\n",
    "    parts = []\n",
    "    # For example, if this is the first 5 seconds of audio, then the end time is 5. If it is 5-10 seconds, the end time is 10\n",
    "    end_time_list=[]\n",
    "    end_time=5\n",
    "    for i in range(0, length_audio, segment_length):\n",
    "        part = audio[0][i:i + segment_length]\n",
    "        # if len(part) == segment_length:  # Ensure the fragment lengths are consistent\n",
    "        parts.append(part)  #Store the raw bytes of audio data\n",
    "        end_time_list.append(end_time)\n",
    "        end_time+=5\n",
    "\n",
    "        \n",
    "\n",
    "    return parts,end_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clips_list=[]\n",
    "clip_names_list=[]\n",
    "\n",
    "for path in pred_files:\n",
    "    # read audio as tensor\n",
    "    audio,sr=read_audio(path=path)\n",
    "\n",
    "    # get audio corresponding informatino\n",
    "    duration_seconds,num_channels=audio_info(audio=audio,sample_rate=sr)\n",
    "\n",
    "    # split audio into multi clips with 5 seconds\n",
    "    audio_clips,end_time_list=split_audio(audio=audio,segment_length=5*sr)\n",
    "\n",
    "    # generate each label name for each clip\n",
    "    soundscape_id=path.stem\n",
    "    clip_name=[f'soundscape_{soundscape_id}_{end_time}' for end_time in end_time_list]\n",
    "\n",
    "    audio_clips_list.extend(audio_clips)\n",
    "    \n",
    "    clip_names_list.extend(clip_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset\n",
    "dataset = Dataset.from_dict({'audio_clip': audio_clips_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify melspec so that it can accept batch as a function and use the map function\n",
    "\n",
    "\n",
    "## Convert audio data into mel spectrogram\n",
    "\n",
    "\n",
    "def mel_transform(batch):\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    n_fft=int(0.04*32000)\n",
    "    hop_length=int(0.02*32000)\n",
    "\n",
    "    # Calculate Mel spectrum\n",
    "    n_mels = 40  # Number of Mel filters\n",
    "\n",
    "    # Setting up the Mel Spectrogram Converter\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=32000,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    audio_clip_batch=batch['audio_clip']\n",
    "\n",
    "    melspec_list=[]\n",
    "\n",
    "    for audio_clip in audio_clip_batch:\n",
    "        \n",
    "        audio_clip=torch.tensor(audio_clip).unsqueeze(0)\n",
    "\n",
    "        melspec=mel_transformer(audio_clip)\n",
    "\n",
    "        melspec_list.append(melspec)\n",
    "\n",
    "\n",
    "    return {'audio_mel':melspec_list}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:04<00:00, 23.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_mel=dataset.map(mel_transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mel_single=dataset_mel.remove_columns('audio_clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__()\n",
    "        self.dataset=dataset\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_melspec=self.dataset['audio_mel'][index]\n",
    "\n",
    "        audio_tensor=torch.tensor(audio_melspec)\n",
    "\n",
    "        return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.4445e-02, 3.2618e-02, 7.9513e-02,  ..., 3.6232e-02,\n",
      "           3.2288e-02, 2.0945e-02],\n",
      "          [1.7908e-03, 1.9852e-02, 7.0106e-02,  ..., 7.7466e-03,\n",
      "           2.4540e-02, 3.0045e-02],\n",
      "          [1.4791e-02, 8.8226e-03, 3.0246e-02,  ..., 2.5429e-02,\n",
      "           2.7441e-02, 1.4711e-02],\n",
      "          ...,\n",
      "          [6.2000e-02, 6.8676e-02, 6.3356e-02,  ..., 7.7837e-02,\n",
      "           5.4044e-02, 6.0893e-02],\n",
      "          [1.0289e-01, 1.1478e-01, 1.0321e-01,  ..., 1.4589e-01,\n",
      "           9.4384e-02, 8.3586e-02],\n",
      "          [1.9869e-01, 1.9208e-01, 1.3534e-01,  ..., 1.5763e-01,\n",
      "           1.7445e-01, 1.2904e-01]]],\n",
      "\n",
      "\n",
      "        [[[8.0196e-03, 7.4538e-03, 3.1931e-02,  ..., 4.7921e-03,\n",
      "           3.5222e-02, 2.3430e-02],\n",
      "          [1.7595e-02, 2.3124e-02, 3.1656e-02,  ..., 1.5919e-02,\n",
      "           1.3917e-02, 9.6076e-02],\n",
      "          [1.2621e-02, 1.5387e-02, 1.7095e-02,  ..., 2.6534e-02,\n",
      "           1.0663e-02, 3.0125e-02],\n",
      "          ...,\n",
      "          [3.7748e-02, 6.0891e-02, 4.9411e-02,  ..., 6.9629e-02,\n",
      "           7.9820e-02, 5.1984e-02],\n",
      "          [8.8558e-02, 9.6752e-02, 1.2687e-01,  ..., 1.1922e-01,\n",
      "           1.0741e-01, 8.7345e-02],\n",
      "          [1.3912e-01, 1.1777e-01, 1.4068e-01,  ..., 1.7273e-01,\n",
      "           1.4047e-01, 1.5812e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.3825e-03, 6.2584e-02, 1.2132e-02,  ..., 5.0036e-03,\n",
      "           2.4862e-02, 1.0201e-02],\n",
      "          [1.1816e-03, 2.4442e-02, 2.2606e-02,  ..., 1.6331e-02,\n",
      "           4.7988e-02, 6.1584e-03],\n",
      "          [9.0949e-03, 9.0599e-03, 1.6228e-02,  ..., 8.6010e-03,\n",
      "           1.1354e-02, 2.4329e-02],\n",
      "          ...,\n",
      "          [5.7139e-02, 6.6760e-02, 6.8528e-02,  ..., 4.1787e-02,\n",
      "           7.0843e-02, 7.5441e-02],\n",
      "          [1.2590e-01, 1.1377e-01, 1.2098e-01,  ..., 7.7461e-02,\n",
      "           1.1998e-01, 9.7624e-02],\n",
      "          [1.0781e-01, 1.2542e-01, 1.0378e-01,  ..., 1.7503e-01,\n",
      "           2.1258e-01, 6.2887e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[5.6025e-02, 2.1953e-03, 1.0775e-02,  ..., 6.7253e-03,\n",
      "           2.0259e-02, 2.0287e-02],\n",
      "          [3.2162e-02, 4.8866e-03, 1.0737e-02,  ..., 2.1167e-02,\n",
      "           2.4761e-02, 1.2213e-02],\n",
      "          [1.0630e-02, 1.3978e-02, 1.4108e-02,  ..., 8.1469e-03,\n",
      "           7.3264e-03, 1.2170e-02],\n",
      "          ...,\n",
      "          [6.7444e-02, 5.5771e-02, 4.5052e-02,  ..., 2.6177e+00,\n",
      "           4.6377e+00, 3.5938e+00],\n",
      "          [8.7104e-02, 1.2600e-01, 9.5551e-02,  ..., 7.2427e+00,\n",
      "           1.8175e+01, 7.5799e+00],\n",
      "          [6.7401e-02, 2.0743e-01, 1.0339e-01,  ..., 5.1783e+00,\n",
      "           9.7141e+00, 3.7579e+00]]],\n",
      "\n",
      "\n",
      "        [[[3.9652e-02, 9.8764e-03, 1.2836e-02,  ..., 4.7189e-02,\n",
      "           6.9018e-03, 1.4298e-02],\n",
      "          [8.0476e-03, 7.6441e-03, 1.1454e-02,  ..., 3.9547e-02,\n",
      "           2.1216e-02, 1.3941e-02],\n",
      "          [5.9511e-02, 1.7370e-02, 6.1559e-03,  ..., 7.6520e-03,\n",
      "           3.2395e-02, 5.9091e-03],\n",
      "          ...,\n",
      "          [2.7313e+00, 1.9646e+00, 1.1361e+00,  ..., 9.4450e-02,\n",
      "           8.1147e-02, 7.8207e-02],\n",
      "          [6.2229e+00, 4.4805e+00, 1.8104e+00,  ..., 1.3982e-01,\n",
      "           1.6304e-01, 1.5803e-01],\n",
      "          [3.0137e+00, 2.9068e+00, 1.3962e+00,  ..., 1.3606e-01,\n",
      "           4.8672e-01, 2.0406e-01]]],\n",
      "\n",
      "\n",
      "        [[[4.1049e-02, 1.0268e-02, 4.9606e-03,  ..., 6.0009e-02,\n",
      "           7.1700e-02, 6.2422e-03],\n",
      "          [1.1630e-02, 3.2747e-02, 1.6535e-02,  ..., 1.3656e-02,\n",
      "           1.6785e-02, 9.9857e-03],\n",
      "          [3.9663e-02, 1.2532e-02, 2.1030e-02,  ..., 1.4755e-02,\n",
      "           7.1135e-03, 2.3837e-02],\n",
      "          ...,\n",
      "          [3.8017e-02, 9.6452e-02, 9.3844e-02,  ..., 6.7476e-02,\n",
      "           4.5699e-02, 4.0181e-02],\n",
      "          [1.4263e-01, 1.7198e-01, 1.5265e-01,  ..., 1.4772e-01,\n",
      "           8.3753e-02, 9.1640e-02],\n",
      "          [2.2343e-01, 2.0228e-01, 1.6881e-01,  ..., 1.3529e-01,\n",
      "           1.0680e-01, 1.2914e-01]]]])\n",
      "torch.Size([32, 1, 40, 251])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "PD=PredDataset(dataset=dataset_mel)\n",
    "\n",
    "dataloader = DataLoader(dataset=PD, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "mel_specs = batch\n",
    "print(mel_specs)\n",
    "print(mel_specs.shape)\n",
    "print(type(mel_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
