{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is modified based on the 13/13.1/13.2 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='../../data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# Find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# Initialize training set and validation set\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# Random select a sample from each category to add to the validation set, and the rest to the training set\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# Divide the dataset by index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select 20,000 data from the training set\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Add these samples to the validation set\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# Remove these samples from the training set\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to interpolate missing values ​​for ratings in metadata csv file\n",
    "\n",
    "def rating_value_interplote(df:pd.DataFrame):\n",
    "    '''\n",
    "    interplote Nan values for rating col in metadata csv \n",
    "\n",
    "    parameters:\n",
    "        df: the df of the metadata csv file\n",
    "\n",
    "    rating col means the quality of the corresponding audio file\n",
    "        5 is high quality\n",
    "        1 is low quality\n",
    "        0 is without defined quality level\n",
    "    '''\n",
    "\n",
    "    if df['rating'].isna().sum()>0: # with missing value\n",
    "        df['rating'].fillna(0, inplace=True)\n",
    "\n",
    "    # For all places where the value is 0, a random value is given, choosing from the specified choices.\n",
    "    mask = df['rating'] == 0  # Create a boolean mask indicating which positions are 0\n",
    "\n",
    "    choices=np.arange(0.5,5.1,0.5).tolist() # [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "    random_values = np.random.choice(choices, size=mask.sum())  # Generate random numbers for these 0 values \n",
    "    df.loc[mask, 'rating'] = random_values  # Fill the generated random numbers back into the corresponding positions of the original DataFrame\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weight of each audio file by rating, which helps model training\n",
    "def audio_weight(df):\n",
    "    '''\n",
    "    calculate the weight corresponding to each audio file through the rating value\n",
    "\n",
    "    Because each audio has different quality level, we use weight to affect the inportance of each audio in models,\n",
    "    the lower the quality of the audio, the lower the weight\n",
    "    '''\n",
    "    # Through rating, we calculate the credibility of each audio and express it through weight. \n",
    "    # The purpose of this is to improve the model by increasing the weight of high-quality audio and reducing the weight of low-quality audio.\n",
    "    df[\"audio_weight\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is an unbalanced dataset, the amount of data in each category is very different\n",
    "# So I will calculate the weight of each category here\n",
    "# **(-0.5) The purpose is to reduce the relative influence of high-frequency categories and increase the influence of low-frequency categories, \n",
    "# so as to help the model better learn those uncommon categories\n",
    "# The purpose of calculating this is to build a WeightedRandomSampler, so that each time a batch is extracted using dataloader, it is more friendly to data of different categories.\n",
    "\n",
    "def sampling_weight(df)->torch.Tensor:\n",
    "    '''\n",
    "    calculate the sampling weight of each audio file\n",
    "\n",
    "    because this is imbalanced dataset\n",
    "    we hope the category with less data has large probability to be picked.\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # Map weights to each row of the original data\n",
    "    sample_weights_map = df['primary_label'].map(sample_weights)\n",
    "\n",
    "    # Convert pandas Series to NumPy array\n",
    "    sample_weights_np = sample_weights_map.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Convert a NumPy array to a PyTorch tensor using torch.from_numpy\n",
    "    sample_weights_tensor = torch.from_numpy(sample_weights_np)\n",
    "\n",
    "    return sample_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.sampler.WeightedRandomSampler object at 0x1144d6800>\n",
      "<torch.utils.data.sampler.WeightedRandomSampler object at 0x11441b9d0>\n"
     ]
    }
   ],
   "source": [
    "# Because I have divided the total metadata csv into two parts, next we need to calculate the sampler for train and val separately\n",
    "\n",
    "# train df\n",
    "sample_weights_tensor=sampling_weight(df=train_df)\n",
    "# Here we will build an argument sampler that will be used by the dataloader\n",
    "# It should be noted that the order of weights in the constructed sampler must be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "#Create a sampler based on the newly obtained weight list\n",
    "train_sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "print(train_sampler)\n",
    "\n",
    "# val df\n",
    "sample_weights_tensor=sampling_weight(df=val_df)\n",
    "# Here we will build an argument sampler that will be used by the dataloader\n",
    "# Note that the order of weights in the constructed sampler must be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "# Create a sampler based on the newly obtained weight list\n",
    "val_sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "print(val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CustomCompose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class CustomOneOf:\n",
    "    def __init__(self, transforms: list, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if np.random.rand() < self.p:\n",
    "            n_trns = len(self.transforms)\n",
    "            trns_idx = np.random.choice(n_trns)\n",
    "            trns = self.transforms[trns_idx]\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GaussianNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=40.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class VolumeControl(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode=\"uniform\"):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        assert mode in [\n",
    "            \"uniform\",\n",
    "            \"fade\",\n",
    "            \"fade\",\n",
    "            \"cosine\",\n",
    "            \"sine\",\n",
    "        ], \"`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'\"\n",
    "\n",
    "        self.db_limit = db_limit\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.db_limit, self.db_limit)\n",
    "        if self.mode == \"uniform\":\n",
    "            db_translated = 10 ** (db / 20)\n",
    "        elif self.mode == \"fade\":\n",
    "            lin = np.arange(len(y))[::-1] / (len(y) - 1)\n",
    "            db_translated = 10 ** (db * lin / 20)\n",
    "        elif self.mode == \"cosine\":\n",
    "            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * cosine / 20)\n",
    "        else:\n",
    "            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * sine / 20)\n",
    "        augmented = y * db_translated\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class NoiseInjection(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_level = (0.0, max_noise_level)\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_level = np.random.uniform(*self.noise_level)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_level).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class GaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_rate = max_rate\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "def _db2float(db: float, amplitude=True):\n",
    "    if amplitude:\n",
    "        return 10 ** (db / 20)\n",
    "    else:\n",
    "        return 10 ** (db / 10)\n",
    "\n",
    "\n",
    "def volume_down(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for decreasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to decrease\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with decreased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(-db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "def volume_up(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for increasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to increase\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with increased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "class RandomVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        if db >= 0:\n",
    "            return volume_up(y, db)\n",
    "        else:\n",
    "            return volume_down(y, db)\n",
    "\n",
    "\n",
    "class CosineVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "        dbs = _db2float(cosine * db)\n",
    "        return y * dbs\n",
    "\n",
    "\n",
    "class AddGaussianNoise(AudioTransform):\n",
    "    \"\"\"Add gaussian noise to the samples\"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self, always_apply=False, min_amplitude=0.001, max_amplitude=0.015, p=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_amplitude: Minimum noise amplification factor\n",
    "        :param max_amplitude: Maximum noise amplification factor\n",
    "        :param p:\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        assert min_amplitude > 0.0\n",
    "        assert max_amplitude > 0.0\n",
    "        assert max_amplitude >= min_amplitude\n",
    "        self.min_amplitude = min_amplitude\n",
    "        self.max_amplitude = max_amplitude\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        amplitude = np.random.uniform(self.min_amplitude, self.max_amplitude)\n",
    "        noise = np.random.randn(*samples.shape).astype(np.float32)\n",
    "        samples = samples + amplitude * noise\n",
    "        return samples\n",
    "\n",
    "\n",
    "class AddGaussianSNR(AudioTransform):\n",
    "    \"\"\"\n",
    "    Add gaussian noise to the input. A random Signal to Noise Ratio (SNR) will be picked\n",
    "    uniformly in the decibel scale. This aligns with human hearing, which is more\n",
    "    logarithmic than linear.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        always_apply=False,\n",
    "        min_snr_in_db: float = 5.0,\n",
    "        max_snr_in_db: float = 40.0,\n",
    "        p: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_snr_in_db: Minimum signal-to-noise ratio in dB. A lower number means more noise.\n",
    "        :param max_snr_in_db: Maximum signal-to-noise ratio in dB. A greater number means less noise.\n",
    "        :param p: The probability of applying this transform\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        self.min_snr_in_db = min_snr_in_db\n",
    "        self.max_snr_in_db = max_snr_in_db\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        snr = np.random.uniform(self.min_snr_in_db, self.max_snr_in_db)\n",
    "\n",
    "        clean_rms = np.sqrt(np.mean(np.square(samples)))\n",
    "\n",
    "        a = float(snr) / 20\n",
    "        noise_rms = clean_rms / (10**a)\n",
    "\n",
    "        noise = np.random.normal(0.0, noise_rms, size=samples.shape).astype(np.float32)\n",
    "        return samples + noise\n",
    "\n",
    "\n",
    "class Normalize(AudioTransform):\n",
    "    \"\"\"\n",
    "    Apply a constant amount of gain, so that highest signal level present in the sound becomes\n",
    "    0 dBFS, i.e. the loudest level allowed if all samples must be between -1 and 1. Also known\n",
    "    as peak normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(self, always_apply=False, apply_to: str = \"all\", p: float = 0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        assert apply_to in (\"all\", \"only_too_loud_sounds\")\n",
    "        self.apply_to = apply_to\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        max_amplitude = np.amax(np.abs(samples))\n",
    "        if self.apply_to == \"only_too_loud_sounds\" and max_amplitude < 1.0:\n",
    "            return samples\n",
    "\n",
    "        if max_amplitude > 0:\n",
    "            return samples / max_amplitude\n",
    "        else:\n",
    "            return samples\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order is very important and needs to be matched one by one in the subsequent training, I will save these types here\n",
    "# Save as .npy file\n",
    "np.save(\"./temp_files/13-2-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malpar1', 'litgre1', 'houspa', 'indrob1', 'comtai1', 'grynig2',\n",
       "       'rufwoo2', 'yebbul3', 'indpit1', 'gyhcaf1', 'ruftre2', 'wynlau1',\n",
       "       'inpher1', 'comkin1', 'comior1', 'tibfly3', 'pomgrp2', 'oripip1',\n",
       "       'indtit1', 'nutman', 'junmyn1', 'rutfly6', 'goflea1', 'litegr',\n",
       "       'lesyel1', 'lewduc1', 'inbrob1', 'crfbar1', 'scamin3', 'shikra1',\n",
       "       'gryfra', 'commoo3', 'grewar3', 'brodro1', 'rocpig', 'categr',\n",
       "       'ingori1', 'plhpar1', 'sbeowl1', 'bwfshr1', 'junowl1', 'orihob2',\n",
       "       'greegr', 'barswa', 'paisto1', 'moipig1', 'plapri1', 'forwag1',\n",
       "       'maghor2', 'brasta1', 'lirplo', 'grecou1', 'kenplo1', 'bkcbul1',\n",
       "       'grbeat1', 'junbab2', 'comsan', 'whbtre1', 'brnhao1', 'brcful1',\n",
       "       'whcbar1', 'hoopoe', 'plaflo1', 'maltro1', 'piekin1', 'brnshr',\n",
       "       'whiter2', 'brfowl1', 'pursun4', 'grehor1', 'pursun3', 'purswa3',\n",
       "       'yebbab1', 'lblwar1', 'malwoo1', 'laudov1', 'grenig1', 'tilwar1',\n",
       "       'heswoo1', 'compea', 'putbab1', 'smamin1', 'rorpar', 'graher1',\n",
       "       'ashpri1', 'piebus1', 'grnwar1', 'eurbla2', 'asikoe2', 'whbwat1',\n",
       "       'sqtbul1', 'brwowl1', 'bncwoo3', 'ashwoo2', 'pabflo1', 'eaywag1',\n",
       "       'ashdro1', 'rerswa1', 'emedov2', 'houcro1', 'btbeat1', 'gargan',\n",
       "       'insowl1', 'bcnher', 'placuc3', 'blaeag1', 'barfly1', 'insbab1',\n",
       "       'rewbul', 'grefla1', 'spoowl1', 'marsan', 'whbsho3', 'kerlau2',\n",
       "       'asiope1', 'redspu1', 'chbeat1', 'blhori1', 'integr', 'zitcis1',\n",
       "       'blakit1', 'grywag', 'crseag1', 'labcro1', 'litspi1', 'spepic1',\n",
       "       'comgre', 'vehpar1', 'gloibi', 'whbwag1', 'jerbus2', 'stbkin1',\n",
       "       'copbar1', 'whtkin2', 'blrwar1', 'rewlap1', 'woosan', 'vefnut1',\n",
       "       'mawthr1', 'isbduc1', 'grtdro1', 'bladro1', 'thbwar1', 'aspswi1',\n",
       "       'aspfly1', 'revbul', 'eucdov', 'asbfly', 'whrmun', 'nilfly2',\n",
       "       'wbbfly1', 'crbsun2', 'commyn', 'purher1', 'bkwsti', 'wemhar1',\n",
       "       'bkskit1', 'litswi1', 'sohmyn1', 'dafbab1', 'eurcoo', 'whbbul2',\n",
       "       'darter2', 'rufbab3', 'whbwoo2', 'sttwoo1', 'gybpri1', 'niwpig1',\n",
       "       'brakit1', 'grnsan', 'grejun2', 'cohcuc1', 'comros', 'brwjac1',\n",
       "       'comfla1', 'spodov', 'blnmon1', 'lobsun2', 'rossta2', 'bkrfla1',\n",
       "       'indrol2', 'cregos1'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_cates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malpar1' 'litgre1' 'houspa' 'indrob1' 'comtai1' 'grynig2' 'rufwoo2'\n",
      " 'yebbul3' 'indpit1' 'gyhcaf1' 'ruftre2' 'wynlau1' 'inpher1' 'comkin1'\n",
      " 'comior1' 'tibfly3' 'pomgrp2' 'oripip1' 'indtit1' 'nutman' 'junmyn1'\n",
      " 'rutfly6' 'goflea1' 'litegr' 'lesyel1' 'lewduc1' 'inbrob1' 'crfbar1'\n",
      " 'scamin3' 'shikra1' 'gryfra' 'commoo3' 'grewar3' 'brodro1' 'rocpig'\n",
      " 'categr' 'ingori1' 'plhpar1' 'sbeowl1' 'bwfshr1' 'junowl1' 'orihob2'\n",
      " 'greegr' 'barswa' 'paisto1' 'moipig1' 'plapri1' 'forwag1' 'maghor2'\n",
      " 'brasta1' 'lirplo' 'grecou1' 'kenplo1' 'bkcbul1' 'grbeat1' 'junbab2'\n",
      " 'comsan' 'whbtre1' 'brnhao1' 'brcful1' 'whcbar1' 'hoopoe' 'plaflo1'\n",
      " 'maltro1' 'piekin1' 'brnshr' 'whiter2' 'brfowl1' 'pursun4' 'grehor1'\n",
      " 'pursun3' 'purswa3' 'yebbab1' 'lblwar1' 'malwoo1' 'laudov1' 'grenig1'\n",
      " 'tilwar1' 'heswoo1' 'compea' 'putbab1' 'smamin1' 'rorpar' 'graher1'\n",
      " 'ashpri1' 'piebus1' 'grnwar1' 'eurbla2' 'asikoe2' 'whbwat1' 'sqtbul1'\n",
      " 'brwowl1' 'bncwoo3' 'ashwoo2' 'pabflo1' 'eaywag1' 'ashdro1' 'rerswa1'\n",
      " 'emedov2' 'houcro1' 'btbeat1' 'gargan' 'insowl1' 'bcnher' 'placuc3'\n",
      " 'blaeag1' 'barfly1' 'insbab1' 'rewbul' 'grefla1' 'spoowl1' 'marsan'\n",
      " 'whbsho3' 'kerlau2' 'asiope1' 'redspu1' 'chbeat1' 'blhori1' 'integr'\n",
      " 'zitcis1' 'blakit1' 'grywag' 'crseag1' 'labcro1' 'litspi1' 'spepic1'\n",
      " 'comgre' 'vehpar1' 'gloibi' 'whbwag1' 'jerbus2' 'stbkin1' 'copbar1'\n",
      " 'whtkin2' 'blrwar1' 'rewlap1' 'woosan' 'vefnut1' 'mawthr1' 'isbduc1'\n",
      " 'grtdro1' 'bladro1' 'thbwar1' 'aspswi1' 'aspfly1' 'revbul' 'eucdov'\n",
      " 'asbfly' 'whrmun' 'nilfly2' 'wbbfly1' 'crbsun2' 'commyn' 'purher1'\n",
      " 'bkwsti' 'wemhar1' 'bkskit1' 'litswi1' 'sohmyn1' 'dafbab1' 'eurcoo'\n",
      " 'whbbul2' 'darter2' 'rufbab3' 'whbwoo2' 'sttwoo1' 'gybpri1' 'niwpig1'\n",
      " 'brakit1' 'grnsan' 'grejun2' 'cohcuc1' 'comros' 'brwjac1' 'comfla1'\n",
      " 'spodov' 'blnmon1' 'lobsun2' 'rossta2' 'bkrfla1' 'indrol2' 'cregos1']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# load .npy file\n",
    "loaded_array = np.load(\"./temp_files/13-2-bird-cates.npy\",allow_pickle=True)\n",
    "\n",
    "# Print the array contents to verify\n",
    "print(loaded_array)\n",
    "print(type(loaded_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the index of the target value in the array\n",
    "index = np.where(loaded_array == 'gyhcaf1')[0][0]\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.zeros(len(loaded_array))\n",
    "\n",
    "a[9]=1\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label      weight\n",
      "0    blrwar1    3.300550\n",
      "1     houspa    4.865550\n",
      "2    grewar3    5.655309\n",
      "3     commyn    5.852887\n",
      "4     hoopoe    6.044591\n",
      "..       ...         ...\n",
      "177  blaeag1   90.727614\n",
      "178  darter2   99.387122\n",
      "179  asiope1  114.762363\n",
      "180   integr  118.790331\n",
      "181  niwpig1  134.013568\n",
      "\n",
      "[182 rows x 2 columns]\n",
      "       label     weight\n",
      "0    malpar1  39.286209\n",
      "1    litgre1   8.817445\n",
      "2     houspa   4.865550\n",
      "3    indrob1  26.328292\n",
      "4    comtai1  10.093835\n",
      "..       ...        ...\n",
      "177  lobsun2  34.497774\n",
      "178  rossta2  18.799193\n",
      "179  bkrfla1  44.898525\n",
      "180  indrol2  31.667370\n",
      "181  cregos1  19.585808\n",
      "\n",
      "[182 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sample_weights = (train_df['primary_label'].value_counts() / train_df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "# Convert sample_weights to a DataFrame for easier processing\n",
    "sample_weights_df = sample_weights.reset_index()\n",
    "sample_weights_df.columns = ['label', 'weight']\n",
    "\n",
    "# sample_weights_df\n",
    "\n",
    "# Convert loaded_array to Categorical type and sort sample_weights_df according to this new order\n",
    "sample_weights_df['label'] = pd.Categorical(sample_weights_df['label'], categories=loaded_array, ordered=True)\n",
    "\n",
    "print(sample_weights_df)\n",
    "\n",
    "# # Sort DataFrame according to new category order\n",
    "sample_weights_df = sample_weights_df.sort_values('label').reset_index(drop=True)\n",
    "\n",
    "# # Now the weights of sample_weights_df are in the order of loaded_array\n",
    "print(sample_weights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 39.2862,   8.8174,   4.8656,  26.3283,  10.0938,  43.3761,  28.6310,\n",
       "         29.3077,  21.5855,  10.6493,  15.4279,  53.1247,  81.1492,   8.0219,\n",
       "         11.0841,  16.3281,  83.9974,  23.0448,  47.9287,  16.3391,  49.3859,\n",
       "         63.4961,  20.9526,   9.7787,  30.3835,  32.5031,  17.5283,  49.3859,\n",
       "         64.1541,  18.2369,  28.9327,   9.0166,   5.6553,  13.4442,  11.8117,\n",
       "         10.0731,  21.7399,  32.8564,  29.8311,  30.9679,  22.7710,  27.4596,\n",
       "         12.8201,   7.1209,  57.8654,  35.5863,  13.7364,  22.7710,  27.3037,\n",
       "         52.3816,   7.6656,  13.8024,  13.2047,  33.6954,  26.9007,  17.2619,\n",
       "          8.6473,  34.0895,  13.9238,  16.4959,  26.6577,   6.0446,  52.3816,\n",
       "         50.6524,  17.6107,  13.5752,  10.1516,  32.1609,  15.8639,  27.5122,\n",
       "         22.5938,  31.2730,  34.6022,  18.0704,  61.0530,  15.7937,  28.0549,\n",
       "         34.7075,  36.1707,  25.7909,  10.0964,  48.4959,   8.0760,   7.1091,\n",
       "         17.5968,  17.2749,  12.7409,  37.9739,  10.0602,  11.1997,  34.0895,\n",
       "         15.3632,  68.5836,  25.6616,  26.1908,   7.9357,  14.9917,  11.5457,\n",
       "         20.6787,  15.3632,  24.0343,  16.1975,  27.7795,   7.5130,  27.3037,\n",
       "         90.7276,  18.6006,  19.9776,   9.7787,  25.3262,  32.1609,  21.9242,\n",
       "         18.4082,  48.4959, 114.7624,  55.9983,  48.2098,  14.1837, 118.7903,\n",
       "          7.9778,  10.4560,   7.8255,  13.7759,  10.7485,  16.1867,  25.8782,\n",
       "          8.2622,  36.9115,  20.5898,  26.1908,  38.8338,  14.8737,  15.8740,\n",
       "         13.5186,   3.3006,  15.7047,   8.3316,  31.6674,  22.6819,  51.3233,\n",
       "          9.3578,  13.1239,  10.9124,  36.5354,  41.0915,  15.1740,   6.6667,\n",
       "         15.2813,  22.0047,  62.2386,  75.1296,  36.1707,   5.8529,  11.6967,\n",
       "          6.8308,   9.7270,   9.6306,  14.7666,  20.1203,  43.3761,   8.4604,\n",
       "         28.2811,  99.3871,  40.7447,  19.1626,  39.1336,  15.7539, 134.0136,\n",
       "         30.0347,   8.7353,  20.9994,  16.5530,   6.2196,  50.6524,  35.3604,\n",
       "         12.8095,  14.2345,  34.4978,  18.7992,  44.8985,  31.6674,  19.5858],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_weight=torch.tensor(sample_weights_df['weight'].values)\n",
    "cate_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        bird_category_dir: str,\n",
    "        audio_dir: str = \"../../data/train_audio\",\n",
    "        train: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            df: the dataframe of metadata (train/val)\n",
    "            bird_category_dir: the directory of the bird category array file (npy)\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            train: If the Datset for train set or val set\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # if the Dataset for training or validation\n",
    "        self.train = train\n",
    "        self.raw_df = df\n",
    "\n",
    "        # inperplote nan or 0 value of rating col\n",
    "        self.raw_df = rating_value_interplote(df=self.raw_df)\n",
    "        # Calculate the weight of each audio file by rating\n",
    "        self.raw_df = audio_weight(self.raw_df)\n",
    "\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "        self.bird_cate_array = np.load(bird_category_dir, allow_pickle=True)\n",
    "\n",
    "        self.np_audio_transforms = (\n",
    "            self.setup_transforms()\n",
    "        )  # initialize data augmentation func\n",
    "\n",
    "    def setup_transforms(self):\n",
    "\n",
    "        return CustomCompose(\n",
    "            [\n",
    "                CustomOneOf(\n",
    "                    [\n",
    "                        NoiseInjection(p=1, max_noise_level=0.04),\n",
    "                        GaussianNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        PinkNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        AddGaussianNoise(\n",
    "                            min_amplitude=0.0001, max_amplitude=0.03, p=0.5\n",
    "                        ),\n",
    "                        AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=15, p=0.5),\n",
    "                    ],\n",
    "                    p=0.3, \n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_audio_path(self, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            file_name: in format category_type/XC-ID.ogg (asbfly/XC134896.ogg)\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        \"\"\"\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir, file_name)\n",
    "\n",
    "    def target_clip(\n",
    "        self, index: int, audio: torch.Tensor, sample_rate: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip\n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time = self.raw_df[\"clip_start_time\"].iloc[index]\n",
    "        duration_seconds = self.raw_df[\"duration\"].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time <= duration_seconds:\n",
    "            clip_start_point = clip_start_time * sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point + segment_duration <= total_samples:\n",
    "                clip = audio[:, clip_start_point : clip_start_point + segment_duration]\n",
    "\n",
    "            # Not long enough, a mask is needed\n",
    "            else:\n",
    "                padding_length = clip_start_point + segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "                # concat the last segment of raw audio with silence\n",
    "                clip = torch.cat((audio[:, clip_start_point:], silence), dim=1)\n",
    "\n",
    "                del silence, padding_length\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The clip start time is out of raw audio length\")\n",
    "\n",
    "        del clip_start_time, segment_duration, total_samples\n",
    "\n",
    "        return clip\n",
    "\n",
    "    def random_audio_augmentation(self, audio: torch.Tensor):\n",
    "        \"\"\"\n",
    "        audio (torch.Tensor): A 2D tensor of audio samples with shape (1, N), where N is the number of samples.\n",
    "        \"\"\"\n",
    "\n",
    "        audio_aug = self.np_audio_transforms(audio[0].numpy())\n",
    "\n",
    "        # tranfer the array to 2D tensor and keep the num channel is 1\n",
    "        # this step is to keep the input and output shape adn type are the same\n",
    "\n",
    "        audio_aug_tensor = torch.from_numpy(audio_aug)\n",
    "        audio_aug_tensor = audio_aug_tensor.unsqueeze(0).to(dtype=torch.float16)\n",
    "\n",
    "        del audio_aug\n",
    "\n",
    "        return audio_aug_tensor\n",
    "\n",
    "    def audio_label_tensor_generator(self, true_label: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a tensor containing all categories based on the given real audio label\n",
    "\n",
    "        Parameters:\n",
    "            true lable: a label string\n",
    "\n",
    "        Return:\n",
    "            If have 10 class, and give a true lable\n",
    "            the return should be tensor([0,1,0,0,0,0,0,0,0,0])\n",
    "        \"\"\"\n",
    "        # Find the index of the target value in the array\n",
    "        idx = np.where(self.bird_cate_array == true_label)[0][0]\n",
    "\n",
    "        # Create a tensor of all zeros with length equal to the length of the array\n",
    "        audio_label_tensor = torch.zeros(len(self.bird_cate_array), dtype=torch.float16)\n",
    "\n",
    "        # Set the value at the corresponding index position to 1\n",
    "        audio_label_tensor[idx] = 1\n",
    "\n",
    "        return audio_label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.raw_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.raw_df.iloc[index]\n",
    "\n",
    "        audio_label = row[\"primary_label\"]\n",
    "        audio_weight = row[\"audio_weight\"]\n",
    "\n",
    "        # Get the path to a single audio file\n",
    "        single_audio_dir = self.get_audio_path(row[\"filename\"])\n",
    "\n",
    "        # Read audio array according to path\n",
    "        audio, sr = read_audio(single_audio_dir)\n",
    "\n",
    "        # augmentation\n",
    "        # only used for train df\n",
    "        if self.train:\n",
    "            audio_augmentation = self.random_audio_augmentation(audio=audio)\n",
    "            # Get the audio clip corresponding to index\n",
    "            clip = self.target_clip(index, audio=audio_augmentation, sample_rate=sr)\n",
    "            del audio_augmentation\n",
    "        else:\n",
    "            clip = self.target_clip(index, audio=audio, sample_rate=sr)\n",
    "\n",
    "        # change audio label to one-hot tensor\n",
    "        audio_label_tensor = self.audio_label_tensor_generator(true_label=audio_label)\n",
    "\n",
    "        audio_label_tensor = torch.tensor(audio_label_tensor, dtype=torch.float16)\n",
    "        clip = torch.tensor(clip, dtype=torch.float16)\n",
    "        audio_weight = torch.tensor(audio_weight, dtype=torch.float16)\n",
    "\n",
    "        del audio\n",
    "\n",
    "        return audio_label_tensor, clip, audio_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD=BirdclefDataset(df=train_df,bird_category_dir=\"./temp_files/13-2-bird-cates.npy\",train=True)\n",
    "train_dataloader = DataLoader(dataset=BD, batch_size=32, sampler=train_sampler, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 182])\n",
      "tensor([[[-3.6499e-02, -2.0172e-02, -4.3945e-03,  ..., -2.1088e-02,\n",
      "          -1.9821e-02, -1.3184e-02]],\n",
      "\n",
      "        [[-1.1091e-03, -8.6746e-03, -1.6012e-03,  ...,  6.3591e-03,\n",
      "          -1.2703e-02, -3.5583e-02]],\n",
      "\n",
      "        [[ 6.0499e-05,  4.7207e-05, -6.4373e-06,  ..., -9.2745e-04,\n",
      "          -6.9523e-04, -2.4796e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.1352e-05, -8.7023e-06,  5.6028e-06,  ..., -8.1682e-04,\n",
      "          -2.3212e-03, -3.0594e-03]],\n",
      "\n",
      "        [[-2.5249e-04,  8.3876e-04,  1.0834e-03,  ...,  2.8312e-05,\n",
      "           1.6108e-03, -2.1839e-03]],\n",
      "\n",
      "        [[ 5.1856e-06,  2.9802e-06, -3.6359e-06,  ...,  1.6174e-02,\n",
      "           5.2147e-03, -5.3453e-04]]], dtype=torch.float16)\n",
      "torch.Size([32, 1, 160000])\n",
      "tensor([0.5000, 0.5000, 0.6001, 1.0000, 0.6001, 0.7002, 0.7998, 1.0000, 0.7998,\n",
      "        1.0000, 1.0000, 0.8999, 0.7998, 1.0000, 1.0000, 0.8999, 0.1000, 0.8999,\n",
      "        0.5000, 0.6001, 0.8999, 0.7998, 1.0000, 0.8999, 0.7002, 0.7002, 0.7998,\n",
      "        0.7998, 1.0000, 0.7998, 1.0000, 0.8999], dtype=torch.float16)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "audio_label,clip,audio_weights = batch\n",
    "print(audio_label)\n",
    "print(type(audio_label))\n",
    "print(audio_label.shape)\n",
    "print(clip)\n",
    "print(clip.shape)\n",
    "print(audio_weights)\n",
    "print(audio_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup_prob, mixup_double):\n",
    "        super(Mixup, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.mixup_double = mixup_double\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0] # Generate a random number p and compare it with mixup_prob to decide whether to mix.\n",
    "        if p < self.mixup_prob:\n",
    "            bs = X.shape[0] # batch size\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs) # Generate a random permutation for randomly selecting samples from the current batch for mixing.\n",
    "\n",
    "            p1 = torch.rand((1,))[0] # If the random number p1 (determines whether to perform double mixing) is less than mixup_double, perform a single mix. Otherwise, perform double mixing:\n",
    "            if p1 < self.mixup_double:\n",
    "                X = X + X[perm]\n",
    "                Y = Y + Y[perm]\n",
    "                Y = torch.clamp(Y, 0, 1) # Use torch.clamp to clamp the values ​​of Y between 0 and 1 (suitable for probabilistic or binary labels).\n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = 0.5 * weight + 0.5 * weight[perm]\n",
    "                    return X, Y, weight\n",
    "            else:\n",
    "                perm2 = torch.randperm(bs)\n",
    "                X = X + X[perm] + X[perm2]\n",
    "                Y = Y + Y[perm] + Y[perm2]\n",
    "                Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = (\n",
    "                        1 / 3 * weight + 1 / 3 * weight[perm] + 1 / 3 * weight[perm2]\n",
    "                    )\n",
    "                    return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mixup function here originally performs data conversion in lightningModule. In order to improve the training efficiency of the model, \n",
    "# I decided to extract this part from loggingModule and replace it with torch.utils.data.Dataset.collate_fn\n",
    "mixup_layer = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "def mixup_collate(batch,mixup_layer):\n",
    "    '''\n",
    "    When creating data batches, define how each batch should be stacked\n",
    "    parameters:\n",
    "        batch: is a list of tuples with (labels, clip, weights)\n",
    "    '''\n",
    "    # Unpack each individual sample in the batch\n",
    "    labels, audios, weights = zip(*batch)\n",
    "    \n",
    "    # Stack the data into new batches\n",
    "    labels = torch.stack(labels)\n",
    "    audios = torch.stack(audios)\n",
    "\n",
    "    weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "    return mixup_layer(X=audios,Y=labels,weight=weights)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "BD=BirdclefDataset(df=train_df,bird_category_dir=\"./temp_files/13-2-bird-cates.npy\",train=True)\n",
    "train_dataloader = DataLoader(dataset=BD, batch_size=32, sampler=train_sampler, pin_memory=True,collate_fn=lambda batch: mixup_collate(batch, mixup_layer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 182])\n",
      "torch.Size([32, 1, 160000])\n",
      "tensor([0.7998, 0.8999, 0.6001, 0.7998, 0.6001, 0.7998, 0.5000, 0.8999, 0.2000,\n",
      "        0.7998, 1.0000, 0.6001, 1.0000, 0.7002, 1.0000, 1.0000, 0.8999, 1.0000,\n",
      "        0.7998, 0.5000, 0.8999, 0.7998, 0.7002, 0.7002, 0.7998, 0.7998, 0.7002,\n",
      "        0.3999, 0.7002, 0.7998, 0.5000, 0.6001], dtype=torch.float16)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "clip,audio_label,audio_weights = batch\n",
    "# print(audio_label)\n",
    "print(type(audio_label))\n",
    "print(audio_label.shape)\n",
    "# print(clip)\n",
    "print(clip.shape)\n",
    "print(audio_weights)\n",
    "print(audio_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=torch.float16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.01,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    n_fft = int(window_size * sample_rate)  \n",
    "\n",
    "    hop_length = int(hop_size * sample_rate)  \n",
    "\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\") -> torch.Tensor:\n",
    "    \"\"\"Compute delta coefficients of a tensor, usually a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "        win_length (int, optional): The window length used for computing delta (Default: 5)\n",
    "        mode (str, optional): Mode parameter passed to padding (Default: \"replicate\")\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "    \"\"\"\n",
    "    device = specgram.device  \n",
    "    dtype = specgram.dtype\n",
    "\n",
    "    shape = specgram.size()\n",
    "    specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "    assert win_length >= 3\n",
    "    n = (win_length - 1) // 2\n",
    "    denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "    kernel = torch.arange(-n, n + 1, 1, dtype=dtype,device=device).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "    output = (\n",
    "        torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "    )\n",
    "\n",
    "    # unpack batch\n",
    "    output = output.reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def make_delta(input_tensor: torch.Tensor):\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    input_tensor = compute_deltas(input_tensor)\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "def image_delta(x):\n",
    "    delta_1 = make_delta(x)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([x, delta_1, delta_2], dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup2(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup2_prob):\n",
    "        super(Mixup2, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup2_prob = mixup2_prob\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0]\n",
    "        if p < self.mixup2_prob:\n",
    "            bs = X.shape[0]\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs)\n",
    "            coeffs = self.beta_distribution.rsample(torch.Size((bs,)))\n",
    "\n",
    "            if n_dims == 2:\n",
    "                X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n",
    "            elif n_dims == 3:\n",
    "                X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n",
    "            else:\n",
    "                X = (\n",
    "                    coeffs.view(-1, 1, 1, 1) * X\n",
    "                    + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n",
    "                )\n",
    "            Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n",
    "            # Y = Y + Y[perm]\n",
    "            # Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n",
    "                return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collate_fn() created above indicates that the mixup has been successfully extracted from the lightningModule and partially integrated with the dataloader\n",
    "# Now, you need to add the remaining data conversion steps to collate_fn()\n",
    "# This step is for the train dataloader\n",
    "\n",
    "mixup_layer = Mixup(mix_beta=5, mixup_prob=0.7, mixup_double=0.5)\n",
    "mixup2_layer = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "audio_transforms = Compose(\n",
    "    [\n",
    "        # AddColoredNoise(p=0.5),\n",
    "        PitchShift(\n",
    "            min_transpose_semitones=-4,\n",
    "            max_transpose_semitones=4,\n",
    "            sample_rate=32000,\n",
    "            p=0.4,\n",
    "        ),\n",
    "        Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def trainloader_collate(batch, mixup_layer):\n",
    "    \"\"\"\n",
    "    When creating data batches, define how each batch should be stacked\n",
    "    parameters:\n",
    "        batch: is a list of tuples with (labels, clip, weights)\n",
    "    \"\"\"\n",
    "    # Unpack each individual sample in the batch\n",
    "    labels, clips, weights = zip(*batch)\n",
    "\n",
    "    # Stack the data into new batches\n",
    "    labels = torch.stack(labels).float()\n",
    "    clips = torch.stack(clips).float()\n",
    "\n",
    "    weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "    clips, labels, weights = mixup_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # Use Compose to combine multiple audio transformation operations. \n",
    "    # These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "    clips = audio_transforms(clips, sample_rate=32000)\n",
    "\n",
    "    # Convert audio data into mel spectrogram\n",
    "    clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "    ##Convert the amplitude of Mel Spectrogram to decibel (Decibel, dB)\n",
    "    clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "    # normalization\n",
    "    clips = (clips + 80) / 80\n",
    "\n",
    "    # Randomly masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "    clips = torchaudio.transforms.TimeMasking(\n",
    "        time_mask_param=20, iid_masks=True, p=0.3\n",
    "    )(clips)\n",
    "\n",
    "    # Calculate the first and second order differences of audio or other time series data, \n",
    "    # usually called delta and delta-delta (also called acceleration) features.\n",
    "    clips = image_delta(clips)\n",
    "\n",
    "    # Mixing Audio\n",
    "    clips, labels,weights = mixup2_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD=BirdclefDataset(df=train_df,bird_category_dir=\"./temp_files/13-2-bird-cates.npy\",train=True)\n",
    "train_dataloader = DataLoader(dataset=BD, batch_size=32, sampler=train_sampler, pin_memory=True,collate_fn=lambda batch: trainloader_collate(batch, mixup_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 182])\n",
      "torch.Size([32, 3, 40, 501])\n",
      "tensor([0.7002, 0.8501, 1.0000, 0.8999, 0.7002, 0.5498, 0.7500, 0.4502, 0.8999,\n",
      "        0.9502, 0.7002, 0.8496, 1.0000, 0.7500, 0.7002, 0.8496, 0.7500, 0.7998,\n",
      "        0.7998, 0.3501, 0.7500, 0.5498, 0.7998, 0.5498, 0.8999, 0.8501, 0.7998,\n",
      "        0.7998, 0.5996, 0.7500, 0.8496, 0.6001], dtype=torch.float16)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "clip,audio_label,audio_weights = batch\n",
    "# print(audio_label)\n",
    "print(type(audio_label))\n",
    "print(audio_label.shape)\n",
    "# print(clip)\n",
    "print(clip.shape)\n",
    "print(audio_weights)\n",
    "print(audio_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, I will analyze the layers of the pre-trained model, select which layers to use as feature extractors, and redefine the new model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = timm.create_model('tf_efficientnetv2_s_in21k', pretrained=True,in_chans=3) # You can change the data channel accepted by the pre-trained model by passing in argument in_chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "EfficientNet                                  [128, 21843]              --\n",
       "├─Conv2dSame: 1-1                             [128, 24, 20, 251]        648\n",
       "├─BatchNormAct2d: 1-2                         [128, 24, 20, 251]        48\n",
       "│    └─Identity: 2-1                          [128, 24, 20, 251]        --\n",
       "│    └─SiLU: 2-2                              [128, 24, 20, 251]        --\n",
       "├─Sequential: 1-3                             [128, 256, 2, 16]         --\n",
       "│    └─Sequential: 2-3                        [128, 24, 20, 251]        --\n",
       "│    │    └─ConvBnAct: 3-1                    [128, 24, 20, 251]        5,232\n",
       "│    │    └─ConvBnAct: 3-2                    [128, 24, 20, 251]        5,232\n",
       "│    └─Sequential: 2-4                        [128, 48, 10, 126]        --\n",
       "│    │    └─EdgeResidual: 3-3                 [128, 48, 10, 126]        25,632\n",
       "│    │    └─EdgeResidual: 3-4                 [128, 48, 10, 126]        92,640\n",
       "│    │    └─EdgeResidual: 3-5                 [128, 48, 10, 126]        92,640\n",
       "│    │    └─EdgeResidual: 3-6                 [128, 48, 10, 126]        92,640\n",
       "│    └─Sequential: 2-5                        [128, 64, 5, 63]          --\n",
       "│    │    └─EdgeResidual: 3-7                 [128, 64, 5, 63]          95,744\n",
       "│    │    └─EdgeResidual: 3-8                 [128, 64, 5, 63]          164,480\n",
       "│    │    └─EdgeResidual: 3-9                 [128, 64, 5, 63]          164,480\n",
       "│    │    └─EdgeResidual: 3-10                [128, 64, 5, 63]          164,480\n",
       "│    └─Sequential: 2-6                        [128, 128, 3, 32]         --\n",
       "│    │    └─InvertedResidual: 3-11            [128, 128, 3, 32]         61,200\n",
       "│    │    └─InvertedResidual: 3-12            [128, 128, 3, 32]         171,296\n",
       "│    │    └─InvertedResidual: 3-13            [128, 128, 3, 32]         171,296\n",
       "│    │    └─InvertedResidual: 3-14            [128, 128, 3, 32]         171,296\n",
       "│    │    └─InvertedResidual: 3-15            [128, 128, 3, 32]         171,296\n",
       "│    │    └─InvertedResidual: 3-16            [128, 128, 3, 32]         171,296\n",
       "│    └─Sequential: 2-7                        [128, 160, 3, 32]         --\n",
       "│    │    └─InvertedResidual: 3-17            [128, 160, 3, 32]         281,440\n",
       "│    │    └─InvertedResidual: 3-18            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-19            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-20            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-21            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-22            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-23            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-24            [128, 160, 3, 32]         397,800\n",
       "│    │    └─InvertedResidual: 3-25            [128, 160, 3, 32]         397,800\n",
       "│    └─Sequential: 2-8                        [128, 256, 2, 16]         --\n",
       "│    │    └─InvertedResidual: 3-26            [128, 256, 2, 16]         490,152\n",
       "│    │    └─InvertedResidual: 3-27            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-28            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-29            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-30            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-31            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-32            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-33            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-34            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-35            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-36            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-37            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-38            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-39            [128, 256, 2, 16]         1,005,120\n",
       "│    │    └─InvertedResidual: 3-40            [128, 256, 2, 16]         1,005,120\n",
       "├─Conv2d: 1-4                                 [128, 1280, 2, 16]        327,680\n",
       "├─BatchNormAct2d: 1-5                         [128, 1280, 2, 16]        2,560\n",
       "│    └─Identity: 2-9                          [128, 1280, 2, 16]        --\n",
       "│    └─SiLU: 2-10                             [128, 1280, 2, 16]        --\n",
       "├─SelectAdaptivePool2d: 1-6                   [128, 1280]               --\n",
       "│    └─AdaptiveAvgPool2d: 2-11                [128, 1280, 1, 1]         --\n",
       "│    └─Flatten: 2-12                          [128, 1280]               --\n",
       "├─Linear: 1-7                                 [128, 21843]              27,980,883\n",
       "===============================================================================================\n",
       "Total params: 48,158,371\n",
       "Trainable params: 48,158,371\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 176.91\n",
       "===============================================================================================\n",
       "Input size (MB): 30.78\n",
       "Forward/backward pass size (MB): 6127.46\n",
       "Params size (MB): 192.02\n",
       "Estimated Total Size (MB): 6350.26\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(128,3,40,501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume model is the loaded complete EfficientNet model\n",
    "# Use the output of the first set of InvertedResidual\n",
    "feature_extractor = torch.nn.Sequential(\n",
    "    *list(model.children())[:-3]  # Remove the last three layers, which needs to be adjusted according to the actual model structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2dSame(3, 24, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (1): BatchNormAct2d(\n",
       "    24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvBnAct(\n",
       "        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): ConvBnAct(\n",
       "        (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): EdgeResidual(\n",
       "        (conv_exp): Conv2dSame(24, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): EdgeResidual(\n",
       "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): EdgeResidual(\n",
       "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): EdgeResidual(\n",
       "        (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): EdgeResidual(\n",
       "        (conv_exp): Conv2dSame(48, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): EdgeResidual(\n",
       "        (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): EdgeResidual(\n",
       "        (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): EdgeResidual(\n",
       "        (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(960, 960, kernel_size=(3, 3), stride=(2, 2), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv_pw): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=21843, bias=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier # classifier is the last fully connected layer of the model, out_features represents the number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "21843\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier.in_features)\n",
    "print(model.classifier.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-trained model (excluding the last 3 layers) for calculation\n",
    "clip=feature_extractor(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1280, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "print(clip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to separate feature extractor from lightningmodule and add it to dataloader as part of data processing\n",
    "def trainloader_collate(batch, mixup_layer,feature_extractor):\n",
    "    \"\"\"\n",
    "    When creating data batches, define how each batch should be stacked\n",
    "    parameters:\n",
    "        batch: is a list of tuples with (labels, clip, weights)\n",
    "    \"\"\"\n",
    "    # Unpack each individual sample in the batch\n",
    "    labels, clips, weights = zip(*batch)\n",
    "\n",
    "    # Stack the data into new batches\n",
    "    labels = torch.stack(labels).float()\n",
    "    clips = torch.stack(clips).float()\n",
    "\n",
    "    weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "    clips, labels, weights = mixup_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # Use Compose to combine multiple audio transformation operations. \n",
    "    # These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "    clips = audio_transforms(clips, sample_rate=32000)\n",
    "\n",
    "    # Convert audio data into mel spectrogram\n",
    "    clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "    ##Convert the amplitude of Mel Spectrogram to decibel (Decibel, dB)\n",
    "    clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "    # normalization\n",
    "    clips = (clips + 80) / 80\n",
    "\n",
    "    # Random masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "    clips = torchaudio.transforms.TimeMasking(\n",
    "        time_mask_param=20, iid_masks=True, p=0.3\n",
    "    )(clips)\n",
    "\n",
    "    # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "    clips = image_delta(clips)\n",
    "\n",
    "    # Mixing Audio\n",
    "    clips, labels,weights = mixup2_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # feature extractor\n",
    "    clips=feature_extractor(clips)\n",
    "\n",
    "    return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD=BirdclefDataset(df=train_df,bird_category_dir=\"./temp_files/13-2-bird-cates.npy\",train=True)\n",
    "train_dataloader = DataLoader(dataset=BD, batch_size=32, sampler=train_sampler, pin_memory=True,collate_fn=lambda batch: trainloader_collate(batch, mixup_layer,feature_extractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 182])\n",
      "torch.Size([32, 1280, 2, 16])\n",
      "tensor([0.7998, 0.8496, 0.6499, 0.7002, 0.7500, 0.8999, 0.7500, 0.5996, 0.8501,\n",
      "        0.7500, 0.6499, 0.7002, 0.7500, 0.8496, 0.6499, 0.6001, 0.7002, 0.7002,\n",
      "        0.8999, 0.7002, 0.8496, 0.8496, 0.5498, 0.7500, 0.8999, 0.8496, 0.7998,\n",
      "        0.6001, 0.7500, 0.7500, 0.6499, 0.7998], dtype=torch.float16)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "clip,audio_label,audio_weights = batch\n",
    "# print(audio_label)\n",
    "print(type(audio_label))\n",
    "print(audio_label.shape)\n",
    "# print(clip)\n",
    "print(clip.shape)\n",
    "print(audio_weights)\n",
    "print(audio_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to separate feature extractor from lightningmodule and add it to dataloader as part of data processing\n",
    "\n",
    "def valloader_collate(batch, feature_extractor):\n",
    "    \"\"\"\n",
    "    在数据批次创建时，定义每个批次该如何堆叠\n",
    "    parameters:\n",
    "        batch: is a list of tuples with (labels, clip, weights)\n",
    "    \"\"\"\n",
    "    # Unpack each individual sample in the batch\n",
    "    labels, clips, weights = zip(*batch)\n",
    "\n",
    "    # Stack the data into new batches\n",
    "    labels = torch.stack(labels).float()\n",
    "    clips = torch.stack(clips).float()\n",
    "\n",
    "    weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "    # clips, labels, weights = mixup_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # # Use Compose to combine multiple audio transformation operations. \n",
    "    # These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "    # clips = audio_transforms(clips, sample_rate=32000)\n",
    "\n",
    "    # Convert audio data into mel spectrogram\n",
    "    clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "    ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "    clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "    # normalization\n",
    "    clips = (clips + 80) / 80\n",
    "\n",
    "    # # Random masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "    # clips = torchaudio.transforms.TimeMasking(\n",
    "    #     time_mask_param=20, iid_masks=True, p=0.3\n",
    "    # )(clips)\n",
    "\n",
    "    # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "    clips = image_delta(clips)\n",
    "\n",
    "    # mixing audio\n",
    "    # clips, labels,weights = mixup2_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # feature extractor\n",
    "    clips=feature_extractor(clips)\n",
    "\n",
    "    return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BD = BirdclefDataset(\n",
    "    df=val_df, bird_category_dir=\"./temp_files/13-2-bird-cates.npy\", train=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=BD,\n",
    "    batch_size=32,\n",
    "    sampler=val_sampler,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda batch: valloader_collate(batch, feature_extractor),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 182])\n",
      "torch.Size([32, 1280, 2, 16])\n",
      "tensor([0.7998, 1.0000, 0.8999, 0.7998, 0.2000, 1.0000, 0.7998, 0.2000, 0.8999,\n",
      "        0.8999, 1.0000, 0.7002, 1.0000, 1.0000, 1.0000, 0.7998, 0.5000, 0.7998,\n",
      "        0.7998, 1.0000, 1.0000, 1.0000, 0.7998, 1.0000, 1.0000, 0.7998, 0.7002,\n",
      "        0.7998, 1.0000, 0.7002, 0.8999, 1.0000], dtype=torch.float16)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "clip,audio_label,audio_weights = batch\n",
    "# print(audio_label)\n",
    "print(type(audio_label))\n",
    "print(audio_label.shape)\n",
    "# print(clip)\n",
    "print(clip.shape)\n",
    "print(audio_weights)\n",
    "print(audio_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.2469e+01,  6.2483e+00, -8.8653e+00,  ..., -4.4712e+00,\n",
      "          -7.1122e+00, -7.2261e+00],\n",
      "         [ 1.2141e+01,  5.6110e+00,  4.7072e+00,  ..., -2.1433e+00,\n",
      "           1.0140e+01,  2.1438e+01],\n",
      "         [ 2.5661e-01,  2.7368e+00,  1.2372e+01,  ...,  1.1742e+01,\n",
      "           1.0400e+01,  1.8145e+00],\n",
      "         ...,\n",
      "         [ 1.9727e+01,  1.4484e+01,  3.8275e+00,  ...,  1.5333e-01,\n",
      "           5.3791e+00,  1.2065e+01],\n",
      "         [-2.0544e+01, -1.3588e+01,  3.1236e+00,  ..., -1.7952e+00,\n",
      "          -4.4621e+00, -9.4821e-01],\n",
      "         [ 5.9797e+00, -2.0643e+00,  2.6429e+00,  ...,  6.0352e+00,\n",
      "           5.7987e+00,  2.4760e+00]],\n",
      "\n",
      "        [[ 3.8677e+00, -1.1622e+00,  4.7125e+00,  ..., -5.4461e+00,\n",
      "          -5.6394e+00, -7.5521e+00],\n",
      "         [-1.6453e+01, -1.6136e+01, -8.6477e+00,  ...,  1.5164e+00,\n",
      "          -1.5115e-02, -3.2839e+00],\n",
      "         [ 9.8462e-01,  4.6436e+00, -1.7860e+00,  ...,  9.0861e+00,\n",
      "           1.2655e+01,  5.7870e+00],\n",
      "         ...,\n",
      "         [ 6.9145e+00, -2.4350e+00, -8.6132e+00,  ..., -5.2516e+00,\n",
      "          -1.9349e+00, -1.0739e+00],\n",
      "         [-1.9009e+01, -1.4546e+01, -3.0492e+00,  ...,  2.1245e+00,\n",
      "           8.0955e+00,  5.8067e+00],\n",
      "         [ 6.3063e+00,  6.9924e+00,  5.2821e+00,  ...,  1.0612e-01,\n",
      "          -6.1152e-01, -7.9396e+00]],\n",
      "\n",
      "        [[-1.8244e+01, -1.3252e+01, -7.9190e+00,  ..., -5.6562e+00,\n",
      "          -2.3416e+00, -3.8929e+00],\n",
      "         [ 1.2919e+00, -6.2810e+00,  6.1923e-02,  ...,  3.9069e-01,\n",
      "          -2.8164e+00,  2.5167e+00],\n",
      "         [-4.7650e+01, -3.7498e+01, -2.5522e+01,  ..., -1.6237e+01,\n",
      "          -1.8776e+01, -2.0188e+01],\n",
      "         ...,\n",
      "         [ 1.6713e+01,  1.7937e+01,  7.3404e+00,  ..., -1.3453e+00,\n",
      "           1.4560e+00, -8.2820e-01],\n",
      "         [-3.2250e+01, -2.4387e+01, -1.0564e+01,  ..., -3.9548e+00,\n",
      "          -1.9838e+00,  1.9767e+00],\n",
      "         [ 1.1350e+01,  4.1083e+00,  9.3754e-02,  ..., -8.0199e-01,\n",
      "          -3.2961e+00, -1.3097e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0821e+00,  1.7817e+00,  9.3213e-02,  ...,  3.0042e+00,\n",
      "          -6.4396e+00, -5.8436e+00],\n",
      "         [-2.2269e+00,  1.8239e+00,  2.2548e+00,  ...,  5.2797e+00,\n",
      "          -5.1498e+00, -7.7791e+00],\n",
      "         [-1.0203e+01, -3.2947e+00, -8.5357e-01,  ..., -3.7632e+00,\n",
      "          -5.2974e+00, -4.2137e+00],\n",
      "         ...,\n",
      "         [ 6.0744e+00, -4.0042e+00, -4.8318e+00,  ...,  5.4378e+00,\n",
      "           1.0058e+01,  5.9228e+00],\n",
      "         [-2.0006e+00, -2.9711e+00, -3.6550e+00,  ...,  1.1374e+01,\n",
      "           2.0605e+00, -1.7119e+00],\n",
      "         [ 3.9961e+00,  1.1648e+01,  3.8269e+00,  ..., -4.1952e+00,\n",
      "          -4.6410e+00, -3.3340e+00]],\n",
      "\n",
      "        [[ 2.1997e+00,  7.2269e-01, -1.6235e-01,  ...,  1.2577e+01,\n",
      "           4.1519e+00, -4.1823e+00],\n",
      "         [-4.2542e+00,  1.3987e+00,  4.0064e+00,  ...,  1.3937e+01,\n",
      "           1.0073e+01, -9.8592e-01],\n",
      "         [-7.6047e+00, -3.6905e-01,  1.4992e+00,  ...,  8.6677e+00,\n",
      "           1.3942e+01,  1.3521e+01],\n",
      "         ...,\n",
      "         [ 1.8191e+00,  6.8647e-01,  2.1205e+00,  ...,  1.7526e+00,\n",
      "           8.1896e+00, -7.2751e+00],\n",
      "         [-2.9314e+00, -9.7524e+00, -4.5892e+00,  ..., -5.0627e-01,\n",
      "           1.5468e+01,  2.8122e+01],\n",
      "         [-4.1576e-01, -3.6964e+00, -2.4973e+00,  ..., -4.8991e+00,\n",
      "          -1.3202e+01, -4.4614e+00]],\n",
      "\n",
      "        [[ 9.0667e+00,  2.5367e-01, -1.1680e+00,  ..., -3.5867e+00,\n",
      "          -2.6600e+00, -2.8056e+00],\n",
      "         [ 1.3787e+01,  9.1114e+00, -4.8017e+00,  ..., -3.5849e-01,\n",
      "           2.2261e+00,  1.0770e+01],\n",
      "         [-2.5691e+01, -9.3499e+00,  8.2761e+00,  ..., -8.7522e+00,\n",
      "          -4.9464e+00,  3.5575e+00],\n",
      "         ...,\n",
      "         [-3.5705e+00, -1.1674e+01, -1.9998e+01,  ..., -2.5440e-01,\n",
      "          -3.3916e+00, -3.9674e+00],\n",
      "         [-6.8318e+00,  3.1841e+00,  2.6804e-01,  ..., -3.5277e+00,\n",
      "          -6.8579e+00, -7.6662e+00],\n",
      "         [ 4.2722e+00,  1.5985e+01,  1.6976e+01,  ...,  6.9422e-01,\n",
      "          -1.1866e+00, -2.0267e+00]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([32, 1280, 32])\n"
     ]
    }
   ],
   "source": [
    "# Use flatten to combine the last two dimensions\n",
    "x_flattened = torch.flatten(clip, start_dim=2)  # The resulting shape is also [32, 1280, 32]\n",
    "\n",
    "print(x_flattened)\n",
    "print(x_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gru1=nn.GRU(input_size=1280,hidden_size=128,num_layers=1,batch_first=True)\n",
    "        self.bn1= nn.BatchNorm1d(num_features=32)\n",
    "        self.gru2=nn.GRU(input_size=128,hidden_size=128,num_layers=1,batch_first=True)\n",
    "        self.bn2= nn.BatchNorm1d(num_features=32)\n",
    "        self.gru3=nn.GRU(input_size=256,hidden_size=128,num_layers=1,batch_first=True)\n",
    "        self.bn3= nn.BatchNorm1d(num_features=32)\n",
    "        self.gru4=nn.GRU(input_size=384,hidden_size=128,num_layers=1,batch_first=True)\n",
    "        self.bn4= nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1=nn.Linear(in_features=128,out_features=182)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        x1=self.bn1(gru_out1)\n",
    "        gru_out2,_=self.gru2(x1)\n",
    "        x2=self.bn2(gru_out2)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x3=torch.cat((x1,x2),dim=2)\n",
    "        gru_out3,_=self.gru3(x3)\n",
    "        x4=self.bn3(gru_out3)\n",
    "        x5=torch.cat((x1,x2,x4),dim=2)\n",
    "        gru_out4,_=self.gru4(x5)\n",
    "        x6 = self.dropout1(gru_out4[:, -1, :]) #Usually take the final output of GRU\n",
    "        out = self.fc1(x6) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 182])\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "clip=model(x_flattened)\n",
    "\n",
    "print(clip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0392,  0.3155,  0.4806,  0.4337,  0.1109,  0.1359, -0.4806, -0.1386,\n",
      "         0.4318, -0.4446, -0.2128, -0.3190,  0.3317,  0.1431,  0.4492, -0.0396,\n",
      "        -0.5315,  0.3659, -0.6303, -0.5312,  0.1577,  0.3573,  0.4484, -0.5918,\n",
      "         0.4210, -0.1279,  0.2047,  0.1297,  0.1064, -0.2537,  0.3900,  0.6682,\n",
      "        -0.0828, -0.2345, -0.3453,  0.1298,  0.4748, -0.0998, -0.1111,  0.1332,\n",
      "        -0.1437, -0.1141, -0.3809,  0.2559,  0.6969, -0.5416, -0.3398, -0.1797,\n",
      "         0.8385, -0.3311,  0.6701,  0.4356,  0.6088, -0.2598, -0.3122,  0.4474,\n",
      "         0.0239, -0.0103,  0.3292, -0.0232,  1.1892, -0.8816, -0.2917,  0.7434,\n",
      "        -0.2166, -0.7863,  0.0615, -0.1152,  0.5993,  0.1749,  0.3961,  0.7628,\n",
      "        -0.3389,  0.0672,  0.0211,  0.2785,  0.1851,  0.1546, -0.0641,  0.0270,\n",
      "        -0.2108, -0.2117,  0.3912,  0.6857, -0.6390,  0.2813,  0.0495,  0.4485,\n",
      "        -0.1133,  0.2673, -0.1169, -0.0609, -0.8051,  0.1734, -0.0587,  0.2924,\n",
      "         0.1399,  0.6104, -0.2507,  0.1664,  0.6699, -0.2290, -0.3453, -0.4528,\n",
      "        -0.3147,  0.0206,  0.1690, -0.1940, -0.2498,  0.1387, -0.2624,  0.2657,\n",
      "         0.6196, -0.6974, -0.0253, -0.3035, -0.8659,  0.6428, -0.6603,  0.4313,\n",
      "         0.5797,  0.0551,  0.3403, -0.4955,  0.1380, -1.0670,  0.0094, -0.0172,\n",
      "        -0.6068, -0.1500, -0.1116, -0.3647, -0.3707, -0.5001,  0.1665,  0.4606,\n",
      "         0.5834, -0.5822,  0.1796,  0.6966, -0.3663,  0.1529, -0.5822, -0.2053,\n",
      "        -0.6843,  0.4873, -0.7593,  0.5715, -0.1666, -0.3353, -0.3027,  0.3353,\n",
      "         0.1653, -0.0556, -0.9123,  0.2709,  0.0356,  0.2754, -0.0222,  0.5175,\n",
      "        -0.1574, -0.3880, -0.3853,  0.3083, -0.1428,  0.0448, -0.1649, -0.9809,\n",
      "         0.2315, -0.0647, -0.0935, -0.0758, -0.7767,  0.4029,  0.9174,  0.5066,\n",
      "        -0.3439, -0.1802, -0.1216,  0.2469, -0.0153,  0.8595],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(clip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, sample_weight=None,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # Class weights\n",
    "        self.sample_weight=sample_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
    "        p_t = torch.exp(-ce_loss) # Modulating Factor\n",
    "        loss = (1 - p_t) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.sample_weight is not None:\n",
    "            loss *= self.sample_weight\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal=FocalLoss(weight=cate_weight,sample_weight=audio_weights)\n",
    "\n",
    "loss=focal(inputs=clip,targets=audio_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64.7562, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
