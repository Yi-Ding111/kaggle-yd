{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all notebooks before I did are based off 5 seconds long clips. However, I cannot get good results from that.\n",
    "\n",
    "I want to adjust the data process from 5 sec to 30 sec long to see if I could get a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../data/train/asiope1/XC194954.ogg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,sr=read_audio(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4184e-05, -1.9732e-05, -6.3533e-06,  ...,  3.2216e-05,\n",
      "         -8.1519e-06, -8.0238e-06]])\n",
      "torch.Size([1, 457247])\n"
     ]
    }
   ],
   "source": [
    "print(audio)\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_audio_30_align(audio: torch.Tensor, sample_rate: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Slice the complete audio tensor into multiple 30 seconds length,\n",
    "    keep all slices having the same length, especially for the last slice.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform of the audio.\n",
    "        sample_rate: The sample rate of the audio file.\n",
    "\n",
    "    Returns:\n",
    "        segments: List of tensors, each representing a 30-second audio segment.\n",
    "    \"\"\"\n",
    "    # Set up the segment duration in samples\n",
    "    segment_duration = 5 * sample_rate  # 5 seconds in number of samples\n",
    "\n",
    "    # Total number of samples in the waveform\n",
    "    total_samples = audio.shape[1]\n",
    "\n",
    "    # Check if the audio is less than 5 seconds\n",
    "    if total_samples < segment_duration:\n",
    "        # Calculate the required padding length\n",
    "        padding_length = segment_duration - total_samples\n",
    "        # Create a tensor of zeros (silence) for padding\n",
    "        silence = torch.zeros(audio.shape[0], padding_length)\n",
    "        # Pad the waveform with silence\n",
    "        padded_waveform = torch.cat([audio, silence], dim=1)\n",
    "        return [padded_waveform]  # Return the padded waveform as a single segment\n",
    "\n",
    "    # If the audio is 5 seconds or longer, proceed as normal\n",
    "    segments = [audio[:, i:i + segment_duration] for i in range(0, total_samples, segment_duration)]\n",
    "\n",
    "    # Ensure the last segment is exactly 5 seconds long\n",
    "    if segments[-1].shape[1] != segment_duration:\n",
    "        # Extract the last segment_duration samples to ensure it's 5 seconds long\n",
    "        last_segment = audio[:, -segment_duration:]\n",
    "        segments[-1] = last_segment  # Replace the last segment with a full 5-second segment\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments=slice_audio_30_align(audio=audio,sample_rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-3.4184e-05, -1.9732e-05, -6.3533e-06,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])]\n",
      "torch.Size([1, 960000])\n"
     ]
    }
   ],
   "source": [
    "print(segments)\n",
    "print(segments[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips=segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.02,hop_size:float=0.005,n_mels:int=64)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    # Determine window size and frame shift\n",
    "    # window_size = 0.04 # 40 milliseconds\n",
    "    # hop_size = 0.02 # 20 milliseconds, usually half the window size\n",
    "    # n_fft = int(window_size * sample_rate) # Convert window size to number of sampling points\n",
    "    n_fft=1024\n",
    "    # hop_length defines the overlap between windows, affecting the resolution of the spectrum graph on the time axis\n",
    "    # hop_length = int(hop_size * sample_rate) # Convert frame shift to sampling point number\n",
    "    hop_length=160\n",
    "\n",
    "    # Calculate Mel Spectrogram\n",
    "    # n_mels = 40 # Number of Mel filters\n",
    "\n",
    "    # Set up Mel Spectrogram converter\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=640,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = mel_transform(sample_rate=32000, audio=clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.3444e-09, 8.3298e-10, 3.3775e-09,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.3504e-09, 2.7876e-09, 1.2644e-08,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.3824e-09, 3.0708e-09, 3.1785e-08,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.0626e-06, 1.4593e-06, 4.4943e-06,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.7839e-06, 2.4552e-06, 5.9570e-06,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0122e-06, 1.6945e-06, 5.0962e-06,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 6001])\n"
     ]
    }
   ],
   "source": [
    "print(clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\") -> torch.Tensor:\n",
    "    \"\"\"Compute delta coefficients of a tensor, usually a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "        win_length (int, optional): The window length used for computing delta (Default: 5)\n",
    "        mode (str, optional): Mode parameter passed to padding (Default: \"replicate\")\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "    \"\"\"\n",
    "    device = specgram.device  # Get the device of the input tensor\n",
    "    dtype = specgram.dtype\n",
    "\n",
    "    # pack batch\n",
    "    shape = specgram.size()\n",
    "    specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "    assert win_length >= 3\n",
    "    n = (win_length - 1) // 2\n",
    "    denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "    # Create the kernel tensor, making sure it is on the same device as the input tensor\n",
    "    kernel = torch.arange(-n, n + 1, 1, dtype=dtype,device=device).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "    output = (\n",
    "        torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "    )\n",
    "\n",
    "    # unpack batch\n",
    "    output = output.reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def make_delta(input_tensor: torch.Tensor):\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    input_tensor = compute_deltas(input_tensor)\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "def image_delta(x):\n",
    "    delta_1 = make_delta(x)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([x, delta_1, delta_2], dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 6001])\n"
     ]
    }
   ],
   "source": [
    "# Because batch is not introduced yet, manually add clips to batch=1 and calculate delta and double-delta\n",
    "clips=clips.unsqueeze(1)\n",
    "\n",
    "print(clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips=image_delta(x=clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 6001])\n"
     ]
    }
   ],
   "source": [
    "print(clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "pretrained_model = timm.create_model('tf_efficientnetv2_s_in21k', pretrained=True,in_chans=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "EfficientNet                                  [1, 21843]                --\n",
       "├─Conv2dSame: 1-1                             [1, 24, 32, 3001]         648\n",
       "├─BatchNormAct2d: 1-2                         [1, 24, 32, 3001]         48\n",
       "│    └─Identity: 2-1                          [1, 24, 32, 3001]         --\n",
       "│    └─SiLU: 2-2                              [1, 24, 32, 3001]         --\n",
       "├─Sequential: 1-3                             [1, 256, 2, 188]          --\n",
       "│    └─Sequential: 2-3                        [1, 24, 32, 3001]         --\n",
       "│    │    └─ConvBnAct: 3-1                    [1, 24, 32, 3001]         5,232\n",
       "│    │    └─ConvBnAct: 3-2                    [1, 24, 32, 3001]         5,232\n",
       "│    └─Sequential: 2-4                        [1, 48, 16, 1501]         --\n",
       "│    │    └─EdgeResidual: 3-3                 [1, 48, 16, 1501]         25,632\n",
       "│    │    └─EdgeResidual: 3-4                 [1, 48, 16, 1501]         92,640\n",
       "│    │    └─EdgeResidual: 3-5                 [1, 48, 16, 1501]         92,640\n",
       "│    │    └─EdgeResidual: 3-6                 [1, 48, 16, 1501]         92,640\n",
       "│    └─Sequential: 2-5                        [1, 64, 8, 751]           --\n",
       "│    │    └─EdgeResidual: 3-7                 [1, 64, 8, 751]           95,744\n",
       "│    │    └─EdgeResidual: 3-8                 [1, 64, 8, 751]           164,480\n",
       "│    │    └─EdgeResidual: 3-9                 [1, 64, 8, 751]           164,480\n",
       "│    │    └─EdgeResidual: 3-10                [1, 64, 8, 751]           164,480\n",
       "│    └─Sequential: 2-6                        [1, 128, 4, 376]          --\n",
       "│    │    └─InvertedResidual: 3-11            [1, 128, 4, 376]          61,200\n",
       "│    │    └─InvertedResidual: 3-12            [1, 128, 4, 376]          171,296\n",
       "│    │    └─InvertedResidual: 3-13            [1, 128, 4, 376]          171,296\n",
       "│    │    └─InvertedResidual: 3-14            [1, 128, 4, 376]          171,296\n",
       "│    │    └─InvertedResidual: 3-15            [1, 128, 4, 376]          171,296\n",
       "│    │    └─InvertedResidual: 3-16            [1, 128, 4, 376]          171,296\n",
       "│    └─Sequential: 2-7                        [1, 160, 4, 376]          --\n",
       "│    │    └─InvertedResidual: 3-17            [1, 160, 4, 376]          281,440\n",
       "│    │    └─InvertedResidual: 3-18            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-19            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-20            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-21            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-22            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-23            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-24            [1, 160, 4, 376]          397,800\n",
       "│    │    └─InvertedResidual: 3-25            [1, 160, 4, 376]          397,800\n",
       "│    └─Sequential: 2-8                        [1, 256, 2, 188]          --\n",
       "│    │    └─InvertedResidual: 3-26            [1, 256, 2, 188]          490,152\n",
       "│    │    └─InvertedResidual: 3-27            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-28            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-29            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-30            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-31            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-32            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-33            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-34            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-35            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-36            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-37            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-38            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-39            [1, 256, 2, 188]          1,005,120\n",
       "│    │    └─InvertedResidual: 3-40            [1, 256, 2, 188]          1,005,120\n",
       "├─Conv2d: 1-4                                 [1, 1280, 2, 188]         327,680\n",
       "├─BatchNormAct2d: 1-5                         [1, 1280, 2, 188]         2,560\n",
       "│    └─Identity: 2-9                          [1, 1280, 2, 188]         --\n",
       "│    └─SiLU: 2-10                             [1, 1280, 2, 188]         --\n",
       "├─SelectAdaptivePool2d: 1-6                   [1, 1280]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-11                [1, 1280, 1, 1]           --\n",
       "│    └─Flatten: 2-12                          [1, 1280]                 --\n",
       "├─Linear: 1-7                                 [1, 21843]                27,980,883\n",
       "===============================================================================================\n",
       "Total params: 48,158,371\n",
       "Trainable params: 48,158,371\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 21.84\n",
       "===============================================================================================\n",
       "Input size (MB): 4.61\n",
       "Forward/backward pass size (MB): 746.27\n",
       "Params size (MB): 192.02\n",
       "Estimated Total Size (MB): 942.90\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(pretrained_model,input_size=(1,3,64,6001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    initilize dense layers' parameters\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # Initialize the weights and biases of the network layer\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # Check if the layer has a bias attribute\n",
    "        if layer.bias is not None: # and bias is not None\n",
    "            layer.bias.data.fill_(0.0) # If there is a bias, initialize it to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later we want to pass the acquired high-dimensional features into an attention module\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "        # x: This is the final output after the attention weights and classification layer.\n",
    "        # shape: (n_samples, out_features). Since the time dimension is summed and compressed, each sample and each output feature ends up having a single value.\n",
    "        # norm_att: This is the output of the attention layer (att) after the softmax and tanh functions, which shows which parts of the input sequence the model should focus on. \n",
    "        # Normalization ensures that the attention weights for all time steps add up to 1, which makes it easier to interpret the importance of each time step.\n",
    "        # shape: (n_samples, out_features, n_time), where out_features is the number of output features of the att convolutional layer, which is the same as the out_features argument of the input. \n",
    "        # Each time step and each output feature has a normalized weight.\n",
    "        # cla: This is the output of the classification layer (cla), which is obtained by processing the input features through another 1D convolutional layer. \n",
    "        # This output layer is often used to directly predict task-related outputs, such as the probability of a class label.\n",
    "        # Shape: (n_samples, out_features, n_time), same shape as norm_att. This means that each output feature corresponding to each time step has a value processed by the activation function.\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(pretrained_model.children())[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips=encoder(clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 2, 188])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = torch.mean(clips, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 188])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = F.max_pool1d(clips, kernel_size=3, stride=1, padding=1)\n",
    "x2 = F.avg_pool1d(clips, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "x=x1+x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 188])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.transpose(1, 2)\n",
    "\n",
    "fc1 = nn.Linear(in_features=1280, out_features=1280, bias=True)\n",
    "x = F.relu_(fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 188, 1280])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 188])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.transpose(1, 2)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_block=AttBlockV2(in_features=1280, out_features=182, activation=\"sigmoid\")\n",
    "target_pred, norm_att, segmentwise_output = att_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5608, 0.5782, 0.1967, 0.4402, 0.4739, 0.2244, 0.5200, 0.4540, 0.6897,\n",
       "         0.4866, 0.4236, 0.4556, 0.3476, 0.6344, 0.4943, 0.3387, 0.3350, 0.6841,\n",
       "         0.4972, 0.3843, 0.6362, 0.7239, 0.4768, 0.6738, 0.4576, 0.4403, 0.6815,\n",
       "         0.4899, 0.4575, 0.4765, 0.3783, 0.4529, 0.6115, 0.5004, 0.3865, 0.6503,\n",
       "         0.3329, 0.3666, 0.2296, 0.4565, 0.3741, 0.6746, 0.5814, 0.6070, 0.4672,\n",
       "         0.4838, 0.5268, 0.6186, 0.7956, 0.4798, 0.7582, 0.2901, 0.4931, 0.6023,\n",
       "         0.5570, 0.5558, 0.4440, 0.6446, 0.3110, 0.3769, 0.4136, 0.4235, 0.7147,\n",
       "         0.5674, 0.5391, 0.3794, 0.7269, 0.4311, 0.5004, 0.5672, 0.6284, 0.5760,\n",
       "         0.7861, 0.6082, 0.4633, 0.6593, 0.3987, 0.5142, 0.6563, 0.6528, 0.6056,\n",
       "         0.2514, 0.3870, 0.3149, 0.4226, 0.3753, 0.3722, 0.6241, 0.5200, 0.5041,\n",
       "         0.3596, 0.4487, 0.5331, 0.5930, 0.3398, 0.3865, 0.4588, 0.4081, 0.4368,\n",
       "         0.5722, 0.3550, 0.5919, 0.5105, 0.3923, 0.4679, 0.4784, 0.5790, 0.5515,\n",
       "         0.6007, 0.4830, 0.5661, 0.6459, 0.6671, 0.3814, 0.6977, 0.6263, 0.3057,\n",
       "         0.6719, 0.7547, 0.6254, 0.6967, 0.5859, 0.4988, 0.6642, 0.6118, 0.5091,\n",
       "         0.6199, 0.5481, 0.6101, 0.4966, 0.4486, 0.4044, 0.6266, 0.4557, 0.3354,\n",
       "         0.5389, 0.4110, 0.6986, 0.6366, 0.7059, 0.2659, 0.3466, 0.6262, 0.4388,\n",
       "         0.5450, 0.7226, 0.2657, 0.6081, 0.3751, 0.2673, 0.5245, 0.5408, 0.6551,\n",
       "         0.3684, 0.4861, 0.4237, 0.5626, 0.4146, 0.5301, 0.3266, 0.6128, 0.2615,\n",
       "         0.4521, 0.4215, 0.5935, 0.5354, 0.3725, 0.5953, 0.2930, 0.6141, 0.6099,\n",
       "         0.3425, 0.6739, 0.7079, 0.4469, 0.5041, 0.6041, 0.5011, 0.8392, 0.2941,\n",
       "         0.4100, 0.3760]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer\n",
    "        # Here I choose to replace these two layers, first remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # Create a fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # Add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them to compress the dimension\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        pass\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
