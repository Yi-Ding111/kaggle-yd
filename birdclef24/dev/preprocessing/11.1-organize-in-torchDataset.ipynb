{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path='../../data/train_metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build up the customized torch Dataset\n",
    "\n",
    "Dataset is a way to encapsulate and process datasets. It is used to load and prepare data in PyTorch so that it can be used with `DataLoader` to achieve efficient data iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build up `__len__()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,audio_dir=None,labels_path=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            audio_dir: the directory of the audio\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.audio_dir=audio_dir\n",
    "        self.labels_path=labels_path\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "\n",
    "        print('marked_label')\n",
    "\n",
    "        # read data into dataframe\n",
    "        labels_df=pd.read_csv(self.labels_path,header=0)\n",
    "        \n",
    "        return len(labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitem__(1)\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marked_label\n",
      "24459\n",
      "marked_label\n",
      "24459\n"
     ]
    }
   ],
   "source": [
    "#test __len__()function\n",
    "\n",
    "dataset_length=len(BirdclefDataset(labels_path=labels_path))\n",
    "\n",
    "print(dataset_length)\n",
    "\n",
    "dataset_length=BirdclefDataset(labels_path=labels_path).__len__()\n",
    "\n",
    "print(dataset_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add path generation function ino Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,audio_dir:str='../../data/train_audio',\n",
    "                 labels_path:str=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.audio_dir=audio_dir\n",
    "        # read data into dataframe\n",
    "        self.labels_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "\n",
    "    def get_audio_path(self,index) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "        # Get the child path of audio from labels_df\n",
    "        audio_child_path=self.labels_df['filename'].iloc[index]\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,audio_child_path)\n",
    "    \n",
    "\n",
    "    def get_audio_label(self,index)->str:\n",
    "        '''\n",
    "        According to the provided index, get the corresponding label from the train metadata file\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "        '''\n",
    "\n",
    "        return self.labels_df['primary_label'].iloc[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "    \n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitem__(1)\n",
    "        # Get the path of a single audio file\n",
    "        single_audio_dir=self.get_audio_path(index)\n",
    "        # Get the corresponding label value\n",
    "        audio_label=self.get_audio_label(index)\n",
    "\n",
    "        return single_audio_dir, audio_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asbfly/XC134896.ogg\n",
      "asbfly/XC134896.ogg\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "# Find the filename of the corresponding index from the metadata file\n",
    "\n",
    "metafile=pd.read_csv(labels_path,header=0)\n",
    "filename=metafile['filename'].iloc[0]\n",
    "print(filename)\n",
    "print(metafile.iloc[:,11][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/train_audio/asbfly/XC134896.ogg\n",
      "asbfly\n",
      "../../data/train_audio/zitcis1/XC858550.ogg\n",
      "zitcis1\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# Get a set of corresponding audio paths and labels from the Dataset\n",
    "\n",
    "audio_path,true_label=BirdclefDataset(labels_path=labels_path).__getitem__(0)\n",
    "\n",
    "print(audio_path)\n",
    "print(true_label)\n",
    "\n",
    "\n",
    "audio_path,true_label=BirdclefDataset(labels_path=labels_path).__getitem__(24458)\n",
    "\n",
    "print(audio_path)\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add audio read function, let Dataset could return `audio array` instead with `audio path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,audio_dir:str='../../data/train_audio',\n",
    "                 labels_path:str=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.audio_dir=audio_dir\n",
    "        # read data into dataframe\n",
    "        self.labels_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "\n",
    "    def get_audio_path(self,index) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "        # Get the child path of audio from labels_df\n",
    "        audio_child_path=self.labels_df['filename'].iloc[index]\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,audio_child_path)\n",
    "    \n",
    "\n",
    "    def get_audio_label(self,index)->str:\n",
    "        '''\n",
    "        According to the provided index, get the corresponding label from the train metadata file\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "        '''\n",
    "\n",
    "        return self.labels_df['primary_label'].iloc[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "    \n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitems__(1)\n",
    "        # Get the path to a single audio file\n",
    "        single_audio_dir=self.get_audio_path(index)\n",
    "        # Get the corresponding label value\n",
    "        audio_label=self.get_audio_label(index)\n",
    "\n",
    "        # Read audio array according to path\n",
    "        audio, sr=read_audio(single_audio_dir)\n",
    "\n",
    "        return audio, audio_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 875207])\n",
      "asbfly\n",
      "torch.Size([1, 875207])\n",
      "asbfly\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# Get a set of corresponding audio array and label from the Dataset\n",
    "\n",
    "audio_path,true_label=BirdclefDataset(labels_path=labels_path).__getitem__(0)\n",
    "\n",
    "print(audio_path.shape)\n",
    "print(true_label)\n",
    "\n",
    "\n",
    "BD=BirdclefDataset(labels_path=labels_path)\n",
    "\n",
    "audio_path,true_label=BD[0]\n",
    "\n",
    "print(audio_path.shape)\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I need to split each signal into 5-second signals, and now I need to add this part to the torch Dataset\n",
    "\n",
    "Because I need to treat each 5-second audio as a whole, and not lose other information of the entire audio\n",
    "\n",
    "If we use the traditional method, that is, Dataset gets an index, then reads the audio corresponding to the index, and then cuts it into multiple segments, when using dataloader to load data, we will use collate_fn to customize the rensor stacking method, but it will cause inaccurate batch size, thus losing data in the training process.\n",
    "\n",
    "So I used 4.1-create-new-trainmatadata-csv.ipynb to rebuild a csv file to correspond to the index.\n",
    "\n",
    "According to this logic,\n",
    "\n",
    "1. I need to create a function, target_clip() to get the corresponding audio clip in the train-metadata-csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,audio_dir:str='../../data/train_audio',\n",
    "                 labels_path:str=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.audio_dir=audio_dir\n",
    "        # read data into dataframe\n",
    "        self.labels_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "\n",
    "    def get_audio_path(self,index) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "        # Get the child path of audio from labels_df\n",
    "        audio_child_path=self.labels_df['filename'].iloc[index]\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,audio_child_path)\n",
    "    \n",
    "\n",
    "    def get_audio_label(self,index)->str:\n",
    "        '''\n",
    "        According to the provided index, get the corresponding label from the train metadata file\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "        '''\n",
    "\n",
    "        return self.labels_df['primary_label'].iloc[index]\n",
    "    \n",
    "\n",
    "    def target_clip(self,index:int,audio:torch.Tensor,sample_rate:int, duration_seconds:float)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip \n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "            duration_seconds: audio duration in seconds\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time=self.labels_df['clip_start_time'].iloc[index]\n",
    "\n",
    "        #define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time<=duration_seconds:\n",
    "            clip_start_point=clip_start_time*sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point+segment_duration<=total_samples:\n",
    "                clip=audio[:, clip_start_point:clip_start_point + segment_duration]\n",
    "\n",
    "            # Not long enough, a mask is needed\n",
    "            else:\n",
    "                padding_length = clip_start_point+segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "                # Join the last segment of raw audio with silence\n",
    "                clip=torch.cat((audio[:,clip_start_point:],silence),dim=1)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('The clip start time is out of raw audio length')\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "    \n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitems__(1)\n",
    "        # Get the path to a single audio file\n",
    "        single_audio_dir=self.get_audio_path(index)\n",
    "        # Get the corresponding label value\n",
    "        audio_label=self.get_audio_label(index)\n",
    "\n",
    "        # Read audio array according to path\n",
    "        audio, sr=read_audio(single_audio_dir)\n",
    "        \n",
    "        # Read the duration and number of channels corresponding to the audio\n",
    "        duration_seconds, num_channels=audio_info(audio,sample_rate=sr)\n",
    "\n",
    "        # Get the audio clip corresponding to index\n",
    "        clip=self.target_clip(index,audio,sample_rate=sr, duration_seconds=duration_seconds)\n",
    "\n",
    "        return audio, audio_label, clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path='../../data/train_metadata_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 425482])\n",
      "malpar1\n",
      "tensor([[ 4.2087e-07, -5.0013e-07, -6.1193e-06,  ...,  3.1517e-03,\n",
      "          5.3921e-03,  7.1919e-03]])\n",
      "torch.Size([1, 160000])\n",
      "torch.Size([1, 425482])\n",
      "malpar1\n",
      "tensor([[ 0.0051,  0.0061, -0.0214,  ...,  0.0072,  0.0065,  0.0038]])\n",
      "torch.Size([1, 160000])\n",
      "torch.Size([1, 425482])\n",
      "malpar1\n",
      "tensor([[-0.0018, -0.0085, -0.0017,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "torch.Size([1, 160000])\n"
     ]
    }
   ],
   "source": [
    "BD=BirdclefDataset(labels_path=labels_path)\n",
    "\n",
    "audio_path,true_label,clip=BD[0]\n",
    "print(audio_path.shape)\n",
    "print(true_label)\n",
    "print(clip)\n",
    "print(clip.shape)\n",
    "\n",
    "\n",
    "audio_path,true_label,clip=BD[1]\n",
    "print(audio_path.shape)\n",
    "print(true_label)\n",
    "print(clip)\n",
    "print(clip.shape)\n",
    "\n",
    "audio_path,true_label,clip=BD[2]\n",
    "print(audio_path.shape)\n",
    "print(true_label)\n",
    "print(clip)\n",
    "print(clip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.2963125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "425482/32000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This step requires converting the audio into mel spectgram representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert audio data into mel spectrogram\n",
    "\n",
    "\n",
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.02,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    n_fft = int(window_size * sample_rate)  \n",
    "    hop_length = int(hop_size * sample_rate) \n",
    "\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,audio_dir:str='../../data/train_audio',\n",
    "                 labels_path:str=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.audio_dir=audio_dir\n",
    "        # read data into dataframe\n",
    "        self.labels_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "\n",
    "    def get_audio_path(self,index) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "        # Get the child path of audio from labels_df\n",
    "        audio_child_path=self.labels_df['filename'].iloc[index]\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,audio_child_path)\n",
    "    \n",
    "\n",
    "    def get_audio_label(self,index)->str:\n",
    "        '''\n",
    "        According to the provided index, get the corresponding label from the train metadata file\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "        '''\n",
    "\n",
    "        return self.labels_df['primary_label'].iloc[index]\n",
    "    \n",
    "\n",
    "    def target_clip(self,index:int,audio:torch.Tensor,sample_rate:int, duration_seconds:float)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip \n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "            duration_seconds: audio duration in seconds\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time=self.labels_df['clip_start_time'].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time<=duration_seconds:\n",
    "            clip_start_point=clip_start_time*sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point+segment_duration<=total_samples:\n",
    "                clip=audio[:, clip_start_point:clip_start_point + segment_duration]\n",
    "\n",
    "            # Not long enough, a mask is needed\n",
    "            else:\n",
    "                padding_length = clip_start_point+segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "                # concat the last segment of raw audio with silence\n",
    "                clip=torch.cat((audio[:,clip_start_point:],silence),dim=1)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('The clip start time is out of raw audio length')\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "    \n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitems__(1)\n",
    "        # Get the path to a single audio file\n",
    "        single_audio_dir=self.get_audio_path(index)\n",
    "        # Get the corresponding label value\n",
    "        audio_label=self.get_audio_label(index)\n",
    "\n",
    "        # Read audio array according to path\n",
    "        audio, sr=read_audio(single_audio_dir)\n",
    "        \n",
    "        # Read the duration and number of channels corresponding to the audio\n",
    "        duration_seconds, num_channels=audio_info(audio,sample_rate=sr)\n",
    "\n",
    "        # Get the audio clip corresponding to index\n",
    "        clip=self.target_clip(index,audio,sample_rate=sr, duration_seconds=duration_seconds)\n",
    "\n",
    "        # mel spectrogram transformation\n",
    "        mel_spec=mel_transform(sample_rate=sr,audio=clip)\n",
    "\n",
    "        return audio_label, mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malpar1\n",
      "tensor([[[1.0330e-08, 2.2865e-06, 1.0572e-05,  ..., 2.3917e-08,\n",
      "          1.0114e-08, 4.6815e-03],\n",
      "         [4.9843e-08, 4.1700e-06, 4.2066e-05,  ..., 3.5288e-06,\n",
      "          1.3134e-05, 5.3317e-03],\n",
      "         [7.5681e-08, 4.7742e-05, 3.2700e-04,  ..., 1.1220e-04,\n",
      "          7.3640e-05, 4.6690e-03],\n",
      "         ...,\n",
      "         [6.5626e-06, 5.8203e-02, 3.5607e-01,  ..., 4.0643e-01,\n",
      "          4.1383e-01, 2.8917e-01],\n",
      "         [6.2657e-06, 3.6519e-02, 2.4755e-01,  ..., 2.7120e-01,\n",
      "          3.3269e-01, 2.1432e-01],\n",
      "         [8.5976e-06, 3.2936e-02, 1.7693e-01,  ..., 1.9959e-01,\n",
      "          1.7707e-01, 1.9223e-01]]])\n",
      "torch.Size([1, 40, 251])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "BD=BirdclefDataset(labels_path=labels_path)\n",
    "\n",
    "audio_label,mel_spec=BD[0]\n",
    "\n",
    "print(audio_label)\n",
    "print(mel_spec)\n",
    "print(mel_spec.shape)\n",
    "print(type(mel_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, use dataloader to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that each batch size is 4, the data is shuffled, and two processes are used for loading\n",
    "dataloader = DataLoader(dataset=BD, batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('houspa', 'grywag', 'grewar3', 'blrwar1', 'whiter2', 'grejun2', 'gybpri1', 'bkwsti', 'litegr', 'bcnher', 'hoopoe', 'maltro1', 'hoopoe', 'labcro1', 'pursun3', 'commyn', 'ruftre2', 'gybpri1', 'kenplo1', 'houspa', 'rocpig', 'comior1', 'zitcis1', 'eaywag1', 'putbab1', 'blrwar1', 'graher1', 'zitcis1', 'comior1', 'ingori1', 'whbwoo2', 'houspa', 'commoo3', 'bkwsti', 'stbkin1', 'ashpri1', 'whtkin2', 'pursun3', 'commyn', 'litgre1', 'gargan', 'grehor1', 'goflea1', 'whiter2', 'grtdro1', 'grywag', 'wemhar1', 'blrwar1', 'eaywag1', 'whbsho3', 'stbkin1', 'junbab2', 'whbtre1', 'barswa', 'blrwar1', 'whbwag1', 'graher1', 'rerswa1', 'plapri1', 'lirplo', 'whtkin2', 'gyhcaf1', 'comkin1', 'comsan', 'purher1', 'woosan', 'blakit1', 'ashpri1', 'eaywag1', 'blakit1', 'gyhcaf1', 'woosan', 'cregos1', 'blrwar1', 'sohmyn1', 'bkwsti', 'commoo3', 'comsan', 'greegr', 'comsan', 'eurcoo', 'whcbar1', 'plapri1', 'inbrob1', 'grywag', 'putbab1', 'rorpar', 'grewar3', 'comtai1', 'grnsan', 'commyn', 'bladro1', 'blnmon1', 'ruftre2', 'labcro1', 'brodro1', 'litgre1', 'stbkin1', 'bcnher', 'eucdov', 'gyhcaf1', 'barswa', 'bkwsti', 'comsan', 'litspi1', 'kenplo1', 'brodro1', 'whbsho3', 'eurcoo', 'eaywag1', 'brodro1', 'commyn', 'lblwar1', 'blrwar1', 'whbwag1', 'eurcoo', 'bkskit1', 'spodov', 'litspi1', 'grnwar1', 'blrwar1', 'bkwsti', 'houspa', 'categr', 'blrwar1', 'graher1', 'indrol2', 'rorpar')\n",
      "tensor([[[[1.1519e-01, 4.1341e-02, 1.0100e-01,  ..., 3.7574e-02,\n",
      "           4.8023e-02, 2.0744e-02],\n",
      "          [7.1241e-02, 2.0733e-02, 4.6258e-02,  ..., 2.6796e-02,\n",
      "           2.8547e-02, 2.3174e-02],\n",
      "          [1.1310e-01, 1.8539e-02, 1.9338e-02,  ..., 1.8488e-02,\n",
      "           1.5405e-02, 4.6546e-03],\n",
      "          ...,\n",
      "          [5.0878e-02, 3.6691e-02, 9.4777e-02,  ..., 3.9491e-02,\n",
      "           2.7731e-02, 2.6994e-02],\n",
      "          [3.6751e-03, 2.0729e-03, 5.8213e-03,  ..., 1.5843e-03,\n",
      "           1.5011e-03, 2.8854e-03],\n",
      "          [6.3128e-04, 1.9571e-12, 4.5958e-13,  ..., 2.7104e-13,\n",
      "           2.3292e-13, 1.1079e-03]]],\n",
      "\n",
      "\n",
      "        [[[9.4400e-01, 4.4472e-03, 3.4955e-02,  ..., 1.5412e-01,\n",
      "           3.1990e-02, 4.5297e-01],\n",
      "          [6.3143e-01, 2.5059e-01, 3.2793e-01,  ..., 8.7333e-01,\n",
      "           2.4930e-01, 5.6797e-01],\n",
      "          [1.5479e+00, 7.7528e-01, 6.0180e-01,  ..., 1.9033e+00,\n",
      "           4.3060e-01, 3.7693e-01],\n",
      "          ...,\n",
      "          [2.4270e-01, 3.6425e-01, 2.7325e-01,  ..., 3.8579e-01,\n",
      "           4.0547e-01, 3.8963e-01],\n",
      "          [3.0065e-01, 3.2777e-01, 3.8071e-01,  ..., 4.6617e-01,\n",
      "           4.2009e-01, 3.3416e-01],\n",
      "          [2.2328e-01, 1.9671e-01, 2.2583e-01,  ..., 2.5697e-01,\n",
      "           1.6123e-01, 3.1155e-01]]],\n",
      "\n",
      "\n",
      "        [[[8.6216e+00, 1.2474e+01, 9.0685e+00,  ..., 5.0032e+01,\n",
      "           1.4717e+01, 6.9930e+00],\n",
      "          [5.5366e-01, 9.3663e-01, 1.4710e+00,  ..., 1.2024e+00,\n",
      "           1.8407e+00, 1.9876e-01],\n",
      "          [3.8102e-01, 1.3507e-01, 1.5237e-01,  ..., 4.8148e-02,\n",
      "           9.7072e-02, 8.3435e-02],\n",
      "          ...,\n",
      "          [5.7898e-04, 1.0029e-03, 1.1959e-03,  ..., 7.9585e-04,\n",
      "           6.6628e-04, 1.5661e-03],\n",
      "          [9.7025e-04, 7.8875e-04, 9.3016e-04,  ..., 1.1545e-03,\n",
      "           8.9866e-04, 1.8774e-03],\n",
      "          [4.2858e-04, 6.6540e-04, 5.4906e-04,  ..., 8.0703e-04,\n",
      "           5.3115e-04, 1.2426e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[2.3813e-02, 7.5125e-04, 2.7028e-04,  ..., 2.4930e-04,\n",
      "           6.4308e-04, 3.9031e-02],\n",
      "          [8.3412e-02, 1.8954e-02, 9.0893e-03,  ..., 6.5564e-03,\n",
      "           8.0258e-03, 6.0722e-02],\n",
      "          [4.7651e-02, 2.9823e-02, 8.8934e-02,  ..., 1.7018e-01,\n",
      "           2.2329e-01, 4.8084e-02],\n",
      "          ...,\n",
      "          [1.8848e-02, 1.7603e-02, 2.4206e-02,  ..., 3.1364e-02,\n",
      "           1.8383e-02, 2.5997e-02],\n",
      "          [2.4684e-02, 4.3360e-02, 2.3020e-02,  ..., 3.7922e-02,\n",
      "           3.1581e-02, 2.7071e-02],\n",
      "          [2.2526e-02, 2.5889e-02, 1.8279e-02,  ..., 2.3106e-02,\n",
      "           2.1158e-02, 1.6199e-02]]],\n",
      "\n",
      "\n",
      "        [[[2.6737e+02, 1.8114e+02, 1.9482e+01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.5236e+02, 1.2079e+02, 5.3218e+01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [8.2060e+01, 1.5595e+01, 1.8740e+01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.9235e-01, 2.1361e-01, 1.9073e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.3570e-01, 2.1573e-01, 2.3186e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.0022e-01, 1.3348e-01, 1.4244e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[2.2575e+00, 1.7884e+00, 4.6888e+00,  ..., 2.7484e+00,\n",
      "           2.9791e+00, 2.1402e+00],\n",
      "          [2.2626e-01, 1.8007e-01, 1.4765e-01,  ..., 6.7562e-01,\n",
      "           5.1030e-01, 1.2970e+00],\n",
      "          [3.1816e-01, 4.6081e-01, 8.9723e-01,  ..., 7.6849e-01,\n",
      "           6.7663e-01, 4.8675e-01],\n",
      "          ...,\n",
      "          [1.8284e-02, 1.0327e-02, 1.5720e-02,  ..., 2.6621e-02,\n",
      "           2.4635e-02, 1.5394e-02],\n",
      "          [2.6248e-02, 1.4202e-02, 1.7180e-02,  ..., 1.9996e-02,\n",
      "           1.9166e-02, 1.7407e-02],\n",
      "          [1.3707e-02, 1.1671e-02, 1.3174e-02,  ..., 8.4502e-03,\n",
      "           1.3776e-02, 1.5075e-02]]]])\n",
      "torch.Size([128, 1, 40, 251])\n"
     ]
    }
   ],
   "source": [
    "# iterrate DataLoader\n",
    "for batch in dataloader:\n",
    "    labels, mel_specs = batch\n",
    "    # Here you can process your batch data\n",
    "    print(labels)\n",
    "    print(mel_specs)\n",
    "    print(mel_specs.shape)\n",
    "    break\n",
    "\n",
    "# mel_specs.shape -> [batch_size,channel_num,height,width]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('grywag', 'comsan', 'eucdov', 'grewar3', 'litswi1', 'blrwar1', 'grewar3', 'blrwar1', 'bladro1', 'rorpar', 'ingori1', 'putbab1', 'comgre', 'bkwsti', 'whbtre1', 'gyhcaf1', 'placuc3', 'houspa', 'houspa', 'woosan', 'litswi1', 'gloibi', 'sbeowl1', 'rufwoo2', 'woosan', 'wemhar1', 'grefla1', 'brnhao1', 'putbab1', 'commyn', 'blakit1', 'crseag1', 'houspa', 'wemhar1', 'asbfly', 'copbar1', 'blrwar1', 'comtai1', 'rorpar', 'zitcis1', 'gryfra', 'labcro1', 'stbkin1', 'graher1', 'ashdro1', 'bkwsti', 'zitcis1', 'cregos1', 'blrwar1', 'graher1', 'litspi1', 'litegr', 'wemhar1', 'inbrob1', 'gloibi', 'bkskit1', 'asbfly', 'blrwar1', 'eucdov', 'asikoe2', 'eurcoo', 'lirplo', 'insowl1', 'comsan', 'whcbar1', 'houspa', 'blakit1', 'placuc3', 'blrwar1', 'grtdro1', 'blrwar1', 'houspa', 'emedov2', 'comgre', 'litegr', 'whiter2', 'commyn', 'spepic1', 'lewduc1', 'blrwar1', 'gyhcaf1', 'piekin1', 'comros', 'piekin1', 'compea', 'houspa', 'commoo3', 'comros', 'eucdov', 'whbwat1', 'zitcis1', 'labcro1', 'grywag', 'blrwar1', 'wbbfly1', 'crseag1', 'commoo3', 'rorpar', 'blrwar1', 'gargan', 'brcful1', 'blnmon1', 'hoopoe', 'eucdov', 'blrwar1', 'bcnher', 'eaywag1', 'bcnher', 'hoopoe', 'eucdov', 'brwowl1', 'houspa', 'bkskit1', 'commoo3', 'blrwar1', 'grewar3', 'brakit1', 'comsan', 'pursun3', 'comgre', 'zitcis1', 'eucdov', 'comkin1', 'comkin1', 'cohcuc1', 'commyn', 'grewar3', 'commyn')\n",
      "tensor([[[[1.7461e-01, 2.3862e-01, 1.4520e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.0383e+00, 4.9813e-01, 6.3482e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.7491e-01, 1.0149e+00, 3.5153e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.6572e-01, 8.5010e-02, 1.2120e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.2762e-02, 1.0191e-01, 1.4232e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.9934e-02, 6.6565e-02, 7.5126e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[8.3803e-01, 4.3563e-01, 2.7942e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.5786e-01, 6.7813e-01, 7.0957e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.5455e-01, 4.0627e-01, 6.2396e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [3.4062e-01, 3.7667e-01, 6.5230e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1100e-01, 3.9205e-01, 7.2724e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.9298e-01, 3.3537e-01, 4.3097e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[1.8454e+02, 1.4876e+02, 1.0986e+02,  ..., 6.7640e+01,\n",
      "           3.8870e+02, 4.4366e+02],\n",
      "          [1.3453e+02, 6.1917e+01, 5.2061e+01,  ..., 4.2966e+01,\n",
      "           4.2945e+01, 1.0462e+02],\n",
      "          [5.1422e+01, 2.3996e+01, 4.1538e+01,  ..., 1.2632e+01,\n",
      "           7.0749e+00, 1.2686e+01],\n",
      "          ...,\n",
      "          [2.6839e+00, 2.8041e+00, 1.7938e+00,  ..., 1.7491e+00,\n",
      "           3.0129e+00, 1.5609e+00],\n",
      "          [2.1881e+00, 2.3240e+00, 1.1609e+00,  ..., 1.7627e+00,\n",
      "           1.9488e+00, 1.0098e+00],\n",
      "          [2.2955e+00, 9.4140e-01, 1.1785e+00,  ..., 1.0857e+00,\n",
      "           1.4825e+00, 8.9260e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[6.3979e+00, 1.7271e+01, 2.7519e+01,  ..., 1.7153e+00,\n",
      "           5.9812e+00, 1.1447e+01],\n",
      "          [2.9385e+01, 1.2310e+02, 4.0821e+01,  ..., 3.3543e+01,\n",
      "           7.1083e+01, 2.2191e+01],\n",
      "          [1.2865e+02, 5.6389e+01, 2.6042e+01,  ..., 4.5257e+01,\n",
      "           6.5410e+01, 2.6958e+01],\n",
      "          ...,\n",
      "          [9.5425e-01, 6.3323e-01, 5.0147e-01,  ..., 2.0218e-02,\n",
      "           1.9674e-02, 2.2201e-02],\n",
      "          [5.8520e-01, 3.2194e-01, 3.1801e-01,  ..., 2.5673e-02,\n",
      "           2.3906e-02, 1.8836e-02],\n",
      "          [2.0024e-01, 1.8638e-01, 1.8547e-01,  ..., 1.8218e-02,\n",
      "           1.3866e-02, 1.6085e-02]]],\n",
      "\n",
      "\n",
      "        [[[9.7730e-03, 6.9282e-02, 2.1295e-02,  ..., 6.6513e-02,\n",
      "           3.4501e-02, 1.0545e-02],\n",
      "          [1.5380e-02, 7.6995e-02, 3.7028e-02,  ..., 3.0591e-02,\n",
      "           2.5422e-02, 2.1106e-03],\n",
      "          [4.8854e-03, 2.7325e-02, 1.5493e-01,  ..., 5.0831e-02,\n",
      "           1.0962e-02, 2.3794e-02],\n",
      "          ...,\n",
      "          [1.9276e-03, 2.5824e-03, 3.3073e-03,  ..., 5.3347e-03,\n",
      "           3.6274e-03, 6.2378e-03],\n",
      "          [2.1258e-03, 2.1366e-03, 2.0671e-03,  ..., 5.2617e-03,\n",
      "           4.0422e-03, 5.0285e-03],\n",
      "          [8.3341e-04, 1.1488e-03, 1.1485e-03,  ..., 4.5065e-03,\n",
      "           2.0564e-03, 2.5149e-03]]],\n",
      "\n",
      "\n",
      "        [[[1.0837e-01, 1.3355e-01, 1.1623e-01,  ..., 1.7326e-02,\n",
      "           7.7431e-03, 3.7605e-03],\n",
      "          [4.7047e-02, 6.5596e-02, 1.2234e-01,  ..., 1.9342e-02,\n",
      "           4.9049e-03, 5.0207e-03],\n",
      "          [4.2418e-02, 3.9669e-02, 5.4146e-02,  ..., 8.4390e-03,\n",
      "           5.5144e-03, 5.4651e-03],\n",
      "          ...,\n",
      "          [1.9410e-05, 1.5470e-05, 1.9289e-05,  ..., 1.0604e-05,\n",
      "           1.4684e-05, 2.1534e-05],\n",
      "          [1.7525e-05, 1.9638e-05, 1.8760e-05,  ..., 1.3785e-05,\n",
      "           1.1560e-05, 1.4981e-05],\n",
      "          [2.0472e-05, 1.4277e-05, 1.4707e-05,  ..., 1.2037e-05,\n",
      "           8.8833e-06, 8.7347e-06]]]])\n",
      "torch.Size([128, 1, 40, 251])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "labels, mel_specs = batch\n",
    "print(labels)\n",
    "print(mel_specs)\n",
    "print(mel_specs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release\n",
    "\n",
    "del batch, dataloader,labels,mel_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add label encoding\n",
    "\n",
    "Note that in the batch output, the labels are still in string format. As the input to the model, I need to encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize label encoder\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malpar1', 'litgre1', 'houspa', 'indrob1', 'comtai1', 'grynig2', 'rufwoo2', 'yebbul3', 'indpit1', 'gyhcaf1', 'ruftre2', 'wynlau1', 'inpher1', 'comkin1', 'comior1', 'tibfly3', 'pomgrp2', 'oripip1', 'indtit1', 'nutman', 'junmyn1', 'rutfly6', 'goflea1', 'litegr', 'lesyel1', 'lewduc1', 'inbrob1', 'crfbar1', 'scamin3', 'shikra1', 'gryfra', 'commoo3', 'grewar3', 'brodro1', 'rocpig', 'categr', 'ingori1', 'plhpar1', 'sbeowl1', 'bwfshr1', 'junowl1', 'orihob2', 'greegr', 'barswa', 'paisto1', 'moipig1', 'plapri1', 'forwag1', 'maghor2', 'brasta1', 'lirplo', 'grecou1', 'kenplo1', 'bkcbul1', 'grbeat1', 'junbab2', 'comsan', 'whbtre1', 'brnhao1', 'brcful1', 'whcbar1', 'hoopoe', 'plaflo1', 'maltro1', 'piekin1', 'brnshr', 'whiter2', 'brfowl1', 'pursun4', 'grehor1', 'pursun3', 'purswa3', 'yebbab1', 'lblwar1', 'malwoo1', 'laudov1', 'grenig1', 'tilwar1', 'heswoo1', 'compea', 'putbab1', 'smamin1', 'rorpar', 'graher1', 'ashpri1', 'piebus1', 'grnwar1', 'eurbla2', 'asikoe2', 'whbwat1', 'sqtbul1', 'brwowl1', 'bncwoo3', 'ashwoo2', 'pabflo1', 'eaywag1', 'ashdro1', 'rerswa1', 'emedov2', 'houcro1', 'btbeat1', 'gargan', 'insowl1', 'bcnher', 'placuc3', 'blaeag1', 'barfly1', 'insbab1', 'rewbul', 'grefla1', 'spoowl1', 'marsan', 'whbsho3', 'kerlau2', 'asiope1', 'redspu1', 'chbeat1', 'blhori1', 'integr', 'zitcis1', 'blakit1', 'grywag', 'crseag1', 'labcro1', 'litspi1', 'spepic1', 'comgre', 'vehpar1', 'gloibi', 'whbwag1', 'jerbus2', 'stbkin1', 'copbar1', 'whtkin2', 'blrwar1', 'rewlap1', 'woosan', 'vefnut1', 'mawthr1', 'isbduc1', 'grtdro1', 'bladro1', 'thbwar1', 'aspswi1', 'aspfly1', 'revbul', 'eucdov', 'asbfly', 'whrmun', 'nilfly2', 'wbbfly1', 'crbsun2', 'commyn', 'purher1', 'bkwsti', 'wemhar1', 'bkskit1', 'litswi1', 'sohmyn1', 'dafbab1', 'eurcoo', 'whbbul2', 'darter2', 'rufbab3', 'whbwoo2', 'sttwoo1', 'gybpri1', 'niwpig1', 'brakit1', 'grnsan', 'grejun2', 'cohcuc1', 'comros', 'brwjac1', 'comfla1', 'spodov', 'blnmon1', 'lobsun2', 'rossta2', 'bkrfla1', 'indrol2', 'cregos1']\n",
      "182\n",
      "[112 107  82  85  45  75 147 180  84  78 146 178  89  39  38 161 131 122\n",
      "  87 120  96 148  61 106 103 104  83  49 150 151  74  40  70  28 142  33\n",
      "  88 130 149  32  97 121  65   9 124 117 129  58 111  23 105  64  98  11\n",
      "  63  95  44 169  26  24 173  80 128 113 126  27 174  25 134  67 133 135\n",
      " 179 102 114 101  69 162  79  42 136 152 143  62   2 125  72  56   4 171\n",
      " 157  30  21   3 123  53   1 138  54  81  31  59  91  10 127  16   8  90\n",
      " 140  66 156 115 168  99   5 137  34  18  92 181  17  76  50 100 108 154\n",
      "  37 164  60 170  94 158  46 176  20 141 177 163 116  93  73  15 160   7\n",
      "   6 139  55   0 175 118 165  47  41 132  14 166  13 109 153  51  57 167\n",
      "  52 145 172 159  77 119  22  71  68  35  43  29  36 155  19 110 144  12\n",
      "  86  48]\n",
      "{'asbfly': 0, 'ashdro1': 1, 'ashpri1': 2, 'ashwoo2': 3, 'asikoe2': 4, 'asiope1': 5, 'aspfly1': 6, 'aspswi1': 7, 'barfly1': 8, 'barswa': 9, 'bcnher': 10, 'bkcbul1': 11, 'bkrfla1': 12, 'bkskit1': 13, 'bkwsti': 14, 'bladro1': 15, 'blaeag1': 16, 'blakit1': 17, 'blhori1': 18, 'blnmon1': 19, 'blrwar1': 20, 'bncwoo3': 21, 'brakit1': 22, 'brasta1': 23, 'brcful1': 24, 'brfowl1': 25, 'brnhao1': 26, 'brnshr': 27, 'brodro1': 28, 'brwjac1': 29, 'brwowl1': 30, 'btbeat1': 31, 'bwfshr1': 32, 'categr': 33, 'chbeat1': 34, 'cohcuc1': 35, 'comfla1': 36, 'comgre': 37, 'comior1': 38, 'comkin1': 39, 'commoo3': 40, 'commyn': 41, 'compea': 42, 'comros': 43, 'comsan': 44, 'comtai1': 45, 'copbar1': 46, 'crbsun2': 47, 'cregos1': 48, 'crfbar1': 49, 'crseag1': 50, 'dafbab1': 51, 'darter2': 52, 'eaywag1': 53, 'emedov2': 54, 'eucdov': 55, 'eurbla2': 56, 'eurcoo': 57, 'forwag1': 58, 'gargan': 59, 'gloibi': 60, 'goflea1': 61, 'graher1': 62, 'grbeat1': 63, 'grecou1': 64, 'greegr': 65, 'grefla1': 66, 'grehor1': 67, 'grejun2': 68, 'grenig1': 69, 'grewar3': 70, 'grnsan': 71, 'grnwar1': 72, 'grtdro1': 73, 'gryfra': 74, 'grynig2': 75, 'grywag': 76, 'gybpri1': 77, 'gyhcaf1': 78, 'heswoo1': 79, 'hoopoe': 80, 'houcro1': 81, 'houspa': 82, 'inbrob1': 83, 'indpit1': 84, 'indrob1': 85, 'indrol2': 86, 'indtit1': 87, 'ingori1': 88, 'inpher1': 89, 'insbab1': 90, 'insowl1': 91, 'integr': 92, 'isbduc1': 93, 'jerbus2': 94, 'junbab2': 95, 'junmyn1': 96, 'junowl1': 97, 'kenplo1': 98, 'kerlau2': 99, 'labcro1': 100, 'laudov1': 101, 'lblwar1': 102, 'lesyel1': 103, 'lewduc1': 104, 'lirplo': 105, 'litegr': 106, 'litgre1': 107, 'litspi1': 108, 'litswi1': 109, 'lobsun2': 110, 'maghor2': 111, 'malpar1': 112, 'maltro1': 113, 'malwoo1': 114, 'marsan': 115, 'mawthr1': 116, 'moipig1': 117, 'nilfly2': 118, 'niwpig1': 119, 'nutman': 120, 'orihob2': 121, 'oripip1': 122, 'pabflo1': 123, 'paisto1': 124, 'piebus1': 125, 'piekin1': 126, 'placuc3': 127, 'plaflo1': 128, 'plapri1': 129, 'plhpar1': 130, 'pomgrp2': 131, 'purher1': 132, 'pursun3': 133, 'pursun4': 134, 'purswa3': 135, 'putbab1': 136, 'redspu1': 137, 'rerswa1': 138, 'revbul': 139, 'rewbul': 140, 'rewlap1': 141, 'rocpig': 142, 'rorpar': 143, 'rossta2': 144, 'rufbab3': 145, 'ruftre2': 146, 'rufwoo2': 147, 'rutfly6': 148, 'sbeowl1': 149, 'scamin3': 150, 'shikra1': 151, 'smamin1': 152, 'sohmyn1': 153, 'spepic1': 154, 'spodov': 155, 'spoowl1': 156, 'sqtbul1': 157, 'stbkin1': 158, 'sttwoo1': 159, 'thbwar1': 160, 'tibfly3': 161, 'tilwar1': 162, 'vefnut1': 163, 'vehpar1': 164, 'wbbfly1': 165, 'wemhar1': 166, 'whbbul2': 167, 'whbsho3': 168, 'whbtre1': 169, 'whbwag1': 170, 'whbwat1': 171, 'whbwoo2': 172, 'whcbar1': 173, 'whiter2': 174, 'whrmun': 175, 'whtkin2': 176, 'woosan': 177, 'wynlau1': 178, 'yebbab1': 179, 'yebbul3': 180, 'zitcis1': 181}\n"
     ]
    }
   ],
   "source": [
    "raw_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "labels_all=raw_df.primary_label.unique().tolist()\n",
    "\n",
    "print(labels_all)\n",
    "print(len(labels_all))\n",
    "\n",
    "labels_encoded=encoder.fit_transform(labels_all)\n",
    "\n",
    "print(labels_encoded)\n",
    "\n",
    "# If needed, you can view the mapping of original labels to encodings\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing._label.LabelEncoder'>\n"
     ]
    }
   ],
   "source": [
    "print(type(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release\n",
    "\n",
    "del raw_df,labels_all,labels_encoded,label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert audio data into mel spectrogram\n",
    "\n",
    "\n",
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.02,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    n_fft = int(window_size * sample_rate)  \n",
    "    hop_length = int(hop_size * sample_rate)  \n",
    "\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 encoder:LabelEncoder,\n",
    "                 audio_dir:str='../../data/train_audio',\n",
    "                 labels_path:str=None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            encoder: label encoder\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.audio_dir=audio_dir\n",
    "        # read data into dataframe\n",
    "        self.labels_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "\n",
    "    def get_audio_path(self,index) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "        # Get the child path of audio from labels_df\n",
    "        audio_child_path=self.labels_df['filename'].iloc[index]\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,audio_child_path)\n",
    "    \n",
    "\n",
    "    def get_audio_label(self,index)->str:\n",
    "        '''\n",
    "        According to the provided index, get the corresponding label from the train metadata file\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "        '''\n",
    "\n",
    "        return self.labels_df['primary_label'].iloc[index]\n",
    "    \n",
    "\n",
    "    def target_clip(self,index:int,audio:torch.Tensor,sample_rate:int, duration_seconds:float)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip \n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "            duration_seconds: audio duration in seconds\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time=self.labels_df['clip_start_time'].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time<=duration_seconds:\n",
    "            clip_start_point=clip_start_time*sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point+segment_duration<=total_samples:\n",
    "                clip=audio[:, clip_start_point:clip_start_point + segment_duration]\n",
    "\n",
    "            # Not long enough, a mask is needed\n",
    "            else:\n",
    "                padding_length = clip_start_point+segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "\n",
    "                clip=torch.cat((audio[:,clip_start_point:],silence),dim=1)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('The clip start time is out of raw audio length')\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "    \n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitems__(1)\n",
    "        # Get the path of a single audio file\n",
    "        single_audio_dir=self.get_audio_path(index)\n",
    "        # Get the corresponding label value\n",
    "        audio_label=self.encoder.transform([self.get_audio_label(index)])[0]\n",
    "\n",
    "        # Read audio array according to path\n",
    "        audio, sr=read_audio(single_audio_dir)\n",
    "        \n",
    "        # Read the duration and number of channels corresponding to the audio\n",
    "        duration_seconds, num_channels=audio_info(audio,sample_rate=sr)\n",
    "\n",
    "        # Get the audio clip corresponding to index\n",
    "        clip=self.target_clip(index,audio,sample_rate=sr, duration_seconds=duration_seconds)\n",
    "\n",
    "        # mel spectrogram transformation\n",
    "        mel_spec=mel_transform(sample_rate=sr,audio=clip)\n",
    "\n",
    "        return audio_label, mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 62,  55,  62, 139,  36,  35,  57,  41,  45, 153,  80,  15,  74, 164,\n",
      "        100, 105, 127,  82,  43,  43, 105,  64,  14,  20, 139, 181,  20, 136,\n",
      "         44, 166,  57, 122, 138,  41,  80,  18,  41,  41,  20, 109,  58,  38,\n",
      "        181,  10,  62,  70,  80,  20,  62,  13,  70,  45, 105, 143,  43,  20,\n",
      "         55,  90,  20,  80,  41,  55,  15, 161,  38, 115,  10, 171,  20,  40,\n",
      "        105,  32,  80, 137, 136,  78,  98,  44,  72,  57, 132, 167,  70,  55,\n",
      "         20,  20, 168,  69,  10, 129,  53,  78, 106,  62,  10, 129,   9,  10,\n",
      "        181, 100,  57,  20,  44,  40,  73, 107,  53,  62,  20, 166,  33,  18,\n",
      "         44, 101, 138,  70,  98, 158,  57,  70,  82, 141,  20, 161, 177,   9,\n",
      "         40, 138])\n",
      "tensor([[[[3.4798e-02, 7.2094e-07, 1.3548e-06,  ..., 4.9665e-07,\n",
      "           1.7406e-07, 6.0493e-05],\n",
      "          [3.7928e-02, 7.6341e-06, 9.7611e-06,  ..., 5.5148e-06,\n",
      "           2.5833e-06, 9.8878e-05],\n",
      "          [4.1640e-02, 3.9300e-05, 1.4848e-04,  ..., 8.8981e-05,\n",
      "           1.6678e-04, 3.2268e-04],\n",
      "          ...,\n",
      "          [1.0865e-02, 3.1186e-03, 3.7998e-03,  ..., 3.2889e-03,\n",
      "           4.0502e-03, 2.6159e-03],\n",
      "          [1.1949e-02, 3.5399e-03, 2.9912e-03,  ..., 3.8148e-03,\n",
      "           3.1082e-03, 2.0310e-03],\n",
      "          [6.9705e-03, 2.7229e-03, 1.2705e-03,  ..., 2.1718e-03,\n",
      "           1.7983e-03, 2.3796e-03]]],\n",
      "\n",
      "\n",
      "        [[[5.5982e+00, 7.0708e-06, 1.5866e-06,  ..., 2.8558e-06,\n",
      "           2.4344e-05, 1.4521e+00],\n",
      "          [6.6293e+00, 3.4579e-05, 5.0718e-05,  ..., 2.7460e-04,\n",
      "           3.2626e-04, 1.7650e+00],\n",
      "          [8.0506e+00, 4.8768e-03, 3.5220e-03,  ..., 1.0314e-02,\n",
      "           2.4039e-02, 2.7868e+00],\n",
      "          ...,\n",
      "          [5.6191e-02, 4.6178e-02, 3.8355e-02,  ..., 4.1736e-02,\n",
      "           4.2228e-02, 2.9492e-02],\n",
      "          [5.2132e-02, 5.7542e-02, 4.9706e-02,  ..., 3.6514e-02,\n",
      "           3.0528e-02, 4.4726e-02],\n",
      "          [2.5240e-02, 3.4559e-02, 3.0979e-02,  ..., 3.7370e-02,\n",
      "           2.0442e-02, 3.0922e-02]]],\n",
      "\n",
      "\n",
      "        [[[6.9539e-05, 7.1779e-08, 7.9998e-08,  ..., 3.2923e-06,\n",
      "           4.1541e-06, 3.4699e-03],\n",
      "          [7.8654e-05, 1.6913e-05, 1.5058e-05,  ..., 5.8264e-05,\n",
      "           1.2607e-04, 5.1790e-03],\n",
      "          [1.0472e-04, 2.7293e-04, 3.3775e-05,  ..., 3.7305e-04,\n",
      "           2.8363e-04, 3.3725e-03],\n",
      "          ...,\n",
      "          [4.7327e-03, 2.9592e-03, 2.1469e-03,  ..., 7.5286e-03,\n",
      "           4.2986e-03, 8.7331e-03],\n",
      "          [3.2581e-03, 3.7104e-03, 2.3038e-03,  ..., 3.9105e-03,\n",
      "           4.9215e-03, 7.1212e-03],\n",
      "          [1.8550e-03, 2.3993e-03, 1.4678e-03,  ..., 2.5426e-03,\n",
      "           2.0754e-03, 5.2303e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.2492e-02, 3.6730e-05, 1.6042e-04,  ..., 1.2013e-05,\n",
      "           1.0854e-04, 7.9571e-03],\n",
      "          [1.4524e-02, 6.3984e-04, 1.3708e-03,  ..., 2.2141e-04,\n",
      "           6.1373e-04, 1.9332e-02],\n",
      "          [3.4118e-02, 1.6143e-02, 8.4029e-03,  ..., 2.6888e-03,\n",
      "           3.4470e-03, 3.1436e-02],\n",
      "          ...,\n",
      "          [3.9031e-03, 2.9933e-03, 3.3226e-03,  ..., 3.7783e-03,\n",
      "           4.1126e-03, 5.4040e-03],\n",
      "          [2.9144e-03, 2.5620e-03, 3.9942e-03,  ..., 3.9301e-03,\n",
      "           3.6189e-03, 3.1624e-03],\n",
      "          [3.5815e-03, 1.8950e-03, 1.8476e-03,  ..., 3.6452e-03,\n",
      "           2.3474e-03, 2.1449e-03]]],\n",
      "\n",
      "\n",
      "        [[[6.0525e+02, 5.7768e+02, 5.8541e+02,  ..., 5.8669e+02,\n",
      "           5.9721e+02, 5.8172e+02],\n",
      "          [4.0665e-01, 1.1539e+00, 2.7896e-01,  ..., 1.3667e+00,\n",
      "           8.5724e-01, 2.1057e+00],\n",
      "          [7.0656e-01, 1.0604e+00, 7.0467e-01,  ..., 1.7142e+00,\n",
      "           1.5331e+00, 3.5518e+00],\n",
      "          ...,\n",
      "          [5.2278e-02, 6.1770e-02, 5.7879e-02,  ..., 5.8706e-02,\n",
      "           6.0879e-02, 6.2716e-02],\n",
      "          [4.5143e-02, 5.2705e-02, 6.0350e-02,  ..., 7.4764e-02,\n",
      "           5.3838e-02, 3.8758e-02],\n",
      "          [3.0387e-02, 2.7504e-02, 3.4354e-02,  ..., 3.7489e-02,\n",
      "           3.9765e-02, 2.4355e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.2246e-03, 2.2719e-09, 3.0810e-09,  ..., 3.3117e-09,\n",
      "           2.7228e-09, 1.2098e-03],\n",
      "          [1.4629e-03, 1.5251e-07, 1.9935e-06,  ..., 6.2353e-07,\n",
      "           1.0160e-07, 1.3787e-03],\n",
      "          [1.8217e-03, 1.4127e-05, 1.5832e-05,  ..., 1.8341e-05,\n",
      "           1.6896e-05, 1.7857e-03],\n",
      "          ...,\n",
      "          [1.5786e-02, 7.2039e-03, 8.8186e-03,  ..., 1.2118e-02,\n",
      "           1.5645e-02, 1.3581e-02],\n",
      "          [1.1660e-02, 1.0133e-02, 1.0205e-02,  ..., 1.0153e-02,\n",
      "           1.5139e-02, 1.1246e-02],\n",
      "          [1.1018e-02, 5.7478e-03, 5.6129e-03,  ..., 6.6018e-03,\n",
      "           7.3139e-03, 5.2484e-03]]]])\n",
      "torch.Size([128, 1, 40, 251])\n"
     ]
    }
   ],
   "source": [
    "BD=BirdclefDataset(encoder=encoder,labels_path=labels_path)\n",
    "\n",
    "dataloader = DataLoader(dataset=BD, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "labels, mel_specs = batch\n",
    "print(labels)\n",
    "print(mel_specs)\n",
    "print(mel_specs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder, batch, dataloader,labels,mel_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then I need to normalize a single clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize label encoder\n",
    "\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malpar1', 'litgre1', 'houspa', 'indrob1', 'comtai1', 'grynig2', 'rufwoo2', 'yebbul3', 'indpit1', 'gyhcaf1', 'ruftre2', 'wynlau1', 'inpher1', 'comkin1', 'comior1', 'tibfly3', 'pomgrp2', 'oripip1', 'indtit1', 'nutman', 'junmyn1', 'rutfly6', 'goflea1', 'litegr', 'lesyel1', 'lewduc1', 'inbrob1', 'crfbar1', 'scamin3', 'shikra1', 'gryfra', 'commoo3', 'grewar3', 'brodro1', 'rocpig', 'categr', 'ingori1', 'plhpar1', 'sbeowl1', 'bwfshr1', 'junowl1', 'orihob2', 'greegr', 'barswa', 'paisto1', 'moipig1', 'plapri1', 'forwag1', 'maghor2', 'brasta1', 'lirplo', 'grecou1', 'kenplo1', 'bkcbul1', 'grbeat1', 'junbab2', 'comsan', 'whbtre1', 'brnhao1', 'brcful1', 'whcbar1', 'hoopoe', 'plaflo1', 'maltro1', 'piekin1', 'brnshr', 'whiter2', 'brfowl1', 'pursun4', 'grehor1', 'pursun3', 'purswa3', 'yebbab1', 'lblwar1', 'malwoo1', 'laudov1', 'grenig1', 'tilwar1', 'heswoo1', 'compea', 'putbab1', 'smamin1', 'rorpar', 'graher1', 'ashpri1', 'piebus1', 'grnwar1', 'eurbla2', 'asikoe2', 'whbwat1', 'sqtbul1', 'brwowl1', 'bncwoo3', 'ashwoo2', 'pabflo1', 'eaywag1', 'ashdro1', 'rerswa1', 'emedov2', 'houcro1', 'btbeat1', 'gargan', 'insowl1', 'bcnher', 'placuc3', 'blaeag1', 'barfly1', 'insbab1', 'rewbul', 'grefla1', 'spoowl1', 'marsan', 'whbsho3', 'kerlau2', 'asiope1', 'redspu1', 'chbeat1', 'blhori1', 'integr', 'zitcis1', 'blakit1', 'grywag', 'crseag1', 'labcro1', 'litspi1', 'spepic1', 'comgre', 'vehpar1', 'gloibi', 'whbwag1', 'jerbus2', 'stbkin1', 'copbar1', 'whtkin2', 'blrwar1', 'rewlap1', 'woosan', 'vefnut1', 'mawthr1', 'isbduc1', 'grtdro1', 'bladro1', 'thbwar1', 'aspswi1', 'aspfly1', 'revbul', 'eucdov', 'asbfly', 'whrmun', 'nilfly2', 'wbbfly1', 'crbsun2', 'commyn', 'purher1', 'bkwsti', 'wemhar1', 'bkskit1', 'litswi1', 'sohmyn1', 'dafbab1', 'eurcoo', 'whbbul2', 'darter2', 'rufbab3', 'whbwoo2', 'sttwoo1', 'gybpri1', 'niwpig1', 'brakit1', 'grnsan', 'grejun2', 'cohcuc1', 'comros', 'brwjac1', 'comfla1', 'spodov', 'blnmon1', 'lobsun2', 'rossta2', 'bkrfla1', 'indrol2', 'cregos1']\n",
      "182\n",
      "[112 107  82  85  45  75 147 180  84  78 146 178  89  39  38 161 131 122\n",
      "  87 120  96 148  61 106 103 104  83  49 150 151  74  40  70  28 142  33\n",
      "  88 130 149  32  97 121  65   9 124 117 129  58 111  23 105  64  98  11\n",
      "  63  95  44 169  26  24 173  80 128 113 126  27 174  25 134  67 133 135\n",
      " 179 102 114 101  69 162  79  42 136 152 143  62   2 125  72  56   4 171\n",
      " 157  30  21   3 123  53   1 138  54  81  31  59  91  10 127  16   8  90\n",
      " 140  66 156 115 168  99   5 137  34  18  92 181  17  76  50 100 108 154\n",
      "  37 164  60 170  94 158  46 176  20 141 177 163 116  93  73  15 160   7\n",
      "   6 139  55   0 175 118 165  47  41 132  14 166  13 109 153  51  57 167\n",
      "  52 145 172 159  77 119  22  71  68  35  43  29  36 155  19 110 144  12\n",
      "  86  48]\n",
      "{'asbfly': 0, 'ashdro1': 1, 'ashpri1': 2, 'ashwoo2': 3, 'asikoe2': 4, 'asiope1': 5, 'aspfly1': 6, 'aspswi1': 7, 'barfly1': 8, 'barswa': 9, 'bcnher': 10, 'bkcbul1': 11, 'bkrfla1': 12, 'bkskit1': 13, 'bkwsti': 14, 'bladro1': 15, 'blaeag1': 16, 'blakit1': 17, 'blhori1': 18, 'blnmon1': 19, 'blrwar1': 20, 'bncwoo3': 21, 'brakit1': 22, 'brasta1': 23, 'brcful1': 24, 'brfowl1': 25, 'brnhao1': 26, 'brnshr': 27, 'brodro1': 28, 'brwjac1': 29, 'brwowl1': 30, 'btbeat1': 31, 'bwfshr1': 32, 'categr': 33, 'chbeat1': 34, 'cohcuc1': 35, 'comfla1': 36, 'comgre': 37, 'comior1': 38, 'comkin1': 39, 'commoo3': 40, 'commyn': 41, 'compea': 42, 'comros': 43, 'comsan': 44, 'comtai1': 45, 'copbar1': 46, 'crbsun2': 47, 'cregos1': 48, 'crfbar1': 49, 'crseag1': 50, 'dafbab1': 51, 'darter2': 52, 'eaywag1': 53, 'emedov2': 54, 'eucdov': 55, 'eurbla2': 56, 'eurcoo': 57, 'forwag1': 58, 'gargan': 59, 'gloibi': 60, 'goflea1': 61, 'graher1': 62, 'grbeat1': 63, 'grecou1': 64, 'greegr': 65, 'grefla1': 66, 'grehor1': 67, 'grejun2': 68, 'grenig1': 69, 'grewar3': 70, 'grnsan': 71, 'grnwar1': 72, 'grtdro1': 73, 'gryfra': 74, 'grynig2': 75, 'grywag': 76, 'gybpri1': 77, 'gyhcaf1': 78, 'heswoo1': 79, 'hoopoe': 80, 'houcro1': 81, 'houspa': 82, 'inbrob1': 83, 'indpit1': 84, 'indrob1': 85, 'indrol2': 86, 'indtit1': 87, 'ingori1': 88, 'inpher1': 89, 'insbab1': 90, 'insowl1': 91, 'integr': 92, 'isbduc1': 93, 'jerbus2': 94, 'junbab2': 95, 'junmyn1': 96, 'junowl1': 97, 'kenplo1': 98, 'kerlau2': 99, 'labcro1': 100, 'laudov1': 101, 'lblwar1': 102, 'lesyel1': 103, 'lewduc1': 104, 'lirplo': 105, 'litegr': 106, 'litgre1': 107, 'litspi1': 108, 'litswi1': 109, 'lobsun2': 110, 'maghor2': 111, 'malpar1': 112, 'maltro1': 113, 'malwoo1': 114, 'marsan': 115, 'mawthr1': 116, 'moipig1': 117, 'nilfly2': 118, 'niwpig1': 119, 'nutman': 120, 'orihob2': 121, 'oripip1': 122, 'pabflo1': 123, 'paisto1': 124, 'piebus1': 125, 'piekin1': 126, 'placuc3': 127, 'plaflo1': 128, 'plapri1': 129, 'plhpar1': 130, 'pomgrp2': 131, 'purher1': 132, 'pursun3': 133, 'pursun4': 134, 'purswa3': 135, 'putbab1': 136, 'redspu1': 137, 'rerswa1': 138, 'revbul': 139, 'rewbul': 140, 'rewlap1': 141, 'rocpig': 142, 'rorpar': 143, 'rossta2': 144, 'rufbab3': 145, 'ruftre2': 146, 'rufwoo2': 147, 'rutfly6': 148, 'sbeowl1': 149, 'scamin3': 150, 'shikra1': 151, 'smamin1': 152, 'sohmyn1': 153, 'spepic1': 154, 'spodov': 155, 'spoowl1': 156, 'sqtbul1': 157, 'stbkin1': 158, 'sttwoo1': 159, 'thbwar1': 160, 'tibfly3': 161, 'tilwar1': 162, 'vefnut1': 163, 'vehpar1': 164, 'wbbfly1': 165, 'wemhar1': 166, 'whbbul2': 167, 'whbsho3': 168, 'whbtre1': 169, 'whbwag1': 170, 'whbwat1': 171, 'whbwoo2': 172, 'whcbar1': 173, 'whiter2': 174, 'whrmun': 175, 'whtkin2': 176, 'woosan': 177, 'wynlau1': 178, 'yebbab1': 179, 'yebbul3': 180, 'zitcis1': 181}\n"
     ]
    }
   ],
   "source": [
    "raw_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "labels_all=raw_df.primary_label.unique().tolist()\n",
    "\n",
    "print(labels_all)\n",
    "print(len(labels_all))\n",
    "\n",
    "labels_encoded=encoder.fit_transform(labels_all)\n",
    "\n",
    "print(labels_encoded)\n",
    "\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert audio data into mel spectrogram\n",
    "\n",
    "\n",
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.02,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    n_fft = int(window_size * sample_rate)  \n",
    "    hop_length = int(hop_size * sample_rate)  \n",
    "\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 encoder:LabelEncoder,\n",
    "                 audio_dir:str='../../data/train_audio',\n",
    "                 labels_path:str=None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            encoder: label encoder\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            labels_path: the file including all corresponding labels\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.audio_dir=audio_dir\n",
    "        # read data into dataframe\n",
    "        self.labels_df=pd.read_csv(labels_path,header=0)\n",
    "\n",
    "\n",
    "    def get_audio_path(self,index) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "        # Get the child path of audio from labels_df\n",
    "        audio_child_path=self.labels_df['filename'].iloc[index]\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,audio_child_path)\n",
    "    \n",
    "\n",
    "    def get_audio_label(self,index)->str:\n",
    "        '''\n",
    "        According to the provided index, get the corresponding label from the train metadata file\n",
    "\n",
    "        Parameters:\n",
    "            index: the index of labels metadata file\n",
    "        '''\n",
    "\n",
    "        return self.labels_df['primary_label'].iloc[index]\n",
    "    \n",
    "\n",
    "    def target_clip(self,index:int,audio:torch.Tensor,sample_rate:int, duration_seconds:float)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip \n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "            duration_seconds: audio duration in seconds\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time=self.labels_df['clip_start_time'].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time<=duration_seconds:\n",
    "            clip_start_point=clip_start_time*sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point+segment_duration<=total_samples:\n",
    "                clip=audio[:, clip_start_point:clip_start_point + segment_duration]\n",
    "\n",
    "            # Not long enough, a mask is needed\n",
    "            else:\n",
    "                padding_length = clip_start_point+segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "                # concat the last segment of raw audio with silence\n",
    "                clip=torch.cat((audio[:,clip_start_point:],silence),dim=1)\n",
    "\n",
    "            # mean and std\n",
    "            mean_vals = clip.mean(dim=1, keepdim=True)\n",
    "            std_vals = clip.std(dim=1, keepdim=True)\n",
    "\n",
    "            # normalization\n",
    "            standardized_clip = (clip - mean_vals) / std_vals\n",
    "\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('The clip start time is out of raw audio length')\n",
    "        \n",
    "\n",
    "\n",
    "        return standardized_clip\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #  return the size of the dataset by many Sampler implementations and the default options of DataLoader.\n",
    "    \n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # a_list[1] -> a_list.__getitems__(1)\n",
    "\n",
    "        single_audio_dir=self.get_audio_path(index)\n",
    "\n",
    "        audio_label=self.encoder.transform([self.get_audio_label(index)])[0]\n",
    "\n",
    "\n",
    "        audio, sr=read_audio(single_audio_dir)\n",
    "        \n",
    "\n",
    "        duration_seconds, num_channels=audio_info(audio,sample_rate=sr)\n",
    "\n",
    "\n",
    "        clip=self.target_clip(index,audio,sample_rate=sr, duration_seconds=duration_seconds)\n",
    "\n",
    "\n",
    "        mel_spec=mel_transform(sample_rate=sr,audio=clip)\n",
    "\n",
    "        return audio_label, mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 66, 171,   2,  70,  65, 100,  82,  82, 116, 129, 136,  41, 127,  61,\n",
      "         50,  20,  65,  73,  82, 129,  78,  39,  37,  73,  66, 146, 122,  20,\n",
      "        107,  20,  73,  55,  82,  71,  57,  47, 174, 136, 156,  43,  41,  53,\n",
      "         86, 105, 134,  71,  76,  97,  39,  19,  20,  20,  37, 143, 126,  76,\n",
      "         20,  82,  69,  44,  71,  41,  20,  20,  55,  20,  28,  22,  59,  41,\n",
      "         45,   0,  70, 129,  35, 146, 139, 135,  70, 161,  70,  38,  20,  82,\n",
      "         56,  38,  70,  64, 155,   0,  57,  80,  81,  71,  60,  73,  28,  20,\n",
      "         62, 107,  14,  43,   9,  62, 172,  62, 181,  39,  50,  43,  44,  20,\n",
      "         70, 168,  10,  21,  37,  73,  45, 160,  82, 136,  14, 105, 100,  82,\n",
      "         55,  70])\n",
      "tensor([[[[5.5261e+02, 2.1294e+03, 2.0573e+03,  ..., 2.6892e-03,\n",
      "           2.6892e-03, 2.6892e-03],\n",
      "          [1.9958e+03, 1.0570e+03, 4.1430e+02,  ..., 9.7440e-19,\n",
      "           9.7440e-19, 9.7440e-19],\n",
      "          [2.3729e+03, 4.7244e+02, 8.3954e+02,  ..., 4.7130e-19,\n",
      "           4.7130e-19, 4.7130e-19],\n",
      "          ...,\n",
      "          [6.7574e+02, 3.5174e+02, 3.8362e+02,  ..., 1.3825e-17,\n",
      "           1.3825e-17, 1.3825e-17],\n",
      "          [4.0782e+02, 2.1999e+02, 1.6710e+02,  ..., 5.1087e-18,\n",
      "           5.1087e-18, 5.1087e-18],\n",
      "          [2.4699e+02, 1.0794e+02, 1.0628e+02,  ..., 6.8209e-18,\n",
      "           6.8209e-18, 6.8209e-18]]],\n",
      "\n",
      "\n",
      "        [[[1.9696e+04, 2.4062e+03, 9.7616e+02,  ..., 9.4685e-05,\n",
      "           9.4685e-05, 9.4685e-05],\n",
      "          [6.4243e+04, 2.1367e+04, 8.3756e+03,  ..., 2.1022e-20,\n",
      "           2.1022e-20, 2.1022e-20],\n",
      "          [1.4655e+04, 3.1410e+04, 4.7075e+03,  ..., 1.6360e-20,\n",
      "           1.6360e-20, 1.6360e-20],\n",
      "          ...,\n",
      "          [7.5576e+02, 5.9517e+02, 1.3030e+02,  ..., 7.8064e-19,\n",
      "           7.8064e-19, 7.8064e-19],\n",
      "          [3.9311e+02, 2.4376e+02, 9.7215e+01,  ..., 2.8697e-19,\n",
      "           2.8697e-19, 2.8697e-19],\n",
      "          [2.1162e+02, 1.3166e+02, 5.0638e+01,  ..., 4.0182e-19,\n",
      "           4.0182e-19, 4.0182e-19]]],\n",
      "\n",
      "\n",
      "        [[[7.5820e-06, 2.1921e-06, 2.4342e-05,  ..., 2.8048e-03,\n",
      "           3.8480e-03, 1.5392e+01],\n",
      "          [1.3608e-06, 2.4939e-07, 8.7486e-05,  ..., 2.7628e-01,\n",
      "           1.0502e-01, 2.1629e+01],\n",
      "          [1.9397e-05, 1.4670e-06, 2.0721e-04,  ..., 1.5465e+00,\n",
      "           6.5721e-01, 5.2866e+01],\n",
      "          ...,\n",
      "          [3.4291e-03, 3.6370e-03, 6.5343e-03,  ..., 1.2871e+02,\n",
      "           3.5973e+01, 4.1464e+01],\n",
      "          [5.3181e-03, 3.2110e-03, 8.2894e-03,  ..., 3.2559e+01,\n",
      "           2.3995e+01, 2.8188e+01],\n",
      "          [4.8377e-03, 2.6863e-03, 8.1819e-03,  ..., 1.5817e+01,\n",
      "           1.6138e+01, 2.0769e+01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.9000e+00, 7.5713e-07, 6.9158e-07,  ..., 2.8080e-06,\n",
      "           1.7086e-06, 4.3269e-01],\n",
      "          [2.2261e+00, 8.5725e-06, 5.3531e-05,  ..., 6.5147e-03,\n",
      "           1.5090e-02, 4.6370e-01],\n",
      "          [3.0641e+00, 3.2286e-02, 2.4382e-03,  ..., 9.1352e-02,\n",
      "           2.7518e-01, 7.5829e-01],\n",
      "          ...,\n",
      "          [5.8514e+02, 3.2370e+02, 3.7890e+02,  ..., 2.8385e+02,\n",
      "           4.0121e+02, 3.3291e+02],\n",
      "          [4.9587e+02, 4.6625e+02, 5.9143e+02,  ..., 3.9693e+02,\n",
      "           3.3689e+02, 3.5177e+02],\n",
      "          [4.1873e+02, 3.9177e+02, 3.6357e+02,  ..., 2.7762e+02,\n",
      "           3.0056e+02, 2.1124e+02]]],\n",
      "\n",
      "\n",
      "        [[[8.7015e+03, 3.2130e+03, 8.2754e+02,  ..., 4.0706e-03,\n",
      "           4.0706e-03, 4.0706e-03],\n",
      "          [2.7330e+04, 1.2930e+04, 2.1800e+04,  ..., 6.7527e-19,\n",
      "           6.7527e-19, 6.7527e-19],\n",
      "          [2.6234e+04, 4.7278e+04, 4.7887e+04,  ..., 7.7947e-19,\n",
      "           7.7947e-19, 7.7947e-19],\n",
      "          ...,\n",
      "          [2.8337e+03, 3.4467e+03, 2.7615e+03,  ..., 2.4117e-17,\n",
      "           2.4117e-17, 2.4117e-17],\n",
      "          [2.3561e+03, 4.3419e+03, 4.0341e+03,  ..., 1.0121e-17,\n",
      "           1.0121e-17, 1.0121e-17],\n",
      "          [2.1230e+03, 3.9844e+03, 3.5990e+03,  ..., 1.1547e-17,\n",
      "           1.1547e-17, 1.1547e-17]]],\n",
      "\n",
      "\n",
      "        [[[1.9794e+04, 1.7699e+04, 1.2531e+04,  ..., 1.5658e+04,\n",
      "           5.0824e+03, 7.0303e+03],\n",
      "          [4.2392e+04, 5.6543e+03, 1.1957e+04,  ..., 1.3709e+04,\n",
      "           4.4108e+03, 1.4529e+04],\n",
      "          [8.3119e+03, 3.0638e+03, 3.4653e+03,  ..., 1.2073e+04,\n",
      "           2.8069e+04, 5.8053e+03],\n",
      "          ...,\n",
      "          [1.2308e+01, 1.3948e+01, 1.5900e+01,  ..., 7.8576e+00,\n",
      "           8.3750e+00, 2.7916e+02],\n",
      "          [8.7155e+00, 1.1075e+01, 1.0698e+01,  ..., 5.1966e+00,\n",
      "           6.0875e+00, 2.1209e+02],\n",
      "          [5.7590e+00, 5.1210e+00, 3.3077e+00,  ..., 2.1008e+00,\n",
      "           5.5190e-01, 2.1531e+02]]]])\n",
      "torch.Size([128, 1, 40, 251])\n"
     ]
    }
   ],
   "source": [
    "BD=BirdclefDataset(encoder=encoder,labels_path=labels_path)\n",
    "\n",
    "dataloader = DataLoader(dataset=BD, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "labels, mel_specs = batch\n",
    "print(labels)\n",
    "print(mel_specs)\n",
    "print(mel_specs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder, batch, dataloader,labels,mel_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
