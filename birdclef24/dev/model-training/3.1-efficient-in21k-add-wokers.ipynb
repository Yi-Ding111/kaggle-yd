{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use workers to speed up data processing, I separated some blocks of 3-efficient-in21k-feature-extractor into separate packages\n",
    "\n",
    "Specific packages can be found in common/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "module_path = '../../'\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.sed_s21k.audiotransform import rating_value_interplote,audio_weight, Mixup,image_delta,Mixup2,mel_transform\n",
    "from common.sed_s21k.audioprocess import read_audio \n",
    "from common.sed_s21k.audioprocess import CustomCompose,CustomOneOf,NoiseInjection,GaussianNoise,PinkNoise,AddGaussianNoise,AddGaussianSNR\n",
    "from common.sed_s21k.audiodatasets import BirdclefDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='../../data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# Find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# Initialize training set and validation set\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# Divide the dataset by index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select 20,000 data from the training set\n",
    "additional_val_samples = train_df.sample(n=2000, random_state=42)\n",
    "\n",
    "# Add these samples to the validation set\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# Remove these samples from the training set\n",
    "train_df = train_df.drop(additional_val_samples.index)\n",
    "\n",
    "\n",
    "# reduce train_df \n",
    "additional_val_samples = train_df.sample(n=150000, random_state=42)\n",
    "# Remove these samples from the training set\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65556, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2182, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is an unbalanced dataset, the amount of data in each category is very different\n",
    "# So I will calculate the weight of each category here\n",
    "# **(-0.5) The purpose is to reduce the relative influence of high-frequency categories and increase the influence of low-frequency categories, \n",
    "# so as to help the model better learn those uncommon categories\n",
    "# The purpose of calculating this is to build a WeightedRandomSampler, \n",
    "# so that each time a batch is extracted using dataloader, it is more friendly to data of different categories.\n",
    "\n",
    "def sampling_weight(df)->torch.Tensor:\n",
    "    '''\n",
    "    calculate the sampling weight of each audio file\n",
    "\n",
    "    because this is imbalanced dataset\n",
    "    we hope the category with less data has large probability to be picked.\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # Map weights to each row of the original data\n",
    "    sample_weights_map = df['primary_label'].map(sample_weights)\n",
    "\n",
    "    # Convert pandas Series to NumPy array\n",
    "    sample_weights_np = sample_weights_map.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Convert a NumPy array to a PyTorch tensor using torch.from_numpy\n",
    "    sample_weights_tensor = torch.from_numpy(sample_weights_np)\n",
    "\n",
    "    return sample_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x10618f130>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "sample_weights_tensor=sampling_weight(df=train_df)\n",
    "# Here we will build an argument sampler that will be used by the dataloader\n",
    "# Note that the order of weights in the constructed sampler must be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "# Create a sampler based on the newly obtained weight list\n",
    "sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order is very important and needs to be matched one by one in the subsequent training, I will save these types here\n",
    "# Save as .npy file\n",
    "np.save(\"./external_files/3-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self,sampler,train_df:pd.DataFrame,val_df:pd.DataFrame,bird_category_dir:str,audio_dir: str = '../../data/train_audio',batch_size:int=128,workers=4):\n",
    "        super().__init__()\n",
    "        self.train_df=train_df\n",
    "        self.val_df=val_df\n",
    "        self.bird_category_dir=bird_category_dir\n",
    "        self.audio_dir=audio_dir\n",
    "        self.batch_size=batch_size\n",
    "        self.sampler=sampler\n",
    "        self.workers=workers\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.train_df,bird_category_dir=self.bird_category_dir,audio_dir=self.audio_dir,train=True)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, sampler=self.sampler, pin_memory=True,num_workers=self.workers)\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.val_df,bird_category_dir=self.bird_category_dir,audio_dir=self.audio_dir,train=False)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, pin_memory=True,num_workers=self.workers)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    initilize layers' parameters\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # Initialize the weights and biases of the network layer\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # Check if the layer has a bias attribute\n",
    "        if layer.bias is not None: # and bias is not None\n",
    "            layer.bias.data.fill_(0.0) # If there is a bias, initialize it to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later we want to pass the acquired high-dimensional features into an attention module\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "        # x: This is the final output after the attention weights and classification layer.\n",
    "        # shape: (n_samples, out_features). Since the time dimension is summed and compressed, each sample and each output feature ends up having a single value.\n",
    "        # norm_att: This is the output of the attention layer (att) after the softmax and tanh functions, \n",
    "        # which shows which parts of the input sequence the model should focus on. Normalization ensures that the attention weights for all time steps add up to 1,\n",
    "        #  which makes it easier to interpret the importance of each time step.\n",
    "        # shape: (n_samples, out_features, n_time), where out_features is the number of output features of the att convolutional layer, \n",
    "        # which is the same as the out_features argument of the input. Each time step and each output feature has a normalized weight.\n",
    "        # cla: This is the output of the classification layer (cla), which is obtained by processing the input features through another 1D convolutional layer.\n",
    "        # This output layer is often used to directly predict task-related outputs, such as the probability of a class label.\n",
    "        # Shape: (n_samples, out_features, n_time), same shape as norm_att. \n",
    "        # This means that each output feature corresponding to each time step has a value processed by the activation function.\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        self.audio_transforms = Compose(\n",
    "            [\n",
    "                # AddColoredNoise(p=0.5),\n",
    "                PitchShift(\n",
    "                    min_transpose_semitones=-4,\n",
    "                    max_transpose_semitones=4,\n",
    "                    sample_rate=32000,\n",
    "                    p=0.4,\n",
    "                ),\n",
    "                Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # load pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers).to(device) # Encapsulate multiple layers in order\n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # create a dense layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "        # freeze parameters\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.encoder.eval()\n",
    "        # self.fc1.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # Calculation using the pre-trained model (excluding the last two layers)\n",
    "        clip=self.encoder(clip.to(device)) # feature extractor\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them Dimensionality compression\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # mix audio up\n",
    "        mixup = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "        clip, audio_label,audio_weights=mixup(X=clip,Y=audio_label,weight=audio_weights)\n",
    "\n",
    "        # Use Compose to combine multiple audio transformation operations. \n",
    "        # These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "        # clip=self.audio_transforms(clip,sample_rate=self.sample_rate)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Random masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "\n",
    "        time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)\n",
    "\n",
    "        clip = time_mask_transform(clip)\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # mix audio up\n",
    "        mixup2 = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "        clip, audio_label,audio_weights = mixup2(clip, audio_label, audio_weights)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # clean memory\n",
    "        del audio_label, clip, audio_weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            print('allocated memory:',torch.cuda.memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "            print('allocated memory after empty cache:',torch.cuda.memory_allocated())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, \n",
    "        # usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # clean memory\n",
    "        del audio_label, clip, audio_weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type              | Params\n",
      "-------------------------------------------------------\n",
      "0 | audio_transforms | Compose           | 0     \n",
      "1 | encoder          | Sequential        | 20.2 M\n",
      "2 | fc1              | Linear            | 1.6 M \n",
      "3 | att_block        | AttBlockV2        | 466 K \n",
      "4 | loss_function    | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "20.2 M    Non-trainable params\n",
      "22.3 M    Total params\n",
      "89.134    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1025/1025 [53:29<00:00,  0.32it/s, v_num=8b07, train_loss_step=167.0, val_loss_step=29.60, val_loss_epoch=327.0, train_loss_epoch=588.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 326.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1025/1025 [41:36<00:00,  0.41it/s, v_num=8b07, train_loss_step=248.0, val_loss_step=26.10, val_loss_epoch=296.0, train_loss_epoch=552.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 30.445 >= min_delta = 0.0. New best score: 296.132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1025/1025 [50:07<00:00,  0.34it/s, v_num=8b07, train_loss_step=174.0, val_loss_step=26.70, val_loss_epoch=292.0, train_loss_epoch=544.0] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 4.297 >= min_delta = 0.0. New best score: 291.836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  61%|██████    | 626/1025 [43:12<27:32,  0.24it/s, v_num=8b07, train_loss_step=756.0, val_loss_step=30.40, val_loss_epoch=320.0, train_loss_epoch=556.0]  "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    # setup MLflow logger\n",
    "    mlflow_logger = MLFlowLogger(\n",
    "        experiment_name=\"BirdClef_Experiment\",\n",
    "        tracking_uri=\"file:///Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/mlruns\"\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',  \n",
    "        dirpath='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/checkpoints/',\n",
    "        filename='sed_s21k_v1-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=1,  \n",
    "        mode='min',  \n",
    "        auto_insert_metric_name=False  \n",
    "    )\n",
    "\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',  \n",
    "        min_delta=0.00,\n",
    "        patience=3,  \n",
    "        verbose=True,\n",
    "        mode='min'  \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # we used a separate dataloader to feed the model\n",
    "    # Here we encapsulate the dataloader and use this class to read data for training\n",
    "    bdm=BirdclefDatasetModule(sampler=sampler,train_df=train_df,val_df=val_df,bird_category_dir='./external_files/3-bird-cates.npy',batch_size=64,workers=num_workers)\n",
    "\n",
    "\n",
    "    class_num=len(np.load('./external_files/3-bird-cates.npy',allow_pickle=True))\n",
    "    BirdModelModule=BirdModelModule(class_num=class_num).to(device)\n",
    "\n",
    "\n",
    "    trainer=L.Trainer(\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/',\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        logger=mlflow_logger,  # use MLflow logger\n",
    "        callbacks=[checkpoint_callback, early_stop_callback], \n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
