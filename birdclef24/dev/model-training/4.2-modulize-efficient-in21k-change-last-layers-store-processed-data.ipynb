{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is designed to pre-store the model to disk to facilitate subsequent model reading and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "module_path = '../../'\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.sed_s21k_v4.audioprocess import rating_value_interplote, audio_weight, sampling_weight, dataloader_sampler_generate,class_weight_generate\n",
    "from common.sed_s21k_v4.audiotransform import read_audio, Mixup, mel_transform,image_delta, Mixup2\n",
    "from common.sed_s21k_v4.audiotransform import CustomCompose,CustomOneOf,NoiseInjection,GaussianNoise,PinkNoise,AddGaussianNoise,AddGaussianSNR\n",
    "from common.sed_s21k_v4.audiodatasets_preprepared import BirdclefDataset\n",
    "from common.sed_s21k_v4.audiodatasets import trainloader_collate,valloader_collate\n",
    "from common.sed_s21k_v4.modelmeasurements import FocalLoss,compute_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='../../data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# Find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# Initialize training set and validation set\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# Divide the dataset by index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select 20,000 data from the training set\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Add these samples to the validation set\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# Remove these samples from the training set\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare dataloader sampler\n",
    "\n",
    "# train_sampler=dataloader_sampler_generate(df=train_df)\n",
    "# val_sampler=dataloader_sampler_generate(df=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First we need to get all the types\n",
    "# meta_df=pd.read_csv(metadata_path,header=0)\n",
    "# bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "# #Because the order is very important and needs to be matched one by one in the subsequent training, I will save these types here\n",
    "# #Save as .npy file\n",
    "# np.save(\"./external_files/13-2-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load .npy file\n",
    "loaded_array = np.load(\"./external_files/13-2-bird-cates-preprepared.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_class_weights=class_weight_generate(df=train_df,loaded_array=loaded_array)\n",
    "loss_val_class_weights=class_weight_generate(df=val_df,loaded_array=loaded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tensor to file\n",
    "torch.save(loss_train_class_weights, './external_files/loss_train_class_weights_4_2.pt')\n",
    "torch.save(loss_val_class_weights,'./external_files/loss_val_class_weights_4_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define DatasetModule\n",
    "\n",
    "\n",
    "# class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         train_sampler,\n",
    "#         val_sampler,\n",
    "#         train_df: pd.DataFrame,\n",
    "#         val_df: pd.DataFrame,\n",
    "#         bird_category_dir: str,\n",
    "#         audio_dir: str = \"data/audio\",\n",
    "#         batch_size: int = 128,\n",
    "#         workers=4,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.train_df = train_df\n",
    "#         self.val_df = val_df\n",
    "#         self.bird_category_dir = bird_category_dir\n",
    "#         self.audio_dir = audio_dir\n",
    "#         self.batch_size = batch_size\n",
    "#         self.train_sampler = train_sampler\n",
    "#         self.val_sampler = val_sampler\n",
    "#         self.workers = workers\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         BD = BirdclefDataset(\n",
    "#             df=self.train_df,\n",
    "#             bird_category_dir=self.bird_category_dir,\n",
    "#             audio_dir=self.audio_dir,\n",
    "#             train=True,\n",
    "#         )\n",
    "#         loader = DataLoader(\n",
    "#             dataset=BD,\n",
    "#             batch_size=self.batch_size,\n",
    "#             sampler=self.train_sampler,\n",
    "#             pin_memory=True,\n",
    "#             num_workers=self.workers,\n",
    "#             # prefetch_factor=64,\n",
    "#             collate_fn=trainloader_collate\n",
    "#         )\n",
    "#         return loader\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         BD = BirdclefDataset(\n",
    "#             df=self.val_df,\n",
    "#             bird_category_dir=self.bird_category_dir,\n",
    "#             audio_dir=self.audio_dir,\n",
    "#             train=False,\n",
    "#         )\n",
    "#         loader = DataLoader(\n",
    "#             dataset=BD,\n",
    "#             batch_size=self.batch_size,\n",
    "#             sampler=self.val_sampler,\n",
    "#             pin_memory=True,\n",
    "#             num_workers=self.workers,\n",
    "#             # prefetch_factor=64,\n",
    "#             collate_fn=valloader_collate\n",
    "#         )\n",
    "#         return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import time\n",
    "# def save_preprocessed_data(dataloader, save_dir):\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     for i, (clips, labels,weights) in enumerate(dataloader):\n",
    "#         # data and labels are the output of batch processing\n",
    "#         torch.save(clips, os.path.join(save_dir, f'clips_batch_{i}.pt'))\n",
    "#         torch.save(labels, os.path.join(save_dir, f'labels_batch_{i}.pt'))\n",
    "#         torch.save(weights, os.path.join(save_dir, f'weights_batch_{i}.pt'))\n",
    "\n",
    "#         gc.collect()\n",
    "\n",
    "#         time.sleep(5)\n",
    "#         print(f\"Batch {i+1} completed. Continuing to next batch after cleanup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = BirdclefDatasetModule(\n",
    "#         train_sampler=train_sampler,\n",
    "#         val_sampler=val_sampler,\n",
    "#         train_df=train_df,\n",
    "#         val_df=val_df,\n",
    "#         bird_category_dir=\"./external_files/13-2-bird-cates.npy\",\n",
    "#         audio_dir=\"../../data/train_audio\",\n",
    "#         batch_size=64,\n",
    "#         workers=4,\n",
    "#     ).train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_preprocessed_data(train_loader, '/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_preprocessed_data(dataloader, save_dir):\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     for i, (clips, labels,weights) in enumerate(dataloader):\n",
    "#         # data and labels are the output of batch processing\n",
    "#         torch.save(clips, os.path.join(save_dir, f'clips_batch_{i}.pt'))\n",
    "#         torch.save(labels, os.path.join(save_dir, f'labels_batch_{i}.pt'))\n",
    "#         torch.save(weights, os.path.join(save_dir, f'weights_batch_{i}.pt'))\n",
    "\n",
    "#         gc.collect()\n",
    "\n",
    "#         time.sleep(2)\n",
    "#         print(f\"Batch {i+1} completed. Continuing to next batch after cleanup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loader = BirdclefDatasetModule(\n",
    "#         train_sampler=train_sampler,\n",
    "#         val_sampler=val_sampler,\n",
    "#         train_df=train_df,\n",
    "#         val_df=val_df,\n",
    "#         bird_category_dir=\"./external_files/13-2-bird-cates.npy\",\n",
    "#         audio_dir=\"../../data/train_audio\",\n",
    "#         batch_size=64,\n",
    "#         workers=4,\n",
    "#     ).val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_preprocessed_data(val_loader, '/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 6.1928e+00,  3.0140e+00, -2.7846e-01,  ..., -1.5057e-02,\n",
      "           -1.7112e-02, -1.7205e-02],\n",
      "          [ 7.4960e+00,  4.6186e+00, -2.7485e-01,  ..., -3.7225e-02,\n",
      "           -3.3719e-02, -5.7819e-02]],\n",
      "\n",
      "         [[-2.2470e-02, -6.5160e-02, -4.2141e-02,  ..., -3.1085e-02,\n",
      "           -3.4848e-02, -5.8637e-02],\n",
      "          [-2.7353e-02, -4.6666e-02, -2.7162e-02,  ..., -4.7861e-02,\n",
      "           -4.4709e-02, -4.5549e-02]],\n",
      "\n",
      "         [[-2.7795e-01,  1.2304e+00,  1.4178e+00,  ...,  4.3890e-01,\n",
      "            4.4359e-01, -9.4312e-02],\n",
      "          [-8.5216e-02,  8.3198e-01,  7.8580e-01,  ...,  3.0063e-01,\n",
      "            1.2384e-01, -2.3686e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5568e-02, -1.5210e-01, -2.2955e-01,  ..., -2.2615e-01,\n",
      "           -2.5099e-01, -1.7814e-01],\n",
      "          [-1.8654e-02, -9.4492e-02, -1.6579e-01,  ..., -2.4671e-01,\n",
      "           -2.2617e-01, -2.0373e-01]],\n",
      "\n",
      "         [[-1.5792e-01, -6.4512e-02, -1.7499e-01,  ..., -2.7529e-01,\n",
      "           -2.6378e-01, -2.2546e-01],\n",
      "          [-2.1967e-01, -1.3107e-01, -1.7708e-01,  ..., -2.5529e-01,\n",
      "           -1.9407e-01, -1.2296e-01]],\n",
      "\n",
      "         [[-3.2412e-03, -2.6085e-03, -4.7625e-02,  ..., -1.2496e-02,\n",
      "           -1.6494e-02, -1.0284e-02],\n",
      "          [-3.9534e-03, -8.0866e-03, -1.3369e-01,  ..., -1.6533e-02,\n",
      "           -3.8472e-02, -2.0269e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7418e+00,  9.0615e-01, -2.1204e-01,  ..., -2.1264e-01,\n",
      "           -1.0173e-01, -8.2439e-02],\n",
      "          [ 2.1051e+00,  1.5045e+00,  1.7888e-01,  ..., -2.7763e-01,\n",
      "           -1.5567e-01, -2.2195e-01]],\n",
      "\n",
      "         [[-1.6727e-02, -1.0417e-02, -2.2837e-02,  ..., -1.4922e-02,\n",
      "           -9.9713e-03, -1.8470e-02],\n",
      "          [-4.9882e-03, -6.2937e-03, -2.2420e-02,  ..., -2.4474e-02,\n",
      "           -1.6720e-02, -3.1186e-02]],\n",
      "\n",
      "         [[ 1.4499e-01,  3.2983e+00,  3.1667e+00,  ...,  5.6461e+00,\n",
      "            5.5105e+00,  4.6742e+00],\n",
      "          [ 4.8340e-01,  2.2502e+00,  2.8632e+00,  ...,  3.5951e+00,\n",
      "            4.2718e+00,  1.7774e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3908e-01, -2.7719e-01, -2.6707e-01,  ..., -2.7786e-01,\n",
      "           -2.3884e-01, -2.7822e-01],\n",
      "          [-2.3918e-01, -2.1187e-01, -2.4091e-01,  ..., -2.6840e-01,\n",
      "           -2.7552e-01, -2.5118e-01]],\n",
      "\n",
      "         [[-1.5045e-01, -4.6666e-02, -4.8786e-02,  ..., -9.9048e-02,\n",
      "           -1.8323e-01, -8.1045e-02],\n",
      "          [-1.7308e-01, -1.1494e-01, -6.5909e-02,  ..., -1.7249e-01,\n",
      "           -2.2203e-01, -1.4504e-01]],\n",
      "\n",
      "         [[-1.0764e-01, -4.4165e-02, -6.6434e-02,  ..., -9.5968e-03,\n",
      "           -2.6862e-03, -1.4865e-03],\n",
      "          [-7.3143e-02, -7.6868e-02, -6.4141e-02,  ..., -1.9306e-02,\n",
      "           -5.2732e-03, -2.5903e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0296e+00,  1.7518e+00,  8.1518e-02,  ..., -1.8810e-01,\n",
      "           -1.1273e-01, -7.0149e-02],\n",
      "          [ 6.2408e+00,  2.8802e+00, -1.8894e-01,  ..., -1.3437e-01,\n",
      "           -1.1652e-01, -8.6823e-02]],\n",
      "\n",
      "         [[-6.5472e-03, -2.7336e-02, -3.8205e-02,  ..., -1.0185e-01,\n",
      "           -7.2236e-02, -1.1331e-01],\n",
      "          [-1.4258e-02, -1.7997e-02, -3.1677e-02,  ..., -8.0941e-02,\n",
      "           -9.2830e-02, -7.3930e-02]],\n",
      "\n",
      "         [[-2.7832e-01,  7.7047e-01,  1.7934e+00,  ...,  3.0725e+00,\n",
      "            2.0944e+00,  1.3979e-01],\n",
      "          [ 9.8423e-02,  2.9543e-01,  4.5267e-01,  ...,  1.2573e+00,\n",
      "            1.3107e+00, -2.4069e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9841e-02, -1.4474e-01, -1.6360e-01,  ..., -1.8418e-01,\n",
      "           -2.0011e-01, -8.6630e-02],\n",
      "          [-4.9286e-02, -1.0300e-01, -9.8569e-02,  ..., -2.1601e-01,\n",
      "           -1.3198e-01, -8.5110e-02]],\n",
      "\n",
      "         [[-2.4214e-01, -1.6730e-01, -2.2163e-01,  ..., -2.6406e-01,\n",
      "           -2.7690e-01, -2.5670e-01],\n",
      "          [-2.7055e-01, -2.4277e-01, -2.0869e-01,  ..., -2.5481e-01,\n",
      "           -2.4616e-01, -1.3827e-01]],\n",
      "\n",
      "         [[-1.7733e-02, -6.4550e-03, -4.1253e-02,  ..., -8.0918e-03,\n",
      "           -1.2324e-02, -5.6691e-03],\n",
      "          [-1.4117e-02, -1.5074e-02, -6.4848e-02,  ..., -1.8037e-02,\n",
      "           -2.5641e-02, -1.6758e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.5739e+00,  3.9732e+00, -1.2175e-01,  ..., -7.9606e-02,\n",
      "           -5.9685e-02, -5.6510e-02],\n",
      "          [ 7.7444e+00,  5.3225e+00, -7.2457e-02,  ..., -8.2737e-02,\n",
      "           -5.6260e-02, -9.0548e-02]],\n",
      "\n",
      "         [[-3.6611e-02, -7.8755e-02, -7.1801e-02,  ..., -5.5664e-02,\n",
      "           -4.8803e-02, -1.1242e-01],\n",
      "          [-3.5813e-02, -5.7738e-02, -5.7615e-02,  ..., -6.8227e-02,\n",
      "           -6.5667e-02, -8.9551e-02]],\n",
      "\n",
      "         [[-2.6670e-01,  3.6716e-01,  4.9940e-01,  ...,  6.4596e-02,\n",
      "            5.4186e-01, -6.9108e-02],\n",
      "          [-3.3829e-02,  5.5057e-01,  5.2110e-01,  ...,  1.2256e-01,\n",
      "            2.2777e-01, -2.2784e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2709e-01, -2.7191e-01, -2.5195e-01,  ..., -2.7365e-01,\n",
      "           -2.6133e-01, -2.2537e-01],\n",
      "          [-5.6331e-02, -1.8450e-01, -2.7769e-01,  ..., -2.6975e-01,\n",
      "           -2.5019e-01, -2.7212e-01]],\n",
      "\n",
      "         [[-1.6762e-01, -7.3039e-02, -8.6650e-02,  ..., -1.5138e-01,\n",
      "           -1.7319e-01, -1.8112e-01],\n",
      "          [-1.5805e-01, -8.0131e-02, -9.4777e-02,  ..., -1.4823e-01,\n",
      "           -1.4703e-01, -4.2219e-02]],\n",
      "\n",
      "         [[-9.2290e-03, -8.3060e-03, -7.6010e-02,  ..., -3.0262e-02,\n",
      "           -1.7516e-02, -9.3269e-03],\n",
      "          [-4.1285e-03, -1.9292e-02, -2.0102e-01,  ..., -4.6206e-02,\n",
      "           -3.2716e-02, -1.5814e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7548e+00,  1.7365e+00, -2.7843e-01,  ..., -1.8675e-01,\n",
      "           -1.7762e-01, -7.3254e-02],\n",
      "          [ 5.2743e+00,  4.3824e+00, -2.4744e-01,  ..., -2.5617e-01,\n",
      "           -1.8118e-01, -1.6395e-01]],\n",
      "\n",
      "         [[-2.3006e-02, -4.0045e-02, -2.7850e-02,  ..., -2.2263e-02,\n",
      "           -3.0338e-02, -7.3697e-02],\n",
      "          [-1.6246e-02, -4.4926e-02, -3.6229e-02,  ..., -1.6941e-02,\n",
      "           -3.9193e-02, -5.0787e-02]],\n",
      "\n",
      "         [[-1.3527e-01,  7.0488e-01, -1.2713e-01,  ...,  4.5361e-01,\n",
      "           -2.4274e-01, -1.5706e-01],\n",
      "          [-2.5575e-01, -2.6582e-01, -2.5923e-01,  ..., -1.3696e-01,\n",
      "           -2.6076e-01, -1.2082e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.9607e-02, -1.3862e-01, -2.0912e-01,  ..., -2.2966e-01,\n",
      "           -2.1452e-01, -1.1987e-01],\n",
      "          [-3.2740e-02, -1.2462e-01, -1.8599e-01,  ..., -1.9324e-01,\n",
      "           -1.6905e-01, -1.4187e-01]],\n",
      "\n",
      "         [[-2.0530e-01, -8.8154e-02, -1.3259e-01,  ..., -2.4611e-01,\n",
      "           -1.8794e-01, -1.4638e-01],\n",
      "          [-1.5516e-01, -1.7237e-01, -2.1577e-01,  ..., -2.6365e-01,\n",
      "           -1.3622e-01, -9.3734e-02]],\n",
      "\n",
      "         [[-1.9016e-02, -5.8296e-03, -3.4095e-02,  ..., -1.3391e-03,\n",
      "           -1.5791e-02, -1.8403e-02],\n",
      "          [-7.3306e-03, -1.6719e-02, -7.0827e-02,  ..., -8.5911e-04,\n",
      "           -3.3306e-02, -4.4474e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0924e+00,  5.2058e+00,  4.7516e+00,  ..., -1.4592e-01,\n",
      "           -1.6234e-01, -2.2257e-01],\n",
      "          [ 5.0998e+00,  6.2344e+00,  4.0784e+00,  ..., -1.8960e-01,\n",
      "           -2.1403e-01, -2.6799e-01]],\n",
      "\n",
      "         [[-1.0755e-02, -3.7765e-02, -3.2917e-02,  ..., -4.6551e-02,\n",
      "           -4.2154e-02, -3.4037e-02],\n",
      "          [-8.6216e-03, -1.3257e-02, -3.5729e-02,  ..., -3.4546e-02,\n",
      "           -3.7551e-02, -4.4845e-02]],\n",
      "\n",
      "         [[ 2.1145e+00,  5.4431e+00,  5.9799e+00,  ...,  6.5030e+00,\n",
      "            6.5550e+00,  5.0510e+00],\n",
      "          [ 3.0925e+00,  6.6269e+00,  7.5493e+00,  ...,  5.5274e+00,\n",
      "            6.7905e+00,  5.1488e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7773e-01, -2.6297e-01, -2.7355e-01,  ..., -2.6319e-01,\n",
      "            1.2403e-01,  5.9642e-02],\n",
      "          [-1.8531e-01, -1.5254e-01, -1.3507e-01,  ..., -2.7548e-01,\n",
      "           -2.2130e-01, -1.8449e-01]],\n",
      "\n",
      "         [[-5.9156e-02, -3.9968e-02, -2.1466e-01,  ..., -2.1145e-01,\n",
      "           -2.0463e-01, -1.5416e-01],\n",
      "          [-6.7274e-02, -7.1112e-02, -1.9301e-01,  ..., -2.4099e-01,\n",
      "           -2.7087e-01, -2.2703e-01]],\n",
      "\n",
      "         [[-1.0899e-02, -5.9902e-03, -8.0336e-03,  ..., -1.5707e-02,\n",
      "           -8.9342e-03, -6.4240e-03],\n",
      "          [-7.2605e-03, -1.9289e-02, -7.7625e-03,  ..., -1.8530e-02,\n",
      "           -1.4655e-02, -1.5005e-03]]]])\n",
      "torch.Size([64, 1280, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Specify the path to the .pt file to load\n",
    "file_path = '/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train/clips_batch_0.pt'\n",
    "\n",
    "# use torch.load() to load file\n",
    "data = torch.load(file_path)\n",
    "\n",
    "# View data content\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path to the .pt file to load\n",
    "# file_path = '/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train/labels_batch_0.pt'\n",
    "\n",
    "# # use torch.load() to load file\n",
    "# data = torch.load(file_path)\n",
    "\n",
    "\n",
    "# print(data)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_path = '/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train/weights_batch_0.pt'\n",
    "\n",
    "\n",
    "# data = torch.load(file_path)\n",
    "\n",
    "# print(data)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "\n",
    "# class BirdclefDataset(Dataset):\n",
    "#     def __init__(self, data_dir):\n",
    "#         self.clip_files = glob(os.path.join(data_dir, \"data_batch_*.pt\"))\n",
    "#         self.label_files = glob(os.path.join(data_dir, \"labels_batch_*.pt\"))\n",
    "#         self.weight_files = glob(os.path.join(data_dir, \"weights_batch_*.pt\"))\n",
    "#         self.clip_files.sort()\n",
    "#         self.label_files.sort()\n",
    "#         self.weight_files.sort()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.clip_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         clips = torch.load(self.clip_files[idx])\n",
    "#         labels = torch.load(self.label_files[idx])\n",
    "#         weights = torch.load(self.weight_files[idx])\n",
    "#         return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 1,\n",
    "        workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            data_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train'\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            prefetch_factor=2,\n",
    "            # shuffle=True,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            data_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train'\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            prefetch_factor=2,\n",
    "            # shuffle=False\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset=BirdclefDataset(data_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train')\n",
    "# test_loader=DataLoader(test_dataset,batch_size=1)\n",
    "\n",
    "# for clips, labels, weights in test_loader:\n",
    "#     print(clips)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader=BirdclefDatasetModule().train_dataloader()\n",
    "\n",
    "# for data in test_loader:\n",
    "#     print(data)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self,class_nums:int=182):\n",
    "        super().__init__()\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=1280, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=128, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru3 = nn.GRU(\n",
    "            input_size=256, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru4 = nn.GRU(\n",
    "            input_size=384, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=class_nums)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        gru_out1, _ = self.gru1(x)\n",
    "        x1 = self.bn1(gru_out1)\n",
    "        gru_out2, _ = self.gru2(x1)\n",
    "        x2 = self.bn2(gru_out2)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x3 = torch.cat((x1, x2), dim=2)\n",
    "        gru_out3, _ = self.gru3(x3)\n",
    "        x4 = self.bn3(gru_out3)\n",
    "        x5 = torch.cat((x1, x2, x4), dim=2)\n",
    "        gru_out4, _ = self.gru4(x5)\n",
    "        x6 = self.dropout1(gru_out4[:, -1, :])  \n",
    "        out = self.fc1(x6)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_class_weight: torch.Tensor,\n",
    "        val_class_weight: torch.Tensor,\n",
    "        sample_rate: int = 32000,\n",
    "        class_num: int = 182,\n",
    "        lr: float = 0.001\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model: the defined model module\n",
    "            train_class_weight: the argument is used for Focal Loss Function, focal loss needs a sequence of class weights to calculate the loss\n",
    "            val_class_weight: the argument is also used for Focal loss function, for validation step\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "        self.train_class_weight = train_class_weight.to(device)\n",
    "        self.val_class_weight = val_class_weight.to(device)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.class_num = class_num\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, clips):\n",
    "\n",
    "        return self.model(clips)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        # print(clips.shape)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "        # print(clips.shape)\n",
    "\n",
    "        # predictions\n",
    "        # target_pred=self(clip.to(device))\n",
    "        target_pred = self(clips)\n",
    "        # print(\"train\", weights.shape)\n",
    "        # initialize loss fn\n",
    "        loss_fn = FocalLoss(weight=self.train_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        # roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"train_roc_auc\",\n",
    "        #     roc_auc,\n",
    "        #     on_step=True,\n",
    "        #     on_epoch=True,\n",
    "        #     prog_bar=True,\n",
    "        #     logger=True,\n",
    "        # )\n",
    "\n",
    "        # clean memory\n",
    "        del labels, clips, weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "        # print(clips.shape)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # print(clips.shape)\n",
    "\n",
    "        # predictions\n",
    "        target_pred = self(clips).detach()\n",
    "\n",
    "        # initialize loss fn\n",
    "        # print(\"val\", weights.shape)\n",
    "        loss_fn = FocalLoss(weight=self.val_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        # roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        # self.log(\n",
    "        #     \"val_roc_auc\",\n",
    "        #     roc_auc,\n",
    "        #     on_step=True,\n",
    "        #     on_epoch=True,\n",
    "        #     prog_bar=True,\n",
    "        #     logger=True,\n",
    "        # )\n",
    "\n",
    "        # clean memory\n",
    "        del labels, clips, weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.lr,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | ChronoNet | 1.0 M \n",
      "------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3087/3087 [17:10<00:00,  3.00it/s, v_num=fivx, train_loss_step=181.0, val_loss_step=180.0, val_loss_epoch=128.0, train_loss_epoch=134.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 127.821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3087/3087 [17:40<00:00,  2.91it/s, v_num=fivx, train_loss_step=178.0, val_loss_step=177.0, val_loss_epoch=123.0, train_loss_epoch=127.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 4.720 >= min_delta = 0.0. New best score: 123.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3087/3087 [17:44<00:00,  2.90it/s, v_num=fivx, train_loss_step=177.0, val_loss_step=175.0, val_loss_epoch=120.0, train_loss_epoch=123.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 3.064 >= min_delta = 0.0. New best score: 120.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3087/3087 [17:41<00:00,  2.91it/s, v_num=fivx, train_loss_step=173.0, val_loss_step=171.0, val_loss_epoch=118.0, train_loss_epoch=120.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.028 >= min_delta = 0.0. New best score: 118.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3087/3087 [17:43<00:00,  2.90it/s, v_num=fivx, train_loss_step=171.0, val_loss_step=169.0, val_loss_epoch=116.0, train_loss_epoch=118.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.979 >= min_delta = 0.0. New best score: 116.029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3087/3087 [17:38<00:00,  2.92it/s, v_num=fivx, train_loss_step=170.0, val_loss_step=167.0, val_loss_epoch=115.0, train_loss_epoch=116.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.478 >= min_delta = 0.0. New best score: 114.551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 3087/3087 [17:43<00:00,  2.90it/s, v_num=fivx, train_loss_step=170.0, val_loss_step=166.0, val_loss_epoch=113.0, train_loss_epoch=114.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.205 >= min_delta = 0.0. New best score: 113.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3087/3087 [17:46<00:00,  2.89it/s, v_num=fivx, train_loss_step=167.0, val_loss_step=165.0, val_loss_epoch=112.0, train_loss_epoch=113.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.562 >= min_delta = 0.0. New best score: 111.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 3087/3087 [17:07<00:00,  3.01it/s, v_num=fivx, train_loss_step=165.0, val_loss_step=164.0, val_loss_epoch=111.0, train_loss_epoch=112.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.181 >= min_delta = 0.0. New best score: 110.604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3087/3087 [16:59<00:00,  3.03it/s, v_num=fivx, train_loss_step=166.0, val_loss_step=163.0, val_loss_epoch=110.0, train_loss_epoch=111.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.720 >= min_delta = 0.0. New best score: 109.884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3087/3087 [1:41:53<00:00,  0.50it/s, v_num=fivx, train_loss_step=168.0, val_loss_step=161.0, val_loss_epoch=111.0, train_loss_epoch=112.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 109.884. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3087/3087 [1:41:53<00:00,  0.50it/s, v_num=fivx, train_loss_step=168.0, val_loss_step=161.0, val_loss_epoch=111.0, train_loss_epoch=112.0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "    # # initilize collate_fn\n",
    "    # valloader_collate=valloader_collate()\n",
    "    # trainloader_collate=trainloader_collate()\n",
    "\n",
    "    logger = WandbLogger(project='BirdClef-mac', name='sef_s21_v1_mac')\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",  \n",
    "        dirpath=\"models/checkpoints\",\n",
    "        filename=\"sed_s21k_v1-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,  \n",
    "        mode=\"min\",  \n",
    "        auto_insert_metric_name=False, \n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",  \n",
    "        min_delta=0.00,\n",
    "        patience=3,  \n",
    "        verbose=True,\n",
    "        mode=\"min\", \n",
    "    )\n",
    "\n",
    "\n",
    "    bdm = BirdclefDatasetModule(\n",
    "        batch_size=None,\n",
    "        workers=4,\n",
    "    )\n",
    "\n",
    "    class_num = len(np.load(\"external_files/13-2-bird-cates.npy\", allow_pickle=True))\n",
    "    # initilize model\n",
    "    chrononet = ChronoNet(class_nums=class_num)\n",
    "\n",
    "    BirdModelModule = BirdModelModule(\n",
    "        model=chrononet,\n",
    "        train_class_weight=loss_train_class_weights,\n",
    "        val_class_weight=loss_val_class_weights,\n",
    "        class_num=class_num,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        # enable mixed precision\n",
    "        precision=16,\n",
    "        # Set up Trainer, use gradient accumulation, and update parameters after accumulating gradients every 64*4 batches\n",
    "        accumulate_grad_batches=4,\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir=\"models/model_training\",\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        logger=logger,  # use MLflow logger\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],  # add callback into trainer\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
