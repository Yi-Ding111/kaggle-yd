{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will call the module to facilitate the use of multiple workers to accelerate training\n",
    "\n",
    "For specific modules, please refer to common/sed-s21k-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "module_path = '../../'\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.sed_s21k_v4.audioprocess import rating_value_interplote, audio_weight, sampling_weight, dataloader_sampler_generate,class_weight_generate\n",
    "from common.sed_s21k_v4.audiotransform import read_audio, Mixup, mel_transform,image_delta, Mixup2\n",
    "from common.sed_s21k_v4.audiotransform import CustomCompose,CustomOneOf,NoiseInjection,GaussianNoise,PinkNoise,AddGaussianNoise,AddGaussianSNR\n",
    "from common.sed_s21k_v4.audiodatasets import BirdclefDataset\n",
    "from common.sed_s21k_v4.audiodatasets import trainloader_collate,valloader_collate\n",
    "from common.sed_s21k_v4.modelmeasurements import FocalLoss,compute_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='../../data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# Find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# Initialize training set and validation set\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# Random select a sample from each category to add to the validation set, and the rest to the training set\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# Divide the dataset by index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select 20,000 data from the training set\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Add these samples to the validation set\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# Remove these samples from the training set\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataloader sampler\n",
    "\n",
    "train_sampler=dataloader_sampler_generate(df=train_df)\n",
    "val_sampler=dataloader_sampler_generate(df=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order is very important and needs to be matched one by one in the subsequent training, I will save these types here\n",
    "# Save as .npy file\n",
    "np.save(\"./external_files/13-2-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .npy file\n",
    "loaded_array = np.load(\"./external_files/13-2-bird-cates.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_class_weights=class_weight_generate(df=train_df,loaded_array=loaded_array)\n",
    "loss_val_class_weights=class_weight_generate(df=val_df,loaded_array=loaded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup_layer = Mixup(mix_beta=5, mixup_prob=0.7, mixup_double=0.5)\n",
    "# mixup2_layer = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "# audio_transforms = Compose(\n",
    "#     [\n",
    "#         # AddColoredNoise(p=0.5),\n",
    "#         PitchShift(\n",
    "#             min_transpose_semitones=-4,\n",
    "#             max_transpose_semitones=4,\n",
    "#             sample_rate=32000,\n",
    "#             p=0.4,\n",
    "#         ),\n",
    "#         Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pretrained model\n",
    "# model = timm.create_model('tf_efficientnetv2_s_in21k', pretrained=True,in_chans=3) # You can change the data channel accepted by the pre-trained model by passing in argument in_chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume that model is the loaded complete EfficientNet model\n",
    "# # Use the output of the first set of InvertedResidual\n",
    "# feature_extractor = torch.nn.Sequential(\n",
    "#     *list(model.children())[:-3]  # Remove the last three layers, which needs to be adjusted according to the actual model structure\n",
    "# )\n",
    "# feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I want to separate feature extractor from lightningmodule and add it to dataloader as part of data processing\n",
    "\n",
    "# def trainloader_collate(batch):\n",
    "#     \"\"\"\n",
    "#     When creating data batches, define how each batch should be stacked\n",
    "#     parameters:\n",
    "#         batch: is a list of tuples with (labels, clip, weights)\n",
    "#         feature_extractor: use a pretrained model as a feature extractor\n",
    "#     \"\"\"\n",
    "#     # Unpack each individual sample in the batch\n",
    "#     labels, clips, weights = zip(*batch)\n",
    "\n",
    "#     # Stack the data into new batches\n",
    "#     labels = torch.stack(labels).float()\n",
    "#     clips = torch.stack(clips).float()\n",
    "\n",
    "#     weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "#     clips, labels, weights = mixup_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "#     # Use Compose to combine multiple audio transformation operations. \n",
    "#     # These operations are applied to the input audio data to improve the generalization and robustness of the model.\n",
    "#     clips = audio_transforms(clips, sample_rate=32000)\n",
    "\n",
    "#     # Convert audio data into mel spectrogram\n",
    "#     clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "#     clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "#     # generalization\n",
    "#     clips = (clips + 80) / 80\n",
    "\n",
    "#     # Random masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "#     clips = torchaudio.transforms.TimeMasking(\n",
    "#         time_mask_param=20, iid_masks=True, p=0.3\n",
    "#     )(clips)\n",
    "\n",
    "#     # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "#     clips = image_delta(clips)\n",
    "\n",
    "#     # mix audio up\n",
    "#     clips, labels,weights = mixup2_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "#     # feature extractor\n",
    "#     # Use torch.no_grad() to ensure feature extraction does not preserve gradients\n",
    "#     with torch.no_grad():\n",
    "#         clips=feature_extractor(clips)\n",
    "\n",
    "#     return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I want to separate feature extractor from lightningmodule and add it to dataloader as part of data processing.\n",
    "\n",
    "\n",
    "# def valloader_collate(batch):\n",
    "#     \"\"\"\n",
    "#     When creating data batches, define how each batch should be stacked\n",
    "#     parameters:\n",
    "#         batch: is a list of tuples with (labels, clip, weights)\n",
    "#         feature_extractor: use a pretrained model as a feature extractor\n",
    "#     \"\"\"\n",
    "#     # Unpack each individual sample in the batch\n",
    "#     labels, clips, weights = zip(*batch)\n",
    "\n",
    "#     # Stack the data into new batches\n",
    "#     labels = torch.stack(labels).float()\n",
    "#     clips = torch.stack(clips).float()\n",
    "\n",
    "#     weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "#     # Convert audio data into mel spectrogram\n",
    "#     clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "#     ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "#     clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "#     # generalization\n",
    "#     clips = (clips + 80) / 80\n",
    "\n",
    "#     # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "#     clips = image_delta(clips)\n",
    "\n",
    "#     # feature extractor\n",
    "#     # Use torch.no_grad() to ensure feature extraction does not preserve gradients\n",
    "#     with torch.no_grad():\n",
    "#         clips = feature_extractor(clips)\n",
    "\n",
    "#     return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_sampler,\n",
    "        val_sampler,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        bird_category_dir: str,\n",
    "        audio_dir: str = \"data/audio\",\n",
    "        batch_size: int = 128,\n",
    "        workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.bird_category_dir = bird_category_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_sampler = train_sampler\n",
    "        self.val_sampler = val_sampler\n",
    "        self.workers = workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            df=self.train_df,\n",
    "            bird_category_dir=self.bird_category_dir,\n",
    "            audio_dir=self.audio_dir,\n",
    "            train=True,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.train_sampler,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            prefetch_factor=64,\n",
    "            collate_fn=trainloader_collate\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            df=self.val_df,\n",
    "            bird_category_dir=self.bird_category_dir,\n",
    "            audio_dir=self.audio_dir,\n",
    "            train=False,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.val_sampler,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            prefetch_factor=64,\n",
    "            collate_fn=valloader_collate\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self,class_nums:int=182):\n",
    "        super().__init__()\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=1280, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=128, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru3 = nn.GRU(\n",
    "            input_size=256, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru4 = nn.GRU(\n",
    "            input_size=384, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=class_nums)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        gru_out1, _ = self.gru1(x)\n",
    "        x1 = self.bn1(gru_out1)\n",
    "        gru_out2, _ = self.gru2(x1)\n",
    "        x2 = self.bn2(gru_out2)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x3 = torch.cat((x1, x2), dim=2)\n",
    "        gru_out3, _ = self.gru3(x3)\n",
    "        x4 = self.bn3(gru_out3)\n",
    "        x5 = torch.cat((x1, x2, x4), dim=2)\n",
    "        gru_out4, _ = self.gru4(x5)\n",
    "        x6 = self.dropout1(gru_out4[:, -1, :]) \n",
    "        out = self.fc1(x6)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_class_weight: torch.Tensor,\n",
    "        val_class_weight: torch.Tensor,\n",
    "        sample_rate: int = 32000,\n",
    "        class_num: int = 182,\n",
    "        lr: float = 0.001\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model: the defined model module\n",
    "            train_class_weight: the argument is used for Focal Loss Function, focal loss needs a sequence of class weights to calculate the loss\n",
    "            val_class_weight: the argument is also used for Focal loss function, for validation step\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "        self.train_class_weight = train_class_weight.to(device)\n",
    "        self.val_class_weight = val_class_weight.to(device)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.class_num = class_num\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, clips):\n",
    "\n",
    "        return self.model(clips)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # predictions\n",
    "        # target_pred=self(clip.to(device))\n",
    "        target_pred = self(clips)\n",
    "        # print(\"train\", weights.shape)\n",
    "        # initialize loss fn\n",
    "        loss_fn = FocalLoss(weight=self.train_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        # roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"train_roc_auc\",\n",
    "        #     roc_auc,\n",
    "        #     on_step=True,\n",
    "        #     on_epoch=True,\n",
    "        #     prog_bar=True,\n",
    "        #     logger=True,\n",
    "        # )\n",
    "\n",
    "        # clean memory\n",
    "        del labels, clips, weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # predictions\n",
    "        target_pred = self(clips).detach()\n",
    "\n",
    "        # initialize loss fn\n",
    "        print(\"val\", weights.shape)\n",
    "        loss_fn = FocalLoss(weight=self.val_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        # roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        # self.log(\n",
    "        #     \"val_roc_auc\",\n",
    "        #     roc_auc,\n",
    "        #     on_step=True,\n",
    "        #     on_epoch=True,\n",
    "        #     prog_bar=True,\n",
    "        #     logger=True,\n",
    "        # )\n",
    "\n",
    "        # clean memory\n",
    "        del labels, clips, weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.lr,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/yiding/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240515_202002-pzozq8b2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dydifferent/BirdClef-mac/runs/pzozq8b2' target=\"_blank\">sef_s21_v1_mac</a></strong> to <a href='https://wandb.ai/dydifferent/BirdClef-mac' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dydifferent/BirdClef-mac' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-mac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dydifferent/BirdClef-mac/runs/pzozq8b2' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-mac/runs/pzozq8b2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | ChronoNet | 1.0 M \n",
      "------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]val torch.Size([64])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  4.46it/s]val torch.Size([64])\n",
      "Epoch 0:   1%|          | 20/3087 [03:35<9:10:35,  0.09it/s, v_num=q8b2, train_loss_step=65.20] "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "    # # initilize collate_fn\n",
    "    # valloader_collate=valloader_collate()\n",
    "    # trainloader_collate=trainloader_collate()\n",
    "\n",
    "    logger = WandbLogger(project='BirdClef-mac', name='sef_s21_v1_mac')\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\", \n",
    "        dirpath=\"models/checkpoints\",\n",
    "        filename=\"sed_s21k_v1-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,  \n",
    "        mode=\"min\", \n",
    "        auto_insert_metric_name=False, \n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", \n",
    "        min_delta=0.00,\n",
    "        patience=3, \n",
    "        verbose=True,\n",
    "        mode=\"min\", \n",
    "    )\n",
    "\n",
    "    # Previously we used a separate dataloader to feed the model\n",
    "    # Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "    bdm = BirdclefDatasetModule(\n",
    "        train_sampler=train_sampler,\n",
    "        val_sampler=val_sampler,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        bird_category_dir=\"./external_files/13-2-bird-cates.npy\",\n",
    "        audio_dir=\"../../data/train_audio\",\n",
    "        batch_size=64,\n",
    "        workers=6,\n",
    "    )\n",
    "\n",
    "    class_num = len(np.load(\"external_files/13-2-bird-cates.npy\", allow_pickle=True))\n",
    "    # initilize model\n",
    "    chrononet = ChronoNet(class_nums=class_num)\n",
    "\n",
    "    BirdModelModule = BirdModelModule(\n",
    "        model=chrononet,\n",
    "        train_class_weight=loss_train_class_weights,\n",
    "        val_class_weight=loss_val_class_weights,\n",
    "        class_num=class_num,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        # Set up Trainer and enable mixed precision\n",
    "        precision=16,\n",
    "        # Set up Trainer, use gradient accumulation, and update parameters after accumulating gradients every 512 batches\n",
    "        accumulate_grad_batches=512,\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir=\"models/model_training\",\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        logger=logger,  # use MLflow logger\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],  \n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
