{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because chrononet layers were used in 4.2 notebook, but the desired effect was not achieved, I tried to replace them with the attention layer used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "module_path = '../../'\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.sed_s21k_v4.audioprocess import rating_value_interplote, audio_weight, sampling_weight, dataloader_sampler_generate,class_weight_generate\n",
    "from common.sed_s21k_v4.audiotransform import read_audio, Mixup, mel_transform,image_delta, Mixup2\n",
    "from common.sed_s21k_v4.audiotransform import CustomCompose,CustomOneOf,NoiseInjection,GaussianNoise,PinkNoise,AddGaussianNoise,AddGaussianSNR\n",
    "from common.sed_s21k_v4.audiodatasets_preprepared import BirdclefDataset\n",
    "from common.sed_s21k_v4.audiodatasets import trainloader_collate,valloader_collate\n",
    "from common.sed_s21k_v4.modelmeasurements import FocalLoss,compute_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义DatasetModule\n",
    "\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 1,\n",
    "        workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            data_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train'\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            prefetch_factor=2,\n",
    "            # shuffle=True,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            data_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/data/preprepared/train'\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            prefetch_factor=2,\n",
    "            # shuffle=False\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    初始化 全联接层的参数\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # Initialize the weights and biases of the network layer\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # Check if the layer has a bias attribute\n",
    "        if layer.bias is not None: # and bias is not None\n",
    "            layer.bias.data.fill_(0.0) # initilize as 0 if bias exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to pass the acquired high-dimensional features into an attention module\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "        # x: This is the final output after the attention weights and classification layer.\n",
    "        # shape: (n_samples, out_features). Since the time dimension is summed and compressed, each sample and each output feature ends up having a single value.\n",
    "        # norm_att: This is the output of the attention layer (att) after the softmax and tanh functions, \n",
    "        # which shows which parts of the input sequence the model should focus on. Normalization ensures that the attention weights for all time steps add up to 1, \n",
    "        # which makes it easier to interpret the importance of each time step.\n",
    "        # shape: (n_samples, out_features, n_time), where out_features is the number of output features of the att convolutional layer, \n",
    "        # which is the same as the out_features argument of the input. Each time step and each output feature has a normalized weight.\n",
    "        # cla: This is the output of the classification layer (cla), which is obtained by processing the input features through another 1D convolutional layer. \n",
    "        # This output layer is often used to directly predict task-related outputs, such as the probability of a class label.\n",
    "        # Shape: (n_samples, out_features, n_time), same shape as norm_att. This means that each output feature corresponding to each time step has a value processed by the activation function.\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        # load pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers).to(device) # Encapsulate multiple layers in order\n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # Create a dense layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them to compress the dimension\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[1]\n",
    "        clip=batch[0]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # clean memory\n",
    "        del audio_label, clip, audio_weights, target_pred\n",
    "        # if torch.cuda.is_available():\n",
    "        #     print('allocated memory:',torch.cuda.memory_allocated())\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     print('allocated memory after empty cache:',torch.cuda.memory_allocated())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[1]\n",
    "        clip=batch[0]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # clean memory\n",
    "        del audio_label, clip, audio_weights, target_pred\n",
    "        # if torch.cuda.is_available():\n",
    "        #     torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdydifferent\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240517_143851-bels9v9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dydifferent/BirdClef-mac/runs/bels9v9l' target=\"_blank\">sef_s21_v2_mac</a></strong> to <a href='https://wandb.ai/dydifferent/BirdClef-mac' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dydifferent/BirdClef-mac' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-mac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dydifferent/BirdClef-mac/runs/bels9v9l' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-mac/runs/bels9v9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder       | Sequential        | 20.2 M\n",
      "1 | fc1           | Linear            | 1.6 M \n",
      "2 | att_block     | AttBlockV2        | 466 K \n",
      "3 | loss_function | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "20.2 M    Non-trainable params\n",
      "22.3 M    Total params\n",
      "89.134    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3087/3087 [01:24<00:00, 36.63it/s, v_num=9v9l, train_loss_step=713.0, val_loss_step=707.0, val_loss_epoch=504.0, train_loss_epoch=552.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 504.471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3087/3087 [01:59<00:00, 25.79it/s, v_num=9v9l, train_loss_step=713.0, val_loss_step=698.0, val_loss_epoch=490.0, train_loss_epoch=500.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 14.071 >= min_delta = 0.0. New best score: 490.400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3087/3087 [01:59<00:00, 25.84it/s, v_num=9v9l, train_loss_step=711.0, val_loss_step=688.0, val_loss_epoch=483.0, train_loss_epoch=490.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 7.843 >= min_delta = 0.0. New best score: 482.557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3087/3087 [01:59<00:00, 25.84it/s, v_num=9v9l, train_loss_step=694.0, val_loss_step=687.0, val_loss_epoch=477.0, train_loss_epoch=483.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 5.331 >= min_delta = 0.0. New best score: 477.227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3087/3087 [01:59<00:00, 25.84it/s, v_num=9v9l, train_loss_step=694.0, val_loss_step=677.0, val_loss_epoch=473.0, train_loss_epoch=478.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 4.706 >= min_delta = 0.0. New best score: 472.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3087/3087 [02:03<00:00, 25.07it/s, v_num=9v9l, train_loss_step=670.0, val_loss_step=673.0, val_loss_epoch=469.0, train_loss_epoch=473.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 3.466 >= min_delta = 0.0. New best score: 469.054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 3087/3087 [02:00<00:00, 25.55it/s, v_num=9v9l, train_loss_step=670.0, val_loss_step=675.0, val_loss_epoch=465.0, train_loss_epoch=468.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 3.756 >= min_delta = 0.0. New best score: 465.299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3087/3087 [02:00<00:00, 25.62it/s, v_num=9v9l, train_loss_step=663.0, val_loss_step=665.0, val_loss_epoch=462.0, train_loss_epoch=464.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 3.181 >= min_delta = 0.0. New best score: 462.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 3087/3087 [01:58<00:00, 26.14it/s, v_num=9v9l, train_loss_step=669.0, val_loss_step=661.0, val_loss_epoch=460.0, train_loss_epoch=461.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.099 >= min_delta = 0.0. New best score: 460.018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3087/3087 [02:01<00:00, 25.47it/s, v_num=9v9l, train_loss_step=664.0, val_loss_step=661.0, val_loss_epoch=459.0, train_loss_epoch=459.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.323 >= min_delta = 0.0. New best score: 458.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3087/3087 [02:00<00:00, 25.72it/s, v_num=9v9l, train_loss_step=683.0, val_loss_step=682.0, val_loss_epoch=464.0, train_loss_epoch=468.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 458.695. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3087/3087 [02:00<00:00, 25.72it/s, v_num=9v9l, train_loss_step=683.0, val_loss_step=682.0, val_loss_epoch=464.0, train_loss_epoch=468.0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "    # # initilize collate_fn\n",
    "    # valloader_collate=valloader_collate()\n",
    "    # trainloader_collate=trainloader_collate()\n",
    "\n",
    "    logger = WandbLogger(project='BirdClef-mac', name='sef_s21_v2_mac')\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",  \n",
    "        dirpath=\"models/checkpoints\",\n",
    "        filename=\"sed_s21k_43-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,  \n",
    "        mode=\"min\", \n",
    "        auto_insert_metric_name=False,  \n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", \n",
    "        min_delta=0.00,\n",
    "        patience=3,  \n",
    "        verbose=True,\n",
    "        mode=\"min\",  \n",
    "    )\n",
    "\n",
    "    # we used a separate dataloader to feed the model\n",
    "    # Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "    bdm = BirdclefDatasetModule(\n",
    "        batch_size=None,\n",
    "        workers=4,\n",
    "    )\n",
    "\n",
    "    class_num = len(np.load(\"external_files/13-2-bird-cates.npy\", allow_pickle=True))\n",
    "    # initilize model\n",
    "    # chrononet = ChronoNet(class_nums=class_num)\n",
    "\n",
    "    BirdModelModule = BirdModelModule(\n",
    "        class_num=class_num,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        # enable mixed precision\n",
    "        precision=16,\n",
    "        # Set up Trainer, use gradient accumulation, and update parameters after accumulating gradients every 64*4 batches\n",
    "        accumulate_grad_batches=4,\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir=\"models/model_training\",\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        logger=logger,  # use MLflow logger\n",
    "        callbacks=[checkpoint_callback, early_stop_callback], \n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
