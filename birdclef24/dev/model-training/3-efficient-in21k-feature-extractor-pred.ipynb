{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference for 13.2-data-process-augmentation-with-batch-add-mix.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# import torch\n",
    "\n",
    "# # Creating a pre-trained model\n",
    "# pretrained_model_name = \"tf_efficientnetv2_s_in21k\"  # model name\n",
    "# model = timm.create_model(pretrained_model_name, pretrained=True, in_chans=3)\n",
    "\n",
    "# # save model weights\n",
    "# save_path='/Users/yiding/personal_projects/ML/github_repo/birdcief/model/backbones/tf-efficientnetv2_s_in21k/'\n",
    "# torch.save(model.state_dict(), save_path+pretrained_model_name + '_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading model weights in an offline environment\n",
    "# model = timm.create_model(pretrained_model_name, pretrained=False, in_chans=3)\n",
    "# model.load_state_dict(torch.load(save_path+pretrained_model_name + '_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='../../data/train_metadata_new_add_rating_2500.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# Find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# Initialize the training set and validation set\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# Random select a sample from each category to join the validation set, and the rest to join the training set\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# Divide the dataset by index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2338, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to interpolate missing values ​​for ratings in metadata csv files\n",
    "\n",
    "def rating_value_interplote(df:pd.DataFrame):\n",
    "    '''\n",
    "    interplote Nan values for rating col in metadata csv \n",
    "\n",
    "    parameters:\n",
    "        df: the df of the metadata csv file\n",
    "\n",
    "    rating col means the quality of the corresponding audio file\n",
    "        5 is high quality\n",
    "        1 is low quality\n",
    "        0 is without defined quality level\n",
    "    '''\n",
    "\n",
    "    if df['rating'].isna().sum()>0: # with missing values\n",
    "        df['rating'].fillna(0, inplace=True)\n",
    "\n",
    "    # Random assign a value to all places where the value is 0, choosing from the specified choices\n",
    "    mask = df['rating'] == 0  # Create a boolean mask indicating which positions are 0\n",
    "\n",
    "    choices=np.arange(0.5,5.1,0.5).tolist() # [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "    random_values = np.random.choice(choices, size=mask.sum())  # Generate random numbers for these 0 values \n",
    "    df.loc[mask, 'rating'] = random_values  # Fill the generated random numbers back into the corresponding positions of the original DataFrame\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the weight of each audio file by rating helps model training\n",
    "def audio_weight(df):\n",
    "    '''\n",
    "    calculate the weight corresponding to each audio file through the rating value\n",
    "\n",
    "    Because each audio has different quality level, we use weight to affect the inportance of each audio in models,\n",
    "    the lower the quality of the audio, the lower the weight\n",
    "    '''\n",
    "    #Through rating, we calculate the credibility of each audio and express it through weight. \n",
    "    # The purpose of this is to improve the model by increasing the weight of high-quality audio and reducing the weight of low-quality audio.\n",
    "    df[\"audio_weight\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is an unbalanced dataset, the amount of data in each category is very different\n",
    "# So I will calculate the weight of each category here\n",
    "# **(-0.5) The purpose is to reduce the relative influence of high-frequency categories and increase the influence of low-frequency categories, \n",
    "# so as to help the model better learn those uncommon categories\n",
    "# The purpose of calculating this is to build a WeightedRandomSampler, so that each time a batch is extracted using dataloader, it is more friendly to data of different categories.\n",
    "\n",
    "def sampling_weight(df)->torch.Tensor:\n",
    "    '''\n",
    "    calculate the sampling weight of each audio file\n",
    "\n",
    "    because this is imbalanced dataset\n",
    "    we hope the category with less data has large probability to be picked.\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # 将权重映射到原始数据的每一行\n",
    "    sample_weights_map = df['primary_label'].map(sample_weights)\n",
    "\n",
    "    # Convert pandas Series to NumPy array\n",
    "    sample_weights_np = sample_weights_map.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Convert a NumPy array to a PyTorch tensor using torch.from_numpy\n",
    "    sample_weights_tensor = torch.from_numpy(sample_weights_np)\n",
    "\n",
    "    return sample_weights_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x107da6e60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "sample_weights_tensor=sampling_weight(df=train_df)\n",
    "# Here we will build an argument sampler that will be used by the dataloader\n",
    "# Note that the order of weights in the constructed sampler must be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "# Create a sampler based on the newly obtained weight list\n",
    "sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CustomCompose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class CustomOneOf:\n",
    "    def __init__(self, transforms: list, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if np.random.rand() < self.p:\n",
    "            n_trns = len(self.transforms)\n",
    "            trns_idx = np.random.choice(n_trns)\n",
    "            trns = self.transforms[trns_idx]\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GaussianNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=40.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class VolumeControl(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode=\"uniform\"):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        assert mode in [\n",
    "            \"uniform\",\n",
    "            \"fade\",\n",
    "            \"fade\",\n",
    "            \"cosine\",\n",
    "            \"sine\",\n",
    "        ], \"`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'\"\n",
    "\n",
    "        self.db_limit = db_limit\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.db_limit, self.db_limit)\n",
    "        if self.mode == \"uniform\":\n",
    "            db_translated = 10 ** (db / 20)\n",
    "        elif self.mode == \"fade\":\n",
    "            lin = np.arange(len(y))[::-1] / (len(y) - 1)\n",
    "            db_translated = 10 ** (db * lin / 20)\n",
    "        elif self.mode == \"cosine\":\n",
    "            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * cosine / 20)\n",
    "        else:\n",
    "            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * sine / 20)\n",
    "        augmented = y * db_translated\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class NoiseInjection(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_level = (0.0, max_noise_level)\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_level = np.random.uniform(*self.noise_level)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_level).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class GaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_rate = max_rate\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "def _db2float(db: float, amplitude=True):\n",
    "    if amplitude:\n",
    "        return 10 ** (db / 20)\n",
    "    else:\n",
    "        return 10 ** (db / 10)\n",
    "\n",
    "\n",
    "def volume_down(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for decreasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to decrease\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with decreased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(-db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "def volume_up(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for increasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to increase\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with increased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "class RandomVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        if db >= 0:\n",
    "            return volume_up(y, db)\n",
    "        else:\n",
    "            return volume_down(y, db)\n",
    "\n",
    "\n",
    "class CosineVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "        dbs = _db2float(cosine * db)\n",
    "        return y * dbs\n",
    "\n",
    "\n",
    "class AddGaussianNoise(AudioTransform):\n",
    "    \"\"\"Add gaussian noise to the samples\"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self, always_apply=False, min_amplitude=0.001, max_amplitude=0.015, p=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_amplitude: Minimum noise amplification factor\n",
    "        :param max_amplitude: Maximum noise amplification factor\n",
    "        :param p:\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        assert min_amplitude > 0.0\n",
    "        assert max_amplitude > 0.0\n",
    "        assert max_amplitude >= min_amplitude\n",
    "        self.min_amplitude = min_amplitude\n",
    "        self.max_amplitude = max_amplitude\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        amplitude = np.random.uniform(self.min_amplitude, self.max_amplitude)\n",
    "        noise = np.random.randn(*samples.shape).astype(np.float32)\n",
    "        samples = samples + amplitude * noise\n",
    "        return samples\n",
    "\n",
    "\n",
    "class AddGaussianSNR(AudioTransform):\n",
    "    \"\"\"\n",
    "    Add gaussian noise to the input. A random Signal to Noise Ratio (SNR) will be picked\n",
    "    uniformly in the decibel scale. This aligns with human hearing, which is more\n",
    "    logarithmic than linear.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        always_apply=False,\n",
    "        min_snr_in_db: float = 5.0,\n",
    "        max_snr_in_db: float = 40.0,\n",
    "        p: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_snr_in_db: Minimum signal-to-noise ratio in dB. A lower number means more noise.\n",
    "        :param max_snr_in_db: Maximum signal-to-noise ratio in dB. A greater number means less noise.\n",
    "        :param p: The probability of applying this transform\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        self.min_snr_in_db = min_snr_in_db\n",
    "        self.max_snr_in_db = max_snr_in_db\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        snr = np.random.uniform(self.min_snr_in_db, self.max_snr_in_db)\n",
    "\n",
    "        clean_rms = np.sqrt(np.mean(np.square(samples)))\n",
    "\n",
    "        a = float(snr) / 20\n",
    "        noise_rms = clean_rms / (10**a)\n",
    "\n",
    "        noise = np.random.normal(0.0, noise_rms, size=samples.shape).astype(np.float32)\n",
    "        return samples + noise\n",
    "\n",
    "\n",
    "class Normalize(AudioTransform):\n",
    "    \"\"\"\n",
    "    Apply a constant amount of gain, so that highest signal level present in the sound becomes\n",
    "    0 dBFS, i.e. the loudest level allowed if all samples must be between -1 and 1. Also known\n",
    "    as peak normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(self, always_apply=False, apply_to: str = \"all\", p: float = 0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        assert apply_to in (\"all\", \"only_too_loud_sounds\")\n",
    "        self.apply_to = apply_to\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        max_amplitude = np.amax(np.abs(samples))\n",
    "        if self.apply_to == \"only_too_loud_sounds\" and max_amplitude < 1.0:\n",
    "            return samples\n",
    "\n",
    "        if max_amplitude > 0:\n",
    "            return samples / max_amplitude\n",
    "        else:\n",
    "            return samples\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order is very important and needs to be matched one by one in the subsequent training, I will save these types here\n",
    "# Save as .npy file\n",
    "np.save(\"./external_files/3-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,df:pd.DataFrame,bird_category_dir:str,audio_dir:str='../../data/train_audio',train:bool=True):\n",
    "        '''\n",
    "        parameters:\n",
    "            df: the dataframe of metadata (train/val)\n",
    "            bird_category_dir: the directory of the bird category array file (npy)\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            train: If the Datset for train set or val set\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # if the Dataset for training or validation\n",
    "        self.train=train\n",
    "        self.raw_df=df\n",
    "\n",
    "        # inperplote nan or 0 value of rating col\n",
    "        self.raw_df=rating_value_interplote(df=self.raw_df)\n",
    "        # Calculate the weight of each audio file by rating\n",
    "        self.raw_df=audio_weight(self.raw_df)\n",
    "\n",
    "        self.audio_dir=audio_dir\n",
    "\n",
    "        self.bird_cate_array=np.load(bird_category_dir,allow_pickle=True)\n",
    "\n",
    "    def get_audio_path(self,file_name:str) -> str:\n",
    "        '''\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            file_name: in format category_type/XC-ID.ogg (asbfly/XC134896.ogg)\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir,file_name)\n",
    "\n",
    "\n",
    "    def target_clip(self,index:int,audio:torch.Tensor,sample_rate:int)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip \n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time=self.raw_df['clip_start_time'].iloc[index]\n",
    "        duration_seconds=self.raw_df['duration'].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time<=duration_seconds:\n",
    "            clip_start_point=clip_start_time*sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point+segment_duration<=total_samples:\n",
    "                clip=audio[:, clip_start_point:clip_start_point + segment_duration]\n",
    "\n",
    "            # Not long enough, a mask is needed\n",
    "            else:\n",
    "                padding_length = clip_start_point+segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "                # concat the last segment of raw audio with silence\n",
    "                clip=torch.cat((audio[:,clip_start_point:],silence),dim=1)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('The clip start time is out of raw audio length')\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "    def random_audio_augmentation(self,audio:torch.Tensor):\n",
    "        '''\n",
    "        audio (torch.Tensor): A 2D tensor of audio samples with shape (1, N), where N is the number of samples.\n",
    "        '''\n",
    "        np_audio_transforms = CustomCompose(\n",
    "            [\n",
    "                CustomOneOf(\n",
    "                    [\n",
    "                        NoiseInjection(p=1, max_noise_level=0.04),\n",
    "                        GaussianNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        PinkNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        AddGaussianNoise(min_amplitude=0.0001, max_amplitude=0.03, p=0.5),\n",
    "                        AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=15, p=0.5),\n",
    "                    ],\n",
    "                    p=0.3,  \n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        audio_aug=np_audio_transforms(audio[0].numpy())\n",
    "\n",
    "        # tranfer the array to 2D tensor and keep the num channel is 1\n",
    "        # this step is to keep the input and output shape adn type are the same\n",
    "\n",
    "        audio_aug_tensor=torch.from_numpy(audio_aug)\n",
    "        audio_aug_tensor=audio_aug_tensor.unsqueeze(0)\n",
    "\n",
    "        return audio_aug_tensor\n",
    "    \n",
    "\n",
    "    def audio_label_tensor_generator(self,true_label:str)-> torch.Tensor:\n",
    "        '''\n",
    "        Generate a tensor containing all categories based on the given real audio label\n",
    "\n",
    "        Parameters:\n",
    "            true lable: a label string\n",
    "\n",
    "        Return:\n",
    "            If have 10 class, and give a true lable\n",
    "            the return should be tensor([0,1,0,0,0,0,0,0,0,0])\n",
    "        '''\n",
    "        # Find the index of the target value in the array\n",
    "        idx = np.where(self.bird_cate_array == true_label)[0][0]\n",
    "        \n",
    "        # Create a tensor of all zeros, with length equal to the length of the array\n",
    "        audio_label_tensor = torch.zeros(len(self.bird_cate_array))\n",
    "\n",
    "        # Set the value of the corresponding index position to 1\n",
    "        audio_label_tensor[idx] = 1\n",
    "\n",
    "        return audio_label_tensor\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.raw_df.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        row=self.raw_df.iloc[index]\n",
    "\n",
    "        audio_label=row['primary_label']\n",
    "        audio_weight=row['audio_weight']\n",
    "\n",
    "        # Get the path of a single audio file\n",
    "        single_audio_dir=self.get_audio_path(row['filename'])\n",
    "\n",
    "        # Read the audio array according to the path\n",
    "        audio, sr=read_audio(single_audio_dir)\n",
    "\n",
    "        # augmentation\n",
    "        if self.train:\n",
    "            audio_augmentation=self.random_audio_augmentation(audio=audio)\n",
    "            # Get the audio clip corresponding to index\n",
    "            clip=self.target_clip(index,audio=audio_augmentation,sample_rate=sr)\n",
    "        else:\n",
    "            clip=self.target_clip(index,audio=audio,sample_rate=sr)\n",
    "\n",
    "        # change audio label to one-hot tensor\n",
    "        audio_label_tensor=self.audio_label_tensor_generator(true_label=audio_label)\n",
    "\n",
    "        audio_label_tensor=torch.tensor(audio_label_tensor, dtype=torch.float16)\n",
    "        clip=torch.tensor(clip, dtype=torch.float16)\n",
    "        audio_weight=torch.tensor(audio_weight, dtype=torch.float16)\n",
    "\n",
    "        \n",
    "        return audio_label_tensor.to(device),clip.to(device),audio_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self,sampler,train_df:pd.DataFrame,val_df:pd.DataFrame,bird_category_dir:str,audio_dir: str = '../../data/train_audio',batch_size:int=128):\n",
    "        super().__init__()\n",
    "        self.train_df=train_df\n",
    "        self.val_df=val_df\n",
    "        self.bird_category_dir=bird_category_dir\n",
    "        self.audio_dir=audio_dir\n",
    "        self.batch_size=batch_size\n",
    "        self.sampler=sampler\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.train_df,bird_category_dir=self.bird_category_dir,audio_dir=self.audio_dir,train=True)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, sampler=self.sampler, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.val_df,bird_category_dir=self.bird_category_dir,audio_dir=self.audio_dir,train=False)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, pin_memory=True)\n",
    "\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup_prob, mixup_double):\n",
    "        super(Mixup, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.mixup_double = mixup_double\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0] # Generate a random number p and compare it with mixup_prob to decide whether to mix.\n",
    "        if p < self.mixup_prob:\n",
    "            bs = X.shape[0] # batch size\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs) # Generate a random permutation for randomly selecting samples from the current batch for mixing.\n",
    "\n",
    "            p1 = torch.rand((1,))[0] # If the random number p1 (determines whether to perform double mixing) is less than mixup_double, perform a single mix. Otherwise, perform double mixing:\n",
    "            if p1 < self.mixup_double:\n",
    "                X = X + X[perm]\n",
    "                Y = Y + Y[perm]\n",
    "                Y = torch.clamp(Y, 0, 1) \n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = 0.5 * weight + 0.5 * weight[perm]\n",
    "                    return X, Y, weight\n",
    "            else:\n",
    "                perm2 = torch.randperm(bs)\n",
    "                X = X + X[perm] + X[perm2]\n",
    "                Y = Y + Y[perm] + Y[perm2]\n",
    "                Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = (\n",
    "                        1 / 3 * weight + 1 / 3 * weight[perm] + 1 / 3 * weight[perm2]\n",
    "                    )\n",
    "                    return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.02,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    # Determine window size and frame shift\n",
    "    # window_size = 0.04 # 40 milliseconds\n",
    "    # hop_size = 0.02 # 20 milliseconds, usually half the window size\n",
    "    n_fft = int(window_size * sample_rate)  # Convert the window size to the number of sampling points\n",
    "    hop_length = int(hop_size * sample_rate)  # Convert frame shift to sampling point number\n",
    "\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    ).to(device)\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\") -> torch.Tensor:\n",
    "    \"\"\"Compute delta coefficients of a tensor, usually a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "        win_length (int, optional): The window length used for computing delta (Default: 5)\n",
    "        mode (str, optional): Mode parameter passed to padding (Default: \"replicate\")\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "    \"\"\"\n",
    "    device = specgram.device  # Get the device of the input tensor\n",
    "    dtype = specgram.dtype\n",
    "\n",
    "    # pack batch\n",
    "    shape = specgram.size()\n",
    "    specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "    assert win_length >= 3\n",
    "    n = (win_length - 1) // 2\n",
    "    denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "    # Create the kernel tensor, making sure it is on the same device as the input tensor\n",
    "    kernel = torch.arange(-n, n + 1, 1, dtype=dtype,device=device).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "    output = (\n",
    "        torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "    )\n",
    "\n",
    "    # unpack batch\n",
    "    output = output.reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def make_delta(input_tensor: torch.Tensor):\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    input_tensor = compute_deltas(input_tensor)\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "def image_delta(x):\n",
    "    delta_1 = make_delta(x)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([x, delta_1, delta_2], dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup2(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup2_prob):\n",
    "        super(Mixup2, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup2_prob = mixup2_prob\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0]\n",
    "        if p < self.mixup2_prob:\n",
    "            bs = X.shape[0]\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs)\n",
    "            coeffs = self.beta_distribution.rsample(torch.Size((bs,))).to(device)\n",
    "\n",
    "            if n_dims == 2:\n",
    "                X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n",
    "            elif n_dims == 3:\n",
    "                X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n",
    "            else:\n",
    "                X = (\n",
    "                    coeffs.view(-1, 1, 1, 1) * X\n",
    "                    + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n",
    "                )\n",
    "            Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n",
    "            # Y = Y + Y[perm]\n",
    "            # Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n",
    "                return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    Initialize the parameters of the fully connected layer\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # Initialize the weights and biases of the network layer\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # Check if the layer has a bias attribute\n",
    "        if layer.bias is not None: # and bias is not None\n",
    "            layer.bias.data.fill_(0.0) # If there is a bias, initialize it to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later we want to pass the acquired high-dimensional features into an attention module\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "        # x: This is the final output after the attention weights and classification layer.\n",
    "        # shape: (n_samples, out_features). Since the time dimension is summed and compressed, each sample and each output feature ends up having a single value.\n",
    "        # norm_att: This is the output of the attention layer (att) after the softmax and tanh functions, \n",
    "        # which shows which parts of the input sequence the model should focus on. Normalization ensures that the attention weights \n",
    "        # for all time steps add up to 1, which makes it easier to interpret the importance of each time step.\n",
    "        # shape: (n_samples, out_features, n_time), where out_features is the number of output features of the att convolutional layer, \n",
    "        # which is the same as the out_features argument of the input. Each time step and each output feature has a normalized weight.\n",
    "        # cla: This is the output of the classification layer (cla), which is obtained by processing the input features through another 1D convolutional layer. \n",
    "        # This output layer is often used to directly predict task-related outputs, such as the probability of a class label.\n",
    "        # Shape: (n_samples, out_features, n_time), same shape as norm_att. \n",
    "        # This means that each output feature corresponding to each time step has a value processed by the activation function.\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        self.audio_transforms = Compose(\n",
    "            [\n",
    "                # AddColoredNoise(p=0.5),\n",
    "                PitchShift(\n",
    "                    min_transpose_semitones=-4,\n",
    "                    max_transpose_semitones=4,\n",
    "                    sample_rate=32000,\n",
    "                    p=0.4,\n",
    "                ),\n",
    "                Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # load pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers).to(device) # Encapsulate multiple layers in order\n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # create a dense layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "        # freeze part of parameters\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.encoder.eval()\n",
    "        # self.fc1.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # Calculation using the pre-trained model (excluding the last two layers)\n",
    "        clip=self.encoder(clip.to(device)) # feature extractor\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them Dimensionality compression\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # mix audio up\n",
    "        mixup = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "        clip, audio_label,audio_weights=mixup(X=clip,Y=audio_label,weight=audio_weights)\n",
    "\n",
    "        # Use Compose to combine multiple audio transformation operations. \n",
    "        # These operations are applied to the input audio data to improve the generalization and robustness of the model.\n",
    "        # clip=self.audio_transforms(clip,sample_rate=self.sample_rate)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Random mask part of the Spectrogram, which helps the model learn to be robust when information is missing in certain time periods.\n",
    "\n",
    "        time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)\n",
    "\n",
    "        clip = time_mask_transform(clip)\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # mix audio up\n",
    "        mixup2 = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "        clip, audio_label,audio_weights = mixup2(clip, audio_label, audio_weights)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # convert audio to mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  \n",
    "    dirpath='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/checkpoints/',\n",
    "    filename='chrononet-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1, \n",
    "    mode='min',  \n",
    "    auto_insert_metric_name=False  \n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0.00,\n",
    "    patience=3, \n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Previous we used a separate dataloader to feed the model\n",
    "# # Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "# bdm=BirdclefDatasetModule(sampler=sampler,train_df=train_df,val_df=val_df,bird_category_dir='./external_files/3-bird-cates.npy',batch_size=128)\n",
    "\n",
    "\n",
    "# class_num=len(np.load('./external_files/3-bird-cates.npy',allow_pickle=True))\n",
    "# BirdModelModule=BirdModelModule(class_num=class_num).to(device)\n",
    "\n",
    "\n",
    "# trainer=L.Trainer(\n",
    "#     max_epochs=10,\n",
    "#     # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "#     # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "#     default_root_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/',\n",
    "#     # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "#     callbacks=[checkpoint_callback, early_stop_callback], \n",
    "# )\n",
    "\n",
    "# # train the model\n",
    "# trainer.fit(\n",
    "#     model=BirdModelModule,\n",
    "#     datamodule=bdm \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        self.audio_transforms = Compose(\n",
    "            [\n",
    "                # AddColoredNoise(p=0.5),\n",
    "                PitchShift(\n",
    "                    min_transpose_semitones=-4,\n",
    "                    max_transpose_semitones=4,\n",
    "                    sample_rate=32000,\n",
    "                    p=0.4,\n",
    "                ),\n",
    "                Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # load pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer.\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers).to(device) # Encapsulate multiple layers in order\n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # create a dense layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "        # freeze parameters\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.encoder.eval()\n",
    "        # self.fc1.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # Use the pre-trained model (excluding the last two layers) for calculation\n",
    "        clip=self.encoder(clip.to(device)) # feature extractor\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them to compress the dimension\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # mix audio up\n",
    "        mixup = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "        clip, audio_label,audio_weights=mixup(X=clip,Y=audio_label,weight=audio_weights)\n",
    "\n",
    "        # Use Compose to combine multiple audio transformation operations. These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "        # clip=self.audio_transforms(clip,sample_rate=self.sample_rate)\n",
    "\n",
    "        #Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Random masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "\n",
    "        time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)\n",
    "\n",
    "        clip = time_mask_transform(clip)\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        mixup2 = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "        clip, audio_label,audio_weights = mixup2(clip, audio_label, audio_weights)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # If you have only one tensor (feature) in your TensorDataset , batch will be a tuple containing the tensor and an empty tuple (since there are no labels)\n",
    "        features= batch\n",
    "        features=features.to(self.device)\n",
    "        predictions = self(features)\n",
    "        # Because what our model ultimately wants is the probability of an object corresponding to all categories, \n",
    "        # the sigmoid function is used here because we want to treat each class as a separate probability, so softmax is not used.\n",
    "        probabilities = predictions.sigmoid().detach()\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load checkpoint\n",
    "class_num=len(np.load('./external_files/3-bird-cates.npy',allow_pickle=True))\n",
    "\n",
    "model = BirdModelModule.load_from_checkpoint(\n",
    "    checkpoint_path=\"./checkpoints/sed_s21k_v1-03-291.84.ckpt\",\n",
    "    class_num=182\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = Path(\"../../data/predict\")\n",
    "pred_files = pred_dir.glob(\"*.ogg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio: torch.Tensor, segment_length:int):\n",
    "\n",
    "    '''\n",
    "    split raw audio tensor into multiple clips with 5 seconds long.\n",
    "\n",
    "    Parameters:\n",
    "        audio: the raw audio tensor\n",
    "        segment_length: the audio length of each 5 seconds\n",
    "\n",
    "    return:\n",
    "        parts: list includes all clips\n",
    "        end_time_list: the list of all clips' end time in seconds\n",
    "    '''\n",
    "\n",
    "    length_audio = audio.shape[1]\n",
    "    parts = []\n",
    "    # For example, if this is the first 5 seconds of audio, then the end time is 5. If it is 5-10 seconds, the end time is 10.\n",
    "    end_time_list=[]\n",
    "    end_time=5\n",
    "    for i in range(0, length_audio, segment_length):\n",
    "        part = audio[0][i:i + segment_length]\n",
    "        # if len(part) == segment_length:  # Ensure the fragment lengths are consistent\n",
    "        parts.append(part)  #Store the raw bytes of audio data\n",
    "        end_time_list.append(end_time)\n",
    "        end_time+=5\n",
    "\n",
    "        \n",
    "\n",
    "    return parts,end_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regarding the data of a single audio, some audio information needs to be paid attention to, such as audio duration and number of channels.\n",
    "\n",
    "\n",
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clips_list=[]\n",
    "clip_names_list=[]\n",
    "\n",
    "for path in pred_files:\n",
    "    # read audio as tensor\n",
    "    audio,sr=read_audio(path=path)\n",
    "\n",
    "    # get audio corresponding informatino\n",
    "    duration_seconds,num_channels=audio_info(audio=audio,sample_rate=sr)\n",
    "\n",
    "    # split audio into multi clips with 5 seconds\n",
    "    audio_clips,end_time_list=split_audio(audio=audio,segment_length=5*sr)\n",
    "\n",
    "    # generate each label name for each clip\n",
    "    soundscape_id=path.stem\n",
    "    clip_name=[f'soundscape_{soundscape_id}_{end_time}' for end_time in end_time_list]\n",
    "\n",
    "    audio_clips_list.extend(audio_clips)\n",
    "    \n",
    "    clip_names_list.extend(clip_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundscape_1000170626_5',\n",
       " 'soundscape_1000170626_10',\n",
       " 'soundscape_1000170626_15',\n",
       " 'soundscape_1000170626_20',\n",
       " 'soundscape_1000170626_25',\n",
       " 'soundscape_1000170626_30',\n",
       " 'soundscape_1000170626_35',\n",
       " 'soundscape_1000170626_40',\n",
       " 'soundscape_1000170626_45',\n",
       " 'soundscape_1000170626_50',\n",
       " 'soundscape_1000170626_55',\n",
       " 'soundscape_1000170626_60',\n",
       " 'soundscape_1000170626_65',\n",
       " 'soundscape_1000170626_70',\n",
       " 'soundscape_1000170626_75',\n",
       " 'soundscape_1000170626_80',\n",
       " 'soundscape_1000170626_85',\n",
       " 'soundscape_1000170626_90',\n",
       " 'soundscape_1000170626_95',\n",
       " 'soundscape_1000170626_100',\n",
       " 'soundscape_1000170626_105',\n",
       " 'soundscape_1000170626_110',\n",
       " 'soundscape_1000170626_115',\n",
       " 'soundscape_1000170626_120',\n",
       " 'soundscape_1000170626_125',\n",
       " 'soundscape_1000170626_130',\n",
       " 'soundscape_1000170626_135',\n",
       " 'soundscape_1000170626_140',\n",
       " 'soundscape_1000170626_145',\n",
       " 'soundscape_1000170626_150',\n",
       " 'soundscape_1000170626_155',\n",
       " 'soundscape_1000170626_160',\n",
       " 'soundscape_1000170626_165',\n",
       " 'soundscape_1000170626_170',\n",
       " 'soundscape_1000170626_175',\n",
       " 'soundscape_1000170626_180',\n",
       " 'soundscape_1000170626_185',\n",
       " 'soundscape_1000170626_190',\n",
       " 'soundscape_1000170626_195',\n",
       " 'soundscape_1000170626_200',\n",
       " 'soundscape_1000170626_205',\n",
       " 'soundscape_1000170626_210',\n",
       " 'soundscape_1000170626_215',\n",
       " 'soundscape_1000170626_220',\n",
       " 'soundscape_1000170626_225',\n",
       " 'soundscape_1000170626_230',\n",
       " 'soundscape_1000170626_235',\n",
       " 'soundscape_1000170626_240',\n",
       " 'soundscape_1000389428_5',\n",
       " 'soundscape_1000389428_10',\n",
       " 'soundscape_1000389428_15',\n",
       " 'soundscape_1000389428_20',\n",
       " 'soundscape_1000389428_25',\n",
       " 'soundscape_1000389428_30',\n",
       " 'soundscape_1000389428_35',\n",
       " 'soundscape_1000389428_40',\n",
       " 'soundscape_1000389428_45',\n",
       " 'soundscape_1000389428_50',\n",
       " 'soundscape_1000389428_55',\n",
       " 'soundscape_1000389428_60',\n",
       " 'soundscape_1000389428_65',\n",
       " 'soundscape_1000389428_70',\n",
       " 'soundscape_1000389428_75',\n",
       " 'soundscape_1000389428_80',\n",
       " 'soundscape_1000389428_85',\n",
       " 'soundscape_1000389428_90',\n",
       " 'soundscape_1000389428_95',\n",
       " 'soundscape_1000389428_100',\n",
       " 'soundscape_1000389428_105',\n",
       " 'soundscape_1000389428_110',\n",
       " 'soundscape_1000389428_115',\n",
       " 'soundscape_1000389428_120',\n",
       " 'soundscape_1000389428_125',\n",
       " 'soundscape_1000389428_130',\n",
       " 'soundscape_1000389428_135',\n",
       " 'soundscape_1000389428_140',\n",
       " 'soundscape_1000389428_145',\n",
       " 'soundscape_1000389428_150',\n",
       " 'soundscape_1000389428_155',\n",
       " 'soundscape_1000389428_160',\n",
       " 'soundscape_1000389428_165',\n",
       " 'soundscape_1000389428_170',\n",
       " 'soundscape_1000389428_175',\n",
       " 'soundscape_1000389428_180',\n",
       " 'soundscape_1000389428_185',\n",
       " 'soundscape_1000389428_190',\n",
       " 'soundscape_1000389428_195',\n",
       " 'soundscape_1000389428_200',\n",
       " 'soundscape_1000389428_205',\n",
       " 'soundscape_1000389428_210',\n",
       " 'soundscape_1000389428_215',\n",
       " 'soundscape_1000389428_220',\n",
       " 'soundscape_1000389428_225',\n",
       " 'soundscape_1000389428_230',\n",
       " 'soundscape_1000389428_235',\n",
       " 'soundscape_1000389428_240']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset\n",
    "dataset = Dataset.from_dict({'audio_clip': audio_clips_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_clip'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_transform(batch):\n",
    "    \"\"\"\n",
    "    Transform audio data into normalized mel spectrogram in decibel scale.\n",
    "    \"\"\"\n",
    "    n_fft = int(0.04 * 32000)  # Convert window size to sample points\n",
    "    hop_length = int(0.02 * 32000)  # Convert hop size to sample points\n",
    "    n_mels = 40  # Number of Mel filters\n",
    "\n",
    "    # Create Mel Spectrogram transformer\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=32000,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "    \n",
    "    # Create dB transformer\n",
    "    db_transform = AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "    audio_clip_batch = batch['audio_clip']\n",
    "    melspec_list = []\n",
    "\n",
    "    for audio_clip in audio_clip_batch:\n",
    "        # Convert audio clip to tensor and add a new dimension\n",
    "        audio_clip = torch.tensor(audio_clip).unsqueeze(0)\n",
    "\n",
    "        # Generate Mel Spectrogram\n",
    "        melspec = mel_transformer(audio_clip)\n",
    "        \n",
    "        # Convert Mel Spectrogram to dB\n",
    "        db_melspec = db_transform(melspec)\n",
    "        \n",
    "        # Normalize the spectrogram\n",
    "        normalized_melspec = (db_melspec + 80) / 80\n",
    "        \n",
    "        # Move the normalized spectrogram to the desired device\n",
    "        normalized_melspec = normalized_melspec.to(device)\n",
    "\n",
    "        melspec_list.append(normalized_melspec)\n",
    "\n",
    "    return {'audio_mel': melspec_list}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:04<00:00, 22.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_mel=dataset.map(pred_transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mel_single=dataset_mel.remove_columns('audio_clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 251])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_mel_single['audio_mel'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_mel'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mel_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_delta(batch):\n",
    "    batch=torch.tensor(batch['audio_mel'])\n",
    "    delta_1 = make_delta(batch)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([batch, delta_1, delta_2], dim=1)\n",
    "    \n",
    "    return {'clip_delta':x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:00<00:00, 101.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_delta=dataset_mel_single.map(image_delta, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 3, 40, 251])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_delta['clip_delta']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_mel_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_delta_single=dataset_delta.remove_columns('audio_mel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['clip_delta'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_delta_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40, 251])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_delta_single['clip_delta'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__()\n",
    "        self.dataset=dataset\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_melspec=self.dataset['clip_delta'][index]\n",
    "\n",
    "        audio_tensor=torch.tensor(audio_melspec)\n",
    "\n",
    "        return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD=PredDataset(dataset=dataset_delta_single)\n",
    "\n",
    "dataloader = DataLoader(dataset=PD, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:35<00:00,  0.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. load model for prediction\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1\n",
    ")\n",
    "\n",
    "# Use trainer to make predictions\n",
    "predictions = trainer.predict(model, dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5008, 0.5087, 0.5071,  ..., 0.5011, 0.5012, 0.5029],\n",
       "         [0.5005, 0.5076, 0.5082,  ..., 0.5014, 0.5011, 0.5026],\n",
       "         [0.5008, 0.5089, 0.5090,  ..., 0.5015, 0.5016, 0.5037],\n",
       "         ...,\n",
       "         [0.5011, 0.5107, 0.5096,  ..., 0.5012, 0.5016, 0.5034],\n",
       "         [0.5008, 0.5078, 0.5084,  ..., 0.5010, 0.5016, 0.5031],\n",
       "         [0.5006, 0.5074, 0.5071,  ..., 0.5007, 0.5016, 0.5025]]),\n",
       " tensor([[0.5011, 0.5066, 0.5097,  ..., 0.5010, 0.5014, 0.5038],\n",
       "         [0.5004, 0.5057, 0.5062,  ..., 0.5006, 0.5009, 0.5021],\n",
       "         [0.5007, 0.5079, 0.5082,  ..., 0.5008, 0.5014, 0.5029],\n",
       "         ...,\n",
       "         [0.5017, 0.5071, 0.5097,  ..., 0.5012, 0.5020, 0.5048],\n",
       "         [0.5006, 0.5074, 0.5079,  ..., 0.5008, 0.5012, 0.5026],\n",
       "         [0.5011, 0.5073, 0.5089,  ..., 0.5015, 0.5016, 0.5040]]),\n",
       " tensor([[0.5010, 0.5096, 0.5098,  ..., 0.5016, 0.5021, 0.5041],\n",
       "         [0.5007, 0.5075, 0.5070,  ..., 0.5007, 0.5010, 0.5026],\n",
       "         [0.5004, 0.5051, 0.5049,  ..., 0.5004, 0.5007, 0.5016],\n",
       "         ...,\n",
       "         [0.5004, 0.5060, 0.5055,  ..., 0.5006, 0.5007, 0.5020],\n",
       "         [0.5004, 0.5055, 0.5062,  ..., 0.5007, 0.5009, 0.5020],\n",
       "         [0.5007, 0.5094, 0.5087,  ..., 0.5009, 0.5014, 0.5023]])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['1']+['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(columns=['row_id']+np.load('./external_files/3-bird-cates.npy',allow_pickle=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>insbab1</th>\n",
       "      <th>whiter2</th>\n",
       "      <th>rocpig</th>\n",
       "      <th>blakit1</th>\n",
       "      <th>asbfly</th>\n",
       "      <th>litegr</th>\n",
       "      <th>houspa</th>\n",
       "      <th>comros</th>\n",
       "      <th>grnwar1</th>\n",
       "      <th>...</th>\n",
       "      <th>bncwoo3</th>\n",
       "      <th>malpar1</th>\n",
       "      <th>crbsun2</th>\n",
       "      <th>insowl1</th>\n",
       "      <th>chbeat1</th>\n",
       "      <th>vehpar1</th>\n",
       "      <th>sttwoo1</th>\n",
       "      <th>eurbla2</th>\n",
       "      <th>junmyn1</th>\n",
       "      <th>oripip1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, insbab1, whiter2, rocpig, blakit1, asbfly, litegr, houspa, comros, grnwar1, wynlau1, graher1, eucdov, cregos1, copbar1, blrwar1, comtai1, eaywag1, grehor1, purher1, zitcis1, thbwar1, bkwsti, indrol2, lobsun2, whtkin2, gloibi, gybpri1, goflea1, rerswa1, comgre, ashwoo2, bcnher, eurcoo, grewar3, piekin1, compea, blnmon1, woosan, forwag1, comsan, gargan, wemhar1, commoo3, ruftre2, barswa, comkin1, asikoe2, whbwag1, bkskit1, pursun4, junbab2, putbab1, brnhao1, inbrob1, laudov1, lirplo, grefla1, pabflo1, grywag, whbwat1, gyhcaf1, grtdro1, hoopoe, commyn, categr, brfowl1, labcro1, litgre1, brwowl1, nutman, grnsan, ashdro1, yebbul3, whbsho3, mawthr1, orihob2, heswoo1, bladro1, ingori1, rewbul, whbbul2, rorpar, cohcuc1, sohmyn1, shikra1, placuc3, litswi1, blhori1, stbkin1, indrob1, comior1, brnshr, smamin1, crseag1, bkrfla1, plapri1, lesyel1, grecou1, lblwar1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 163 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each tensor to a NumPy array and use them as rows of the DataFrame\n",
    "data_frames = [pd.DataFrame(tensor.numpy()) for tensor in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrames into one big DataFrame\n",
    "# Each tensor forms a block of the DataFrame\n",
    "df = pd.concat(data_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500847</td>\n",
       "      <td>0.508715</td>\n",
       "      <td>0.507131</td>\n",
       "      <td>0.500731</td>\n",
       "      <td>0.504230</td>\n",
       "      <td>0.500931</td>\n",
       "      <td>0.500945</td>\n",
       "      <td>0.500395</td>\n",
       "      <td>0.501287</td>\n",
       "      <td>0.504073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506515</td>\n",
       "      <td>0.500559</td>\n",
       "      <td>0.501985</td>\n",
       "      <td>0.504159</td>\n",
       "      <td>0.501750</td>\n",
       "      <td>0.500453</td>\n",
       "      <td>0.501527</td>\n",
       "      <td>0.501061</td>\n",
       "      <td>0.501202</td>\n",
       "      <td>0.502913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.507622</td>\n",
       "      <td>0.508183</td>\n",
       "      <td>0.501068</td>\n",
       "      <td>0.504058</td>\n",
       "      <td>0.500899</td>\n",
       "      <td>0.500944</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.501162</td>\n",
       "      <td>0.503265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506215</td>\n",
       "      <td>0.500726</td>\n",
       "      <td>0.501299</td>\n",
       "      <td>0.503370</td>\n",
       "      <td>0.501399</td>\n",
       "      <td>0.500514</td>\n",
       "      <td>0.501542</td>\n",
       "      <td>0.501441</td>\n",
       "      <td>0.501074</td>\n",
       "      <td>0.502585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500816</td>\n",
       "      <td>0.508852</td>\n",
       "      <td>0.509012</td>\n",
       "      <td>0.500980</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.501387</td>\n",
       "      <td>0.501179</td>\n",
       "      <td>0.500673</td>\n",
       "      <td>0.501862</td>\n",
       "      <td>0.504559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508769</td>\n",
       "      <td>0.500878</td>\n",
       "      <td>0.502046</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.502175</td>\n",
       "      <td>0.500769</td>\n",
       "      <td>0.501870</td>\n",
       "      <td>0.501455</td>\n",
       "      <td>0.501611</td>\n",
       "      <td>0.503708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500460</td>\n",
       "      <td>0.507444</td>\n",
       "      <td>0.505193</td>\n",
       "      <td>0.500502</td>\n",
       "      <td>0.502649</td>\n",
       "      <td>0.500780</td>\n",
       "      <td>0.500567</td>\n",
       "      <td>0.500248</td>\n",
       "      <td>0.500807</td>\n",
       "      <td>0.503195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505132</td>\n",
       "      <td>0.500455</td>\n",
       "      <td>0.501061</td>\n",
       "      <td>0.502447</td>\n",
       "      <td>0.501107</td>\n",
       "      <td>0.500447</td>\n",
       "      <td>0.500970</td>\n",
       "      <td>0.500538</td>\n",
       "      <td>0.500802</td>\n",
       "      <td>0.501826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500870</td>\n",
       "      <td>0.508636</td>\n",
       "      <td>0.509370</td>\n",
       "      <td>0.501127</td>\n",
       "      <td>0.505013</td>\n",
       "      <td>0.501530</td>\n",
       "      <td>0.501477</td>\n",
       "      <td>0.500645</td>\n",
       "      <td>0.502033</td>\n",
       "      <td>0.504597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509093</td>\n",
       "      <td>0.500943</td>\n",
       "      <td>0.502345</td>\n",
       "      <td>0.504458</td>\n",
       "      <td>0.502089</td>\n",
       "      <td>0.501030</td>\n",
       "      <td>0.502110</td>\n",
       "      <td>0.501276</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.503278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.500338</td>\n",
       "      <td>0.505972</td>\n",
       "      <td>0.506470</td>\n",
       "      <td>0.500438</td>\n",
       "      <td>0.502620</td>\n",
       "      <td>0.500555</td>\n",
       "      <td>0.500543</td>\n",
       "      <td>0.500143</td>\n",
       "      <td>0.500649</td>\n",
       "      <td>0.503359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504548</td>\n",
       "      <td>0.500394</td>\n",
       "      <td>0.501050</td>\n",
       "      <td>0.502213</td>\n",
       "      <td>0.501090</td>\n",
       "      <td>0.500320</td>\n",
       "      <td>0.500877</td>\n",
       "      <td>0.500380</td>\n",
       "      <td>0.500803</td>\n",
       "      <td>0.501748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.500480</td>\n",
       "      <td>0.507602</td>\n",
       "      <td>0.507627</td>\n",
       "      <td>0.500755</td>\n",
       "      <td>0.503814</td>\n",
       "      <td>0.500723</td>\n",
       "      <td>0.501146</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.501320</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505877</td>\n",
       "      <td>0.500651</td>\n",
       "      <td>0.501638</td>\n",
       "      <td>0.503184</td>\n",
       "      <td>0.501508</td>\n",
       "      <td>0.500566</td>\n",
       "      <td>0.501323</td>\n",
       "      <td>0.500768</td>\n",
       "      <td>0.501412</td>\n",
       "      <td>0.502094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.500352</td>\n",
       "      <td>0.505996</td>\n",
       "      <td>0.505472</td>\n",
       "      <td>0.500510</td>\n",
       "      <td>0.502127</td>\n",
       "      <td>0.500595</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.500194</td>\n",
       "      <td>0.500704</td>\n",
       "      <td>0.502782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504038</td>\n",
       "      <td>0.500653</td>\n",
       "      <td>0.501016</td>\n",
       "      <td>0.502739</td>\n",
       "      <td>0.500857</td>\n",
       "      <td>0.500304</td>\n",
       "      <td>0.500849</td>\n",
       "      <td>0.500579</td>\n",
       "      <td>0.500652</td>\n",
       "      <td>0.501967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.500439</td>\n",
       "      <td>0.505509</td>\n",
       "      <td>0.506158</td>\n",
       "      <td>0.500442</td>\n",
       "      <td>0.503384</td>\n",
       "      <td>0.500635</td>\n",
       "      <td>0.500833</td>\n",
       "      <td>0.500338</td>\n",
       "      <td>0.501091</td>\n",
       "      <td>0.502910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505003</td>\n",
       "      <td>0.500456</td>\n",
       "      <td>0.501603</td>\n",
       "      <td>0.502521</td>\n",
       "      <td>0.501381</td>\n",
       "      <td>0.500385</td>\n",
       "      <td>0.501152</td>\n",
       "      <td>0.500656</td>\n",
       "      <td>0.500901</td>\n",
       "      <td>0.501962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.500701</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.508651</td>\n",
       "      <td>0.500829</td>\n",
       "      <td>0.503933</td>\n",
       "      <td>0.500907</td>\n",
       "      <td>0.501259</td>\n",
       "      <td>0.500351</td>\n",
       "      <td>0.501379</td>\n",
       "      <td>0.504448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506473</td>\n",
       "      <td>0.500548</td>\n",
       "      <td>0.501853</td>\n",
       "      <td>0.504145</td>\n",
       "      <td>0.501640</td>\n",
       "      <td>0.500524</td>\n",
       "      <td>0.501279</td>\n",
       "      <td>0.500876</td>\n",
       "      <td>0.501370</td>\n",
       "      <td>0.502282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.500847  0.508715  0.507131  0.500731  0.504230  0.500931  0.500945   \n",
       "1   0.500545  0.507622  0.508183  0.501068  0.504058  0.500899  0.500944   \n",
       "2   0.500816  0.508852  0.509012  0.500980  0.504600  0.501387  0.501179   \n",
       "3   0.500460  0.507444  0.505193  0.500502  0.502649  0.500780  0.500567   \n",
       "4   0.500870  0.508636  0.509370  0.501127  0.505013  0.501530  0.501477   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "91  0.500338  0.505972  0.506470  0.500438  0.502620  0.500555  0.500543   \n",
       "92  0.500480  0.507602  0.507627  0.500755  0.503814  0.500723  0.501146   \n",
       "93  0.500352  0.505996  0.505472  0.500510  0.502127  0.500595  0.500700   \n",
       "94  0.500439  0.505509  0.506158  0.500442  0.503384  0.500635  0.500833   \n",
       "95  0.500701  0.509434  0.508651  0.500829  0.503933  0.500907  0.501259   \n",
       "\n",
       "         7         8         9    ...       172       173       174       175  \\\n",
       "0   0.500395  0.501287  0.504073  ...  0.506515  0.500559  0.501985  0.504159   \n",
       "1   0.500286  0.501162  0.503265  ...  0.506215  0.500726  0.501299  0.503370   \n",
       "2   0.500673  0.501862  0.504559  ...  0.508769  0.500878  0.502046  0.504065   \n",
       "3   0.500248  0.500807  0.503195  ...  0.505132  0.500455  0.501061  0.502447   \n",
       "4   0.500645  0.502033  0.504597  ...  0.509093  0.500943  0.502345  0.504458   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "91  0.500143  0.500649  0.503359  ...  0.504548  0.500394  0.501050  0.502213   \n",
       "92  0.500300  0.501320  0.503300  ...  0.505877  0.500651  0.501638  0.503184   \n",
       "93  0.500194  0.500704  0.502782  ...  0.504038  0.500653  0.501016  0.502739   \n",
       "94  0.500338  0.501091  0.502910  ...  0.505003  0.500456  0.501603  0.502521   \n",
       "95  0.500351  0.501379  0.504448  ...  0.506473  0.500548  0.501853  0.504145   \n",
       "\n",
       "         176       177       178       179       180       181  \n",
       "0   0.501750  0.500453  0.501527  0.501061  0.501202  0.502913  \n",
       "1   0.501399  0.500514  0.501542  0.501441  0.501074  0.502585  \n",
       "2   0.502175  0.500769  0.501870  0.501455  0.501611  0.503708  \n",
       "3   0.501107  0.500447  0.500970  0.500538  0.500802  0.501826  \n",
       "4   0.502089  0.501030  0.502110  0.501276  0.501500  0.503278  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "91  0.501090  0.500320  0.500877  0.500380  0.500803  0.501748  \n",
       "92  0.501508  0.500566  0.501323  0.500768  0.501412  0.502094  \n",
       "93  0.500857  0.500304  0.500849  0.500579  0.500652  0.501967  \n",
       "94  0.501381  0.500385  0.501152  0.500656  0.500901  0.501962  \n",
       "95  0.501640  0.500524  0.501279  0.500876  0.501370  0.502282  \n",
       "\n",
       "[96 rows x 182 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 182 elements, new values have 162 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./external_files/3-bird-cates.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/pandas/core/generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/pandas/core/generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/pandas/core/internals/managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/pandas/core/internals/base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 182 elements, new values have 162 elements"
     ]
    }
   ],
   "source": [
    "df.columns=np.load('./external_files/3-bird-cates.npy',allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insbab1</th>\n",
       "      <th>whiter2</th>\n",
       "      <th>rocpig</th>\n",
       "      <th>blakit1</th>\n",
       "      <th>asbfly</th>\n",
       "      <th>litegr</th>\n",
       "      <th>houspa</th>\n",
       "      <th>comros</th>\n",
       "      <th>grnwar1</th>\n",
       "      <th>wynlau1</th>\n",
       "      <th>...</th>\n",
       "      <th>bncwoo3</th>\n",
       "      <th>malpar1</th>\n",
       "      <th>crbsun2</th>\n",
       "      <th>insowl1</th>\n",
       "      <th>chbeat1</th>\n",
       "      <th>vehpar1</th>\n",
       "      <th>sttwoo1</th>\n",
       "      <th>eurbla2</th>\n",
       "      <th>junmyn1</th>\n",
       "      <th>oripip1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.502747</td>\n",
       "      <td>0.500176</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>0.500471</td>\n",
       "      <td>0.508517</td>\n",
       "      <td>0.532191</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.500020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>0.504719</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.533823</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>0.500132</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.509141</td>\n",
       "      <td>0.516256</td>\n",
       "      <td>0.533277</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500295</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.503245</td>\n",
       "      <td>0.500991</td>\n",
       "      <td>0.500040</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.504204</td>\n",
       "      <td>0.532776</td>\n",
       "      <td>0.537239</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.502176</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.509151</td>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.501754</td>\n",
       "      <td>0.500065</td>\n",
       "      <td>0.506159</td>\n",
       "      <td>0.523961</td>\n",
       "      <td>0.536590</td>\n",
       "      <td>0.500011</td>\n",
       "      <td>0.500058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.502940</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.505027</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>0.501812</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.504708</td>\n",
       "      <td>0.535431</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.500002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510557</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.500452</td>\n",
       "      <td>0.527358</td>\n",
       "      <td>0.500230</td>\n",
       "      <td>0.501527</td>\n",
       "      <td>0.500256</td>\n",
       "      <td>0.524080</td>\n",
       "      <td>0.501026</td>\n",
       "      <td>0.510450</td>\n",
       "      <td>0.500033</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.511648</td>\n",
       "      <td>0.500009</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.503903</td>\n",
       "      <td>0.500130</td>\n",
       "      <td>0.515409</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.522831</td>\n",
       "      <td>0.523866</td>\n",
       "      <td>0.512087</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.504960</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.500027</td>\n",
       "      <td>0.500318</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.500239</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.509084</td>\n",
       "      <td>0.502225</td>\n",
       "      <td>0.506571</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.500032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.501020</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>0.500026</td>\n",
       "      <td>0.502344</td>\n",
       "      <td>0.500052</td>\n",
       "      <td>0.515034</td>\n",
       "      <td>0.524997</td>\n",
       "      <td>0.519913</td>\n",
       "      <td>0.500009</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.543843</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.503317</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>0.500525</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.501473</td>\n",
       "      <td>0.503620</td>\n",
       "      <td>0.504667</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>0.500002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     insbab1   whiter2    rocpig   blakit1    asbfly    litegr    houspa  \\\n",
       "0   0.500001  0.502747  0.500176  0.500365  0.500028  0.500471  0.508517   \n",
       "1   0.500004  0.533823  0.500019  0.500132  0.500004  0.509141  0.516256   \n",
       "2   0.500003  0.503245  0.500991  0.500040  0.500003  0.504204  0.532776   \n",
       "3   0.500005  0.509151  0.500025  0.501754  0.500065  0.506159  0.523961   \n",
       "4   0.500001  0.505027  0.500007  0.501812  0.500005  0.539216  0.504708   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "91  0.500452  0.527358  0.500230  0.501527  0.500256  0.524080  0.501026   \n",
       "92  0.500001  0.503903  0.500130  0.515409  0.500005  0.522831  0.523866   \n",
       "93  0.500027  0.500318  0.500018  0.500239  0.500004  0.509084  0.502225   \n",
       "94  0.500031  0.505785  0.500026  0.502344  0.500052  0.515034  0.524997   \n",
       "95  0.500000  0.503317  0.500019  0.500525  0.500001  0.501473  0.503620   \n",
       "\n",
       "      comros   grnwar1   wynlau1  ...   bncwoo3   malpar1   crbsun2  insowl1  \\\n",
       "0   0.532191  0.500003  0.500020  ...  0.500008  0.504719  0.500000      0.5   \n",
       "1   0.533277  0.500019  0.500001  ...  0.500000  0.500295  0.500000      0.5   \n",
       "2   0.537239  0.500001  0.500001  ...  0.500000  0.502176  0.500004      0.5   \n",
       "3   0.536590  0.500011  0.500058  ...  0.500000  0.502940  0.500005      0.5   \n",
       "4   0.535431  0.500001  0.500002  ...  0.500000  0.510557  0.500003      0.5   \n",
       "..       ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "91  0.510450  0.500033  0.500004  ...  0.500000  0.511648  0.500009      0.5   \n",
       "92  0.512087  0.500008  0.500001  ...  0.500005  0.504960  0.500007      0.5   \n",
       "93  0.506571  0.500001  0.500032  ...  0.500000  0.501020  0.500000      0.5   \n",
       "94  0.519913  0.500009  0.500007  ...  0.500000  0.543843  0.500000      0.5   \n",
       "95  0.504667  0.500008  0.500002  ...  0.500000  0.505431  0.500000      0.5   \n",
       "\n",
       "    chbeat1   vehpar1  sttwoo1  eurbla2   junmyn1  oripip1  \n",
       "0       0.5  0.500003      0.5      0.5  0.500001      0.5  \n",
       "1       0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "2       0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "3       0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "4       0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "..      ...       ...      ...      ...       ...      ...  \n",
       "91      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "92      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "93      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "94      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "95      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "\n",
       "[96 rows x 162 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Series\n",
    "new_column = pd.Series(clip_names_list, name='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0,'row_id',new_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>insbab1</th>\n",
       "      <th>whiter2</th>\n",
       "      <th>rocpig</th>\n",
       "      <th>blakit1</th>\n",
       "      <th>asbfly</th>\n",
       "      <th>litegr</th>\n",
       "      <th>houspa</th>\n",
       "      <th>comros</th>\n",
       "      <th>grnwar1</th>\n",
       "      <th>...</th>\n",
       "      <th>bncwoo3</th>\n",
       "      <th>malpar1</th>\n",
       "      <th>crbsun2</th>\n",
       "      <th>insowl1</th>\n",
       "      <th>chbeat1</th>\n",
       "      <th>vehpar1</th>\n",
       "      <th>sttwoo1</th>\n",
       "      <th>eurbla2</th>\n",
       "      <th>junmyn1</th>\n",
       "      <th>oripip1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soundscape_1000170626_5</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.502747</td>\n",
       "      <td>0.500176</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>0.500471</td>\n",
       "      <td>0.508517</td>\n",
       "      <td>0.532191</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>0.504719</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soundscape_1000170626_10</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.533823</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>0.500132</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.509141</td>\n",
       "      <td>0.516256</td>\n",
       "      <td>0.533277</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500295</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soundscape_1000170626_15</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.503245</td>\n",
       "      <td>0.500991</td>\n",
       "      <td>0.500040</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.504204</td>\n",
       "      <td>0.532776</td>\n",
       "      <td>0.537239</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.502176</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soundscape_1000170626_20</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.509151</td>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.501754</td>\n",
       "      <td>0.500065</td>\n",
       "      <td>0.506159</td>\n",
       "      <td>0.523961</td>\n",
       "      <td>0.536590</td>\n",
       "      <td>0.500011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.502940</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soundscape_1000170626_25</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.505027</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>0.501812</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.504708</td>\n",
       "      <td>0.535431</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510557</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>soundscape_1000389428_220</td>\n",
       "      <td>0.500452</td>\n",
       "      <td>0.527358</td>\n",
       "      <td>0.500230</td>\n",
       "      <td>0.501527</td>\n",
       "      <td>0.500256</td>\n",
       "      <td>0.524080</td>\n",
       "      <td>0.501026</td>\n",
       "      <td>0.510450</td>\n",
       "      <td>0.500033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.511648</td>\n",
       "      <td>0.500009</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>soundscape_1000389428_225</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.503903</td>\n",
       "      <td>0.500130</td>\n",
       "      <td>0.515409</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.522831</td>\n",
       "      <td>0.523866</td>\n",
       "      <td>0.512087</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.504960</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>soundscape_1000389428_230</td>\n",
       "      <td>0.500027</td>\n",
       "      <td>0.500318</td>\n",
       "      <td>0.500018</td>\n",
       "      <td>0.500239</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.509084</td>\n",
       "      <td>0.502225</td>\n",
       "      <td>0.506571</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.501020</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>soundscape_1000389428_235</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>0.500026</td>\n",
       "      <td>0.502344</td>\n",
       "      <td>0.500052</td>\n",
       "      <td>0.515034</td>\n",
       "      <td>0.524997</td>\n",
       "      <td>0.519913</td>\n",
       "      <td>0.500009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.543843</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>soundscape_1000389428_240</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.503317</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>0.500525</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.501473</td>\n",
       "      <td>0.503620</td>\n",
       "      <td>0.504667</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       row_id   insbab1   whiter2    rocpig   blakit1  \\\n",
       "0     soundscape_1000170626_5  0.500001  0.502747  0.500176  0.500365   \n",
       "1    soundscape_1000170626_10  0.500004  0.533823  0.500019  0.500132   \n",
       "2    soundscape_1000170626_15  0.500003  0.503245  0.500991  0.500040   \n",
       "3    soundscape_1000170626_20  0.500005  0.509151  0.500025  0.501754   \n",
       "4    soundscape_1000170626_25  0.500001  0.505027  0.500007  0.501812   \n",
       "..                        ...       ...       ...       ...       ...   \n",
       "91  soundscape_1000389428_220  0.500452  0.527358  0.500230  0.501527   \n",
       "92  soundscape_1000389428_225  0.500001  0.503903  0.500130  0.515409   \n",
       "93  soundscape_1000389428_230  0.500027  0.500318  0.500018  0.500239   \n",
       "94  soundscape_1000389428_235  0.500031  0.505785  0.500026  0.502344   \n",
       "95  soundscape_1000389428_240  0.500000  0.503317  0.500019  0.500525   \n",
       "\n",
       "      asbfly    litegr    houspa    comros   grnwar1  ...   bncwoo3   malpar1  \\\n",
       "0   0.500028  0.500471  0.508517  0.532191  0.500003  ...  0.500008  0.504719   \n",
       "1   0.500004  0.509141  0.516256  0.533277  0.500019  ...  0.500000  0.500295   \n",
       "2   0.500003  0.504204  0.532776  0.537239  0.500001  ...  0.500000  0.502176   \n",
       "3   0.500065  0.506159  0.523961  0.536590  0.500011  ...  0.500000  0.502940   \n",
       "4   0.500005  0.539216  0.504708  0.535431  0.500001  ...  0.500000  0.510557   \n",
       "..       ...       ...       ...       ...       ...  ...       ...       ...   \n",
       "91  0.500256  0.524080  0.501026  0.510450  0.500033  ...  0.500000  0.511648   \n",
       "92  0.500005  0.522831  0.523866  0.512087  0.500008  ...  0.500005  0.504960   \n",
       "93  0.500004  0.509084  0.502225  0.506571  0.500001  ...  0.500000  0.501020   \n",
       "94  0.500052  0.515034  0.524997  0.519913  0.500009  ...  0.500000  0.543843   \n",
       "95  0.500001  0.501473  0.503620  0.504667  0.500008  ...  0.500000  0.505431   \n",
       "\n",
       "     crbsun2  insowl1  chbeat1   vehpar1  sttwoo1  eurbla2   junmyn1  oripip1  \n",
       "0   0.500000      0.5      0.5  0.500003      0.5      0.5  0.500001      0.5  \n",
       "1   0.500000      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "2   0.500004      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "3   0.500005      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "4   0.500003      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "..       ...      ...      ...       ...      ...      ...       ...      ...  \n",
       "91  0.500009      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "92  0.500007      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "93  0.500000      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "94  0.500000      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "95  0.500000      0.5      0.5  0.500000      0.5      0.5  0.500000      0.5  \n",
       "\n",
       "[96 rows x 163 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
