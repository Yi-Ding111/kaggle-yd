{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is derived from the first version, removing unnecessary parts to make the process clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import pydub\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "import joblib\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torchmetrics\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping,Callback\n",
    "\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240420\n",
      "Is CUDA available:  False\n",
      "Is Metal available:  True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Is Metal available: \", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.backends.mps.is_available():\n",
    "#     device='mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_directory='../../data/train_audio'\n",
    "\n",
    "# sub_folders=glob.glob(os.path.join(parent_directory,'*'))\n",
    "\n",
    "# files=[]\n",
    "\n",
    "# for folder_path in sub_folders:\n",
    "#     first_file=glob.glob(os.path.join(folder_path,'*.ogg'))\n",
    "#     files.extend(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory='../../data/train_audio'\n",
    "\n",
    "sub_folder_names=['purswa3','spepic1','pabflo1']\n",
    "\n",
    "\n",
    "files=[]\n",
    "\n",
    "for i in sub_folder_names:\n",
    "\n",
    "    sub_folders=glob.glob(os.path.join(parent_directory,i))\n",
    "\n",
    "    for folder_path in sub_folders:\n",
    "        first_file=glob.glob(os.path.join(folder_path,'*.ogg'))\n",
    "        files.extend(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/train_audio/purswa3/XC857310.ogg',\n",
       " '../../data/train_audio/purswa3/XC857306.ogg',\n",
       " '../../data/train_audio/purswa3/XC579300.ogg',\n",
       " '../../data/train_audio/purswa3/XC857307.ogg',\n",
       " '../../data/train_audio/purswa3/XC822001.ogg',\n",
       " '../../data/train_audio/purswa3/XC778689.ogg',\n",
       " '../../data/train_audio/purswa3/XC722949.ogg',\n",
       " '../../data/train_audio/purswa3/XC778659.ogg',\n",
       " '../../data/train_audio/purswa3/XC604209.ogg',\n",
       " '../../data/train_audio/purswa3/XC784018.ogg',\n",
       " '../../data/train_audio/purswa3/XC429769.ogg',\n",
       " '../../data/train_audio/purswa3/XC684691.ogg',\n",
       " '../../data/train_audio/purswa3/XC382827.ogg',\n",
       " '../../data/train_audio/purswa3/XC278753.ogg',\n",
       " '../../data/train_audio/purswa3/XC775452.ogg',\n",
       " '../../data/train_audio/purswa3/XC458036.ogg',\n",
       " '../../data/train_audio/purswa3/XC129174.ogg',\n",
       " '../../data/train_audio/purswa3/XC715602.ogg',\n",
       " '../../data/train_audio/purswa3/XC382828.ogg',\n",
       " '../../data/train_audio/purswa3/XC284531.ogg',\n",
       " '../../data/train_audio/purswa3/XC586194.ogg',\n",
       " '../../data/train_audio/purswa3/XC605218.ogg',\n",
       " '../../data/train_audio/purswa3/XC858113.ogg',\n",
       " '../../data/train_audio/purswa3/XC548990.ogg',\n",
       " '../../data/train_audio/purswa3/XC573342.ogg',\n",
       " '../../data/train_audio/purswa3/XC283188.ogg',\n",
       " '../../data/train_audio/purswa3/XC537106.ogg',\n",
       " '../../data/train_audio/purswa3/XC196259.ogg',\n",
       " '../../data/train_audio/purswa3/XC496483.ogg',\n",
       " '../../data/train_audio/purswa3/XC797490.ogg',\n",
       " '../../data/train_audio/purswa3/XC196260.ogg',\n",
       " '../../data/train_audio/purswa3/XC701794.ogg',\n",
       " '../../data/train_audio/purswa3/XC453731.ogg',\n",
       " '../../data/train_audio/purswa3/XC667376.ogg',\n",
       " '../../data/train_audio/purswa3/XC196261.ogg',\n",
       " '../../data/train_audio/purswa3/XC713024.ogg',\n",
       " '../../data/train_audio/purswa3/XC772627.ogg',\n",
       " '../../data/train_audio/purswa3/XC857309.ogg',\n",
       " '../../data/train_audio/purswa3/XC857308.ogg',\n",
       " '../../data/train_audio/spepic1/XC460097.ogg',\n",
       " '../../data/train_audio/spepic1/XC783773.ogg',\n",
       " '../../data/train_audio/spepic1/XC768458.ogg',\n",
       " '../../data/train_audio/spepic1/XC305330.ogg',\n",
       " '../../data/train_audio/spepic1/XC282703.ogg',\n",
       " '../../data/train_audio/spepic1/XC767355.ogg',\n",
       " '../../data/train_audio/spepic1/XC706691.ogg',\n",
       " '../../data/train_audio/spepic1/XC653914.ogg',\n",
       " '../../data/train_audio/spepic1/XC23009.ogg',\n",
       " '../../data/train_audio/spepic1/XC706692.ogg',\n",
       " '../../data/train_audio/spepic1/XC313861.ogg',\n",
       " '../../data/train_audio/spepic1/XC705359.ogg',\n",
       " '../../data/train_audio/spepic1/XC706693.ogg',\n",
       " '../../data/train_audio/spepic1/XC305586.ogg',\n",
       " '../../data/train_audio/spepic1/XC239928.ogg',\n",
       " '../../data/train_audio/spepic1/XC788508.ogg',\n",
       " '../../data/train_audio/spepic1/XC686241.ogg',\n",
       " '../../data/train_audio/spepic1/XC348335.ogg',\n",
       " '../../data/train_audio/spepic1/XC787967.ogg',\n",
       " '../../data/train_audio/spepic1/XC784671.ogg',\n",
       " '../../data/train_audio/spepic1/XC783875.ogg',\n",
       " '../../data/train_audio/spepic1/XC783874.ogg',\n",
       " '../../data/train_audio/spepic1/XC589301.ogg',\n",
       " '../../data/train_audio/spepic1/XC784689.ogg',\n",
       " '../../data/train_audio/spepic1/XC804335.ogg',\n",
       " '../../data/train_audio/spepic1/XC806907.ogg',\n",
       " '../../data/train_audio/spepic1/XC806906.ogg',\n",
       " '../../data/train_audio/spepic1/XC613292.ogg',\n",
       " '../../data/train_audio/spepic1/XC804336.ogg',\n",
       " '../../data/train_audio/spepic1/XC115690.ogg',\n",
       " '../../data/train_audio/spepic1/XC23010.ogg',\n",
       " '../../data/train_audio/spepic1/XC616538.ogg',\n",
       " '../../data/train_audio/spepic1/XC842835.ogg',\n",
       " '../../data/train_audio/spepic1/XC804337.ogg',\n",
       " '../../data/train_audio/spepic1/XC845319.ogg',\n",
       " '../../data/train_audio/spepic1/XC629222.ogg',\n",
       " '../../data/train_audio/spepic1/XC328297.ogg',\n",
       " '../../data/train_audio/spepic1/XC303984.ogg',\n",
       " '../../data/train_audio/spepic1/XC802723.ogg',\n",
       " '../../data/train_audio/spepic1/XC492385.ogg',\n",
       " '../../data/train_audio/spepic1/XC804431.ogg',\n",
       " '../../data/train_audio/spepic1/XC768457.ogg',\n",
       " '../../data/train_audio/spepic1/XC705651.ogg',\n",
       " '../../data/train_audio/spepic1/XC787691.ogg',\n",
       " '../../data/train_audio/spepic1/XC804432.ogg',\n",
       " '../../data/train_audio/pabflo1/XC619048.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652897.ogg',\n",
       " '../../data/train_audio/pabflo1/XC756570.ogg',\n",
       " '../../data/train_audio/pabflo1/XC306985.ogg',\n",
       " '../../data/train_audio/pabflo1/XC756571.ogg',\n",
       " '../../data/train_audio/pabflo1/XC491069.ogg',\n",
       " '../../data/train_audio/pabflo1/XC582559.ogg',\n",
       " '../../data/train_audio/pabflo1/XC207734.ogg',\n",
       " '../../data/train_audio/pabflo1/XC741594.ogg',\n",
       " '../../data/train_audio/pabflo1/XC535681.ogg',\n",
       " '../../data/train_audio/pabflo1/XC442391.ogg',\n",
       " '../../data/train_audio/pabflo1/XC604022.ogg',\n",
       " '../../data/train_audio/pabflo1/XC741595.ogg',\n",
       " '../../data/train_audio/pabflo1/XC777755.ogg',\n",
       " '../../data/train_audio/pabflo1/XC476696.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652767.ogg',\n",
       " '../../data/train_audio/pabflo1/XC475572.ogg',\n",
       " '../../data/train_audio/pabflo1/XC346123.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652772.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652764.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652770.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652765.ogg',\n",
       " '../../data/train_audio/pabflo1/XC536071.ogg',\n",
       " '../../data/train_audio/pabflo1/XC207756.ogg',\n",
       " '../../data/train_audio/pabflo1/XC541564.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652775.ogg',\n",
       " '../../data/train_audio/pabflo1/XC681129.ogg',\n",
       " '../../data/train_audio/pabflo1/XC308276.ogg',\n",
       " '../../data/train_audio/pabflo1/XC632645.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652774.ogg',\n",
       " '../../data/train_audio/pabflo1/XC541567.ogg',\n",
       " '../../data/train_audio/pabflo1/XC778606.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652776.ogg',\n",
       " '../../data/train_audio/pabflo1/XC652777.ogg',\n",
       " '../../data/train_audio/pabflo1/XC267077.ogg',\n",
       " '../../data/train_audio/pabflo1/XC392178.ogg',\n",
       " '../../data/train_audio/pabflo1/XC19798.ogg',\n",
       " '../../data/train_audio/pabflo1/XC621595.ogg',\n",
       " '../../data/train_audio/pabflo1/XC475408.ogg',\n",
       " '../../data/train_audio/pabflo1/XC454349.ogg',\n",
       " '../../data/train_audio/pabflo1/XC646904.ogg',\n",
       " '../../data/train_audio/pabflo1/XC164024.ogg',\n",
       " '../../data/train_audio/pabflo1/XC207881.ogg',\n",
       " '../../data/train_audio/pabflo1/XC665955.ogg',\n",
       " '../../data/train_audio/pabflo1/XC392125.ogg',\n",
       " '../../data/train_audio/pabflo1/XC424194.ogg',\n",
       " '../../data/train_audio/pabflo1/XC406120.ogg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/train_audio/purswa3/XC857308.ogg', '../../data/train_audio/purswa3/XC196261.ogg', '../../data/train_audio/spepic1/XC787691.ogg', '../../data/train_audio/purswa3/XC579300.ogg', '../../data/train_audio/purswa3/XC857308.ogg', '../../data/train_audio/purswa3/XC382827.ogg', '../../data/train_audio/purswa3/XC784018.ogg', '../../data/train_audio/purswa3/XC382828.ogg', '../../data/train_audio/spepic1/XC804336.ogg', '../../data/train_audio/purswa3/XC579300.ogg', '../../data/train_audio/purswa3/XC278753.ogg', '../../data/train_audio/pabflo1/XC164024.ogg', '../../data/train_audio/purswa3/XC715602.ogg', '../../data/train_audio/pabflo1/XC741594.ogg', '../../data/train_audio/spepic1/XC706693.ogg', '../../data/train_audio/pabflo1/XC652774.ogg', '../../data/train_audio/pabflo1/XC665955.ogg', '../../data/train_audio/pabflo1/XC632645.ogg', '../../data/train_audio/spepic1/XC768458.ogg', '../../data/train_audio/spepic1/XC305330.ogg']\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "random_files=[]\n",
    "while i<20:\n",
    "    idx=random.randint(0,len(files)-1)\n",
    "    random_files.append(files[idx])\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(random_files)\n",
    "\n",
    "remain_files = [file for file in files if file not in random_files]\n",
    "\n",
    "files=remain_files\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split train and val paths\n",
    "# # i decide to randomly choose one single file from each category folder as val path, and the left part as train paths\n",
    "\n",
    "# parent_directory='../../data/train'\n",
    "\n",
    "# sub_folders=glob.glob(os.path.join(parent_directory,'*'))\n",
    "\n",
    "\n",
    "# # Used to store randomly selected file paths\n",
    "# random_files = []\n",
    "# # Path to store remaining files\n",
    "# files=[] \n",
    "\n",
    "# # Loop through each subfolder\n",
    "# for folder in sub_folders:\n",
    "#     # Get all file paths in subfolders\n",
    "#     all_files = glob.glob(os.path.join(folder, '*'))\n",
    "#     if all_files:  # Make sure the folder is not empty\n",
    "#         # Random select a file from the file list\n",
    "#         chosen_file = random.choice(all_files)\n",
    "#         # Add to random file list\n",
    "#         random_files.append(chosen_file)\n",
    "#         # Add the remaining files to another list\n",
    "#         files.extend([file for file in all_files if file != chosen_file])\n",
    "\n",
    "# print(\"Randomly selected files:\", random_files)\n",
    "# print(\"Remaining files:\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path:str)->pydub.audio_segment.AudioSegment:\n",
    "    \"\"\"\n",
    "    read ogg file as pydub.audio_segment.AudioSegment for the following steps\n",
    "\n",
    "    parametere:\n",
    "        path: *.ogg file path\n",
    "\n",
    "    return\n",
    "        audio: the readed audio data\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(path, format=\"ogg\")\n",
    "\n",
    "    return audio    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regarding the data of a single audio, some audio information needs to be paid attention to, such as audio duration, sampling rate, bit rate and number of channels.\n",
    "\n",
    "def audio_info(audio:pydub.audio_segment.AudioSegment):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio\n",
    "\n",
    "    Parameters:\n",
    "        Audio: the readed audio data\n",
    "\n",
    "    Return:\n",
    "        the information of the audio\n",
    "    \"\"\"\n",
    "    # the audio duration time (seconds)\n",
    "    duration_seconds=len(audio)/1000.0\n",
    "\n",
    "    # the audio sampling rate\n",
    "    sr=audio.frame_rate\n",
    "\n",
    "    # the num of channels\n",
    "    num_channels=audio.channels\n",
    "\n",
    "    #bit rate\n",
    "    bit_rate=audio.sample_width * 8\n",
    "\n",
    "    return duration_seconds, sr, num_channels, bit_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert audio data into array\n",
    "\n",
    "def audio2array(audio_slices:list)->np.array:\n",
    "    \"\"\"\n",
    "    transform audio segments to arrays\n",
    "    \"\"\"\n",
    "    audio_arrays=np.array([np.array(audio_slice.get_array_of_samples()) for audio_slice in audio_slices])\n",
    "\n",
    "    return audio_arrays\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_audio_5_align(audio:pydub.audio_segment.AudioSegment)->List[pydub.audio_segment.AudioSegment]:\n",
    "    \"\"\"\n",
    "    Slice the complete audio into multiple 5 seconds length,\n",
    "    keep all slice have the same length, especially for the last slice\n",
    "\n",
    "    Return the list of all audio segments\n",
    "    \"\"\"\n",
    "    # set up the segment duration\n",
    "    # Set up the segment duration\n",
    "    segment_duration = 5 * 1000  # 5 seconds in milliseconds\n",
    "\n",
    "    # Check if the audio is less than 5 seconds\n",
    "    if len(audio) < segment_duration:\n",
    "        # Calculate the required padding length\n",
    "        padding_length = segment_duration - len(audio)\n",
    "        # Create a silent audio segment for padding\n",
    "        silence = AudioSegment.silent(duration=padding_length)\n",
    "        # Pad the audio with silence\n",
    "        padded_audio = audio + silence\n",
    "        return [padded_audio]  # Return the padded audio as a single segment\n",
    "\n",
    "    # If the audio is 5 seconds or longer, proceed as normal\n",
    "    segments = [audio[i:i + segment_duration] for i in range(0, len(audio), segment_duration)]\n",
    "\n",
    "    # Ensure the last segment is exactly 5 seconds long\n",
    "    if len(segments[-1]) != segment_duration:\n",
    "        last_segment = audio[-segment_duration:]  # Get the last 5 seconds of the audio\n",
    "        segments[-1] = last_segment  # Replace the last segment with a full 5-second segment\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_bitrate_norm(bit_rate:float,audio_array:np.array):\n",
    "    \"\"\"\n",
    "    because the .ogg file readed through pydub would based off the audio original bit rate,\n",
    "    we want the value of the audio keep small, \n",
    "    so do normalization based off the bit rate.\n",
    "\n",
    "    Parameters:\n",
    "        bit_rate: the bit rate of the audio\n",
    "        audio_array: the data in array form for each single slice\n",
    "    \"\"\"\n",
    "    audio_array_norm = audio_array / float(2**(bit_rate-1))\n",
    "\n",
    "    return audio_array_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_random_sampling_2(total_samples:int,audio_segment:pydub.audio_segment.AudioSegment)-> List[pydub.audio_segment.AudioSegment]:\n",
    "    \"\"\"\n",
    "    Randomly extract audio clips and combine them into 2 seconds of audio\n",
    "\n",
    "    Parameters:\n",
    "        total_samples: the number of randomly synthesized audio clips\n",
    "        audio_segment: the single audio segment in form `pydub.audio_segment.AudioSegment`\n",
    "\n",
    "    Return:\n",
    "        The list of all random extract audio clips in form `pydub.audio_segment.AudioSegment`\n",
    "\n",
    "    \"\"\"\n",
    "    random_clips=[]\n",
    "    clip_num=0\n",
    "\n",
    "    # Our goal is to randomly extract a total of 2 seconds of audio\n",
    "    total_duration_ms = 2*1000\n",
    "\n",
    "    while clip_num<total_samples:\n",
    "        #Store the extracted fragments\n",
    "        extracted_segments = AudioSegment.silent(duration=0)  # Create a silent segment for subsequent splicing\n",
    "\n",
    "        # Continue looping when the total length of the extracted segments is less than 2 seconds\n",
    "        while extracted_segments.duration_seconds < 2:\n",
    "            # Random choose a starting point\n",
    "            start_ms = random.randint(0, len(audio_segment) - 1)\n",
    "            # Calculate the maximum duration that can be extracted\n",
    "            max_extract_ms = total_duration_ms - int(extracted_segments.duration_seconds * 1000)\n",
    "            # Random determine the duration of this draw\n",
    "            extract_duration_ms = random.randint(1, max_extract_ms)\n",
    "            # Random extracted fragments\n",
    "            extract = audio_segment[start_ms:start_ms+extract_duration_ms]\n",
    "            # concat the extracted fragments\n",
    "            extracted_segments += extract\n",
    "\n",
    "        random_two_seconds = extracted_segments\n",
    "\n",
    "        random_clips.append(random_two_seconds)\n",
    "\n",
    "        clip_num+=1\n",
    "\n",
    "    return random_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeD2preqD(normalized_clip:np.array,sr:int):\n",
    "    \"\"\"\n",
    "    transform audio from time domain to frequency domain\n",
    "\n",
    "    Parameters:\n",
    "        normalized_clip: the single clip in array format (data)\n",
    "        sr: sampling rate\n",
    "\n",
    "    Return:\n",
    "        Due to symmetry, only half the spectrum is needed.\n",
    "        And becuase the range of frequencies depends on the sampling rate of the audio signal.\n",
    "        we do not need the frequency info, all of them are the same.\n",
    "    \"\"\"\n",
    "    fft = np.fft.fft(normalized_clip)\n",
    "    magnitude = np.abs(fft)\n",
    "    frequency = np.linspace(0, sr, len(magnitude))\n",
    "\n",
    "    half_len = len(magnitude) // 2 \n",
    "    frequency=frequency[:half_len]\n",
    "    magnitude=magnitude[:half_len]\n",
    "\n",
    "    return magnitude,frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/train_audio/purswa3/XC857310.ogg\n",
      "../../data/train_audio/purswa3/XC857306.ogg\n",
      "../../data/train_audio/purswa3/XC857307.ogg\n",
      "../../data/train_audio/purswa3/XC822001.ogg\n",
      "../../data/train_audio/purswa3/XC778689.ogg\n",
      "../../data/train_audio/purswa3/XC722949.ogg\n",
      "../../data/train_audio/purswa3/XC778659.ogg\n",
      "../../data/train_audio/purswa3/XC604209.ogg\n",
      "../../data/train_audio/purswa3/XC429769.ogg\n",
      "../../data/train_audio/purswa3/XC684691.ogg\n",
      "../../data/train_audio/purswa3/XC775452.ogg\n",
      "../../data/train_audio/purswa3/XC458036.ogg\n",
      "../../data/train_audio/purswa3/XC129174.ogg\n",
      "../../data/train_audio/purswa3/XC284531.ogg\n",
      "../../data/train_audio/purswa3/XC586194.ogg\n",
      "../../data/train_audio/purswa3/XC605218.ogg\n",
      "../../data/train_audio/purswa3/XC858113.ogg\n",
      "../../data/train_audio/purswa3/XC548990.ogg\n",
      "../../data/train_audio/purswa3/XC573342.ogg\n",
      "../../data/train_audio/purswa3/XC283188.ogg\n",
      "../../data/train_audio/purswa3/XC537106.ogg\n",
      "../../data/train_audio/purswa3/XC196259.ogg\n",
      "../../data/train_audio/purswa3/XC496483.ogg\n",
      "../../data/train_audio/purswa3/XC797490.ogg\n",
      "../../data/train_audio/purswa3/XC196260.ogg\n",
      "../../data/train_audio/purswa3/XC701794.ogg\n",
      "../../data/train_audio/purswa3/XC453731.ogg\n",
      "../../data/train_audio/purswa3/XC667376.ogg\n",
      "../../data/train_audio/purswa3/XC713024.ogg\n",
      "../../data/train_audio/purswa3/XC772627.ogg\n",
      "../../data/train_audio/purswa3/XC857309.ogg\n",
      "../../data/train_audio/spepic1/XC460097.ogg\n",
      "../../data/train_audio/spepic1/XC783773.ogg\n",
      "../../data/train_audio/spepic1/XC282703.ogg\n",
      "../../data/train_audio/spepic1/XC767355.ogg\n",
      "../../data/train_audio/spepic1/XC706691.ogg\n",
      "../../data/train_audio/spepic1/XC653914.ogg\n",
      "../../data/train_audio/spepic1/XC23009.ogg\n",
      "../../data/train_audio/spepic1/XC706692.ogg\n",
      "../../data/train_audio/spepic1/XC313861.ogg\n",
      "../../data/train_audio/spepic1/XC705359.ogg\n",
      "../../data/train_audio/spepic1/XC305586.ogg\n",
      "../../data/train_audio/spepic1/XC239928.ogg\n",
      "../../data/train_audio/spepic1/XC788508.ogg\n",
      "../../data/train_audio/spepic1/XC686241.ogg\n",
      "../../data/train_audio/spepic1/XC348335.ogg\n",
      "../../data/train_audio/spepic1/XC787967.ogg\n",
      "../../data/train_audio/spepic1/XC784671.ogg\n",
      "../../data/train_audio/spepic1/XC783875.ogg\n",
      "../../data/train_audio/spepic1/XC783874.ogg\n",
      "../../data/train_audio/spepic1/XC589301.ogg\n",
      "../../data/train_audio/spepic1/XC784689.ogg\n",
      "../../data/train_audio/spepic1/XC804335.ogg\n",
      "../../data/train_audio/spepic1/XC806907.ogg\n",
      "../../data/train_audio/spepic1/XC806906.ogg\n",
      "../../data/train_audio/spepic1/XC613292.ogg\n",
      "../../data/train_audio/spepic1/XC115690.ogg\n",
      "../../data/train_audio/spepic1/XC23010.ogg\n",
      "../../data/train_audio/spepic1/XC616538.ogg\n",
      "../../data/train_audio/spepic1/XC842835.ogg\n",
      "../../data/train_audio/spepic1/XC804337.ogg\n",
      "../../data/train_audio/spepic1/XC845319.ogg\n",
      "../../data/train_audio/spepic1/XC629222.ogg\n",
      "../../data/train_audio/spepic1/XC328297.ogg\n",
      "../../data/train_audio/spepic1/XC303984.ogg\n",
      "../../data/train_audio/spepic1/XC802723.ogg\n",
      "../../data/train_audio/spepic1/XC492385.ogg\n",
      "../../data/train_audio/spepic1/XC804431.ogg\n",
      "../../data/train_audio/spepic1/XC768457.ogg\n",
      "../../data/train_audio/spepic1/XC705651.ogg\n",
      "../../data/train_audio/spepic1/XC804432.ogg\n",
      "../../data/train_audio/pabflo1/XC619048.ogg\n",
      "../../data/train_audio/pabflo1/XC652897.ogg\n",
      "../../data/train_audio/pabflo1/XC756570.ogg\n",
      "../../data/train_audio/pabflo1/XC306985.ogg\n",
      "../../data/train_audio/pabflo1/XC756571.ogg\n",
      "../../data/train_audio/pabflo1/XC491069.ogg\n",
      "../../data/train_audio/pabflo1/XC582559.ogg\n",
      "../../data/train_audio/pabflo1/XC207734.ogg\n",
      "../../data/train_audio/pabflo1/XC535681.ogg\n",
      "../../data/train_audio/pabflo1/XC442391.ogg\n",
      "../../data/train_audio/pabflo1/XC604022.ogg\n",
      "../../data/train_audio/pabflo1/XC741595.ogg\n",
      "../../data/train_audio/pabflo1/XC777755.ogg\n",
      "../../data/train_audio/pabflo1/XC476696.ogg\n",
      "../../data/train_audio/pabflo1/XC652767.ogg\n",
      "../../data/train_audio/pabflo1/XC475572.ogg\n",
      "../../data/train_audio/pabflo1/XC346123.ogg\n",
      "../../data/train_audio/pabflo1/XC652772.ogg\n",
      "../../data/train_audio/pabflo1/XC652764.ogg\n",
      "../../data/train_audio/pabflo1/XC652770.ogg\n",
      "../../data/train_audio/pabflo1/XC652765.ogg\n",
      "../../data/train_audio/pabflo1/XC536071.ogg\n",
      "../../data/train_audio/pabflo1/XC207756.ogg\n",
      "../../data/train_audio/pabflo1/XC541564.ogg\n",
      "../../data/train_audio/pabflo1/XC652775.ogg\n",
      "../../data/train_audio/pabflo1/XC681129.ogg\n",
      "../../data/train_audio/pabflo1/XC308276.ogg\n",
      "../../data/train_audio/pabflo1/XC541567.ogg\n",
      "../../data/train_audio/pabflo1/XC778606.ogg\n",
      "../../data/train_audio/pabflo1/XC652776.ogg\n",
      "../../data/train_audio/pabflo1/XC652777.ogg\n",
      "../../data/train_audio/pabflo1/XC267077.ogg\n",
      "../../data/train_audio/pabflo1/XC392178.ogg\n",
      "../../data/train_audio/pabflo1/XC19798.ogg\n",
      "../../data/train_audio/pabflo1/XC621595.ogg\n",
      "../../data/train_audio/pabflo1/XC475408.ogg\n",
      "../../data/train_audio/pabflo1/XC454349.ogg\n",
      "../../data/train_audio/pabflo1/XC646904.ogg\n",
      "../../data/train_audio/pabflo1/XC207881.ogg\n",
      "../../data/train_audio/pabflo1/XC392125.ogg\n",
      "../../data/train_audio/pabflo1/XC424194.ogg\n",
      "../../data/train_audio/pabflo1/XC406120.ogg\n"
     ]
    }
   ],
   "source": [
    "labels_list=[]\n",
    "all_audios_magnitude=[]\n",
    "for path in files:\n",
    "    print(path)\n",
    "    labels=[]\n",
    "    # extract label\n",
    "    label=path.split('/')[-2]\n",
    "\n",
    "    # read audio\n",
    "    audio=read_audio(path)\n",
    "\n",
    "    # grab audio information\n",
    "    duration_secconds,sr,num_channels,bit_rate=audio_info(audio)\n",
    "\n",
    "    # slice audio into multi 5 seconds\n",
    "    slice_5_all=slice_audio_5_align(audio)\n",
    "\n",
    "    for single_slice in slice_5_all:\n",
    "\n",
    "        # slice audio on each 5 sec long audio and generate multi 2 sec clips\n",
    "        random_clips=audio_random_sampling_2(total_samples=5,audio_segment=single_slice)\n",
    "\n",
    "        # convert all 2 sec clips to array format\n",
    "        audio_arrays_2sec=audio2array(random_clips)\n",
    "\n",
    "        # normalize each 2 sec audio array \n",
    "        arrays_2sec_norm=[]\n",
    "        for i in audio_arrays_2sec:\n",
    "            array_2sec_norm=audio_bitrate_norm(bit_rate=bit_rate,audio_array=i)\n",
    "            arrays_2sec_norm.append(array_2sec_norm)\n",
    "        \n",
    "        # arrays_2sec_norm=np.array(arrays_2sec_norm)\n",
    "\n",
    "        # conver audio from time domain to frequency domain\n",
    "        audios_magnitude=[]\n",
    "        for i in arrays_2sec_norm:\n",
    "            magnitude,frequency=timeD2preqD(normalized_clip=i,sr=sr)\n",
    "            audios_magnitude.append(magnitude)\n",
    "\n",
    "        # audios_magnitude=np.array(audios_magnitude)\n",
    "\n",
    "        labels.append(label)\n",
    "        all_audios_magnitude.append(audios_magnitude)\n",
    "\n",
    "\n",
    "    # all_audios_magnitude=np.array(all_audios_magnitude)\n",
    "\n",
    "    \n",
    "    labels_list.append(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/train_audio/purswa3/XC857308.ogg\n",
      "../../data/train_audio/purswa3/XC196261.ogg\n",
      "../../data/train_audio/spepic1/XC787691.ogg\n",
      "../../data/train_audio/purswa3/XC579300.ogg\n",
      "../../data/train_audio/purswa3/XC857308.ogg\n",
      "../../data/train_audio/purswa3/XC382827.ogg\n",
      "../../data/train_audio/purswa3/XC784018.ogg\n",
      "../../data/train_audio/purswa3/XC382828.ogg\n",
      "../../data/train_audio/spepic1/XC804336.ogg\n",
      "../../data/train_audio/purswa3/XC579300.ogg\n",
      "../../data/train_audio/purswa3/XC278753.ogg\n",
      "../../data/train_audio/pabflo1/XC164024.ogg\n",
      "../../data/train_audio/purswa3/XC715602.ogg\n",
      "../../data/train_audio/pabflo1/XC741594.ogg\n",
      "../../data/train_audio/spepic1/XC706693.ogg\n",
      "../../data/train_audio/pabflo1/XC652774.ogg\n",
      "../../data/train_audio/pabflo1/XC665955.ogg\n",
      "../../data/train_audio/pabflo1/XC632645.ogg\n",
      "../../data/train_audio/spepic1/XC768458.ogg\n",
      "../../data/train_audio/spepic1/XC305330.ogg\n"
     ]
    }
   ],
   "source": [
    "# same step for val-set prepare\n",
    "\n",
    "val_labels_list=[]\n",
    "val_all_audios_magnitude=[]\n",
    "for path in random_files:\n",
    "    print(path)\n",
    "    labels=[]\n",
    "    # extract label\n",
    "    label=path.split('/')[-2]\n",
    "\n",
    "    # read audio\n",
    "    audio=read_audio(path)\n",
    "\n",
    "    # grab audio information\n",
    "    duration_secconds,sr,num_channels,bit_rate=audio_info(audio)\n",
    "\n",
    "    # slice audio into multi 5 seconds\n",
    "    slice_5_all=slice_audio_5_align(audio)\n",
    "\n",
    "    for single_slice in slice_5_all:\n",
    "\n",
    "        # slice audio on each 5 sec long audio and generate multi 2 sec clips\n",
    "        random_clips=audio_random_sampling_2(total_samples=5,audio_segment=single_slice)\n",
    "\n",
    "        # convert all 2 sec clips to array format\n",
    "        audio_arrays_2sec=audio2array(random_clips)\n",
    "\n",
    "        # normalize each 2 sec audio array \n",
    "        arrays_2sec_norm=[]\n",
    "        for i in audio_arrays_2sec:\n",
    "            array_2sec_norm=audio_bitrate_norm(bit_rate=bit_rate,audio_array=i)\n",
    "            arrays_2sec_norm.append(array_2sec_norm)\n",
    "        \n",
    "        # arrays_2sec_norm=np.array(arrays_2sec_norm)\n",
    "\n",
    "        # conver audio from time domain to frequency domain\n",
    "        audios_magnitude=[]\n",
    "        for i in arrays_2sec_norm:\n",
    "            magnitude,frequency=timeD2preqD(normalized_clip=i,sr=sr)\n",
    "            audios_magnitude.append(magnitude)\n",
    "\n",
    "        # audios_magnitude=np.array(audios_magnitude)\n",
    "\n",
    "        labels.append(label)\n",
    "        val_all_audios_magnitude.append(audios_magnitude)\n",
    "\n",
    "\n",
    "    # all_audios_magnitude=np.array(all_audios_magnitude)\n",
    "\n",
    "    \n",
    "    val_labels_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_flatten_list = [element for sublist in labels_list for element in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audios_magnitude=np.array(all_audios_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_flatten_list = [element for sublist in val_labels_list for element in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_all_audios_magnitude=np.array(val_all_audios_magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize label encoder\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "# use Labelencoder to transform labels\n",
    "encoded_labels=encoder.fit_transform(labels_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pabflo1': 0, 'purswa3': 1, 'spepic1': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If needed, you can view the mapping of original labels to encodings\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then use the loaded encoder to encode val set labels\n",
    "\n",
    "val_encoded_labels=encoder.transform(val_labels_flatten_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "all_audios_magnitude_norm=scaler.fit_transform(all_audios_magnitude.reshape(-1,all_audios_magnitude.shape[-1])).reshape(all_audios_magnitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_audios_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_all_audios_magnitude_norm = scaler.transform(val_all_audios_magnitude.reshape(-1, val_all_audios_magnitude.shape[-1])).reshape(val_all_audios_magnitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_all_audios_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.Tensor(all_audios_magnitude_norm)\n",
    "val_features = torch.Tensor(val_all_audios_magnitude_norm)\n",
    "train_labels = torch.Tensor(encoded_labels)\n",
    "val_labels = torch.Tensor(val_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([779, 5, 32000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build up neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=5,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=5,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "        self.conv3=nn.Conv1d(in_channels=5,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "    def forward(self,x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x)\n",
    "        x3=self.conv3(x)\n",
    "        # The length of the input data shape is 32000, and the stride is 2, so after conv1d, the length becomes 16000\n",
    "        # The number of output channels of each conv1d layer is 32, so for the shape of the entire output, regardless of batchsize, it is 32*16000\n",
    "        # Because of the chrononet architecture, we need to connect the outputs of the three layers to become 96*16000 output data.\n",
    "        x=torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "        self.conv3=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x)\n",
    "        x3=self.conv3(x)\n",
    "        # From the output of ConvBlock1, we know that the input shape of convBlock2 is 96*16000\n",
    "        # After the calculation of this block, the output will become 96*8000\n",
    "        x=torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "        self.conv3=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x)\n",
    "        x3=self.conv3(x)\n",
    "        # From the output of ConvBlock1, we know that the input shape of convBlock2 is 96*8000\n",
    "        # After the calculation of this block, the output will become 96*4000\n",
    "        x=torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we need to train the model first, we need to calculate the loss\n",
    "# For multi-classification problems, if you choose to use nn.crossentropylss, you need to remove F.softmax(),\n",
    "# Because this loss function combines Log-Softmax and NLL Loss (Negative Log Likelihood Loss).\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=4000)\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=4000)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=4000)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=4000)\n",
    "        self.gru4=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=4000)\n",
    "        self.fc1=nn.Linear(in_features=32,out_features=64)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 3)  # num_classes is the number of categories\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "\n",
    "        # add batch normalization\n",
    "        x=self.bn1(x)\n",
    "        # add relu activation fucntion\n",
    "        x = F.relu(x) \n",
    "\n",
    "        gru_out1,_=self.gru1(x)\n",
    "\n",
    "        x=self.bn2(gru_out1)\n",
    "        x=F.relu(x)\n",
    "\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "\n",
    "        x=self.bn3(gru_out2)\n",
    "        x=F.relu(x)\n",
    "\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "\n",
    "        x=self.bn4(gru_out3)\n",
    "        x=F.relu(x)\n",
    "\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "        gru_out4,_=self.gru4(x)\n",
    "\n",
    "        x=self.bn5(gru_out4)\n",
    "        x=F.relu(x)\n",
    "\n",
    "        x = self.fc1(gru_out4[:, -1, :]) \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNetModule(L.LightningModule):\n",
    "    def __init__(self,model,learning_rate):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        self.lr=learning_rate\n",
    "        self.train_acc=torchmetrics.Accuracy(task='multiclass',num_classes=3)\n",
    "        self.val_acc=torchmetrics.Accuracy(task='multiclass',num_classes=3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x: feature data for training \n",
    "\n",
    "        This is the part of the neural model that is used to read or build\n",
    "        define the computation performed at every call define the computation performed at every call\n",
    "\n",
    "        return:\n",
    "            model's output\n",
    "        '''\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        we need to train the model right here\n",
    "        including provide the loss step, acc calculation step\n",
    "\n",
    "        This function will perform the following operations:\n",
    "        1. Calculate the loss value for each training batch\n",
    "        2. Perform optimization and gradient descent (automatically performed by lightningModule)\n",
    "        3. Update parameters (automatically performed by lightningModule)\n",
    "        https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#training\n",
    "        '''\n",
    "        # read batch data\n",
    "        features,labels=batch\n",
    "\n",
    "        # send data to GPU for training \n",
    "        features=features.to(self.device)\n",
    "        labels=labels.to(self.device)\n",
    "        \n",
    "        # feeding feature to the model\n",
    "        # Only self() is used here because the forward() function is called automatically\n",
    "        # forward propagation\n",
    "        out=self(features)\n",
    "\n",
    "        # After getting the output of the model, you need to calculate the loss function\n",
    "        loss=F.cross_entropy(out, labels)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # After adding self.train_acc=torchmetrics.Accuracy(task='multiclass',num_classes=6)\n",
    "        # You can add the steps to calculate accuracy below\n",
    "        # Because we use cross_entropy() as the loss function\n",
    "        # So we need to use argmax to convert to normal values ​​for accuracy calculation\n",
    "        # predicted_labels=torch.argmax(out)\n",
    "        # But torchmetrics.Accuracy is already configured to handle logits for multi-class classification problems. \n",
    "        # It will apply softmax (or log_softmax) and calculate argmax internally to determine the most likely category.\n",
    "        acc=self.train_acc(out,labels)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "\n",
    "        # In training_step(), we only calculate and return the loss. \n",
    "        # The optimization part does not belong to this part, and the optimization method will be defined in configure_optimizers.\n",
    "        return loss # this is passed to the optimizer for training\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        val step is not used in traning, only in validation\n",
    "        '''\n",
    "        features,labels=batch\n",
    "\n",
    "        # send data to GPU for training\n",
    "        features=features.to(self.device)\n",
    "        labels=labels.to(self.device)\n",
    "        \n",
    "        out=self(features)\n",
    "        loss=F.cross_entropy(out, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        acc=self.val_acc(out,labels)\n",
    "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "\n",
    "        The optimizer defined here will be automatically called by lightningModule\n",
    "        Used in the training step\n",
    "        '''\n",
    "        optimizer=torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # If you have only one tensor (feature) in your TensorDataset, batch will be a tuple containing a tensor and an empty tuple (since there are no labels)\n",
    "        features= batch[0]\n",
    "        features=features.to(self.device)\n",
    "        predictions = self(features)\n",
    "        # Because what our model ultimately wants is the probability of an object corresponding to all categories, so add the softmax function here\n",
    "        probabilities = torch.softmax(predictions, dim=1)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNetDataModule(L.LightningDataModule):\n",
    "    def __init__(self,train=None,train_label=None,val=None,val_label=None,pred=None,batch_size:int=32,num_workers:int=12):\n",
    "        super().__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.num_workers=num_workers\n",
    "        self.train=train\n",
    "        self.train_label=train_label\n",
    "        self.val=val\n",
    "        self.val_label=val_label\n",
    "        self.pred=pred\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # You need to create a tensor to include the data and labels together\n",
    "        # The length of the first dimension of the two must be equal\n",
    "        dataset=TensorDataset(self.train,self.train_label)\n",
    "        loader= DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True, shuffle=True)\n",
    "\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset=TensorDataset(self.val,self.val_label)\n",
    "        loader= DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True,shuffle=False)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataset=TensorDataset(self.pred)\n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,shuffle=False)\n",
    "\n",
    "        return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ChronoNetDataModule object at 0x2ab6bc370>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/chrononet\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ChronoNet          | 171 K \n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "171 K     Trainable params\n",
      "0         Non-trainable params\n",
      "171 K     Total params\n",
      "0.686     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 25/25 [12:25<00:00,  0.03it/s, v_num=0, train_loss_step=0.937, train_acc_step=0.636, val_loss_step=0.656, val_acc_step=0.818, val_loss_epoch=1.170, val_acc_epoch=0.299, train_loss_epoch=1.040, train_acc_epoch=0.506]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 25/25 [12:25<00:00,  0.03it/s, v_num=0, train_loss_step=0.937, train_acc_step=0.636, val_loss_step=0.656, val_acc_step=0.818, val_loss_epoch=1.170, val_acc_epoch=0.299, train_loss_epoch=1.040, train_acc_epoch=0.506]\n"
     ]
    }
   ],
   "source": [
    "# Previous we used a separate dataloader to feed the model\n",
    "# Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "dm=ChronoNetDataModule(train=train_features,train_label=train_labels,val=val_features,val_label=val_labels,batch_size=32).to(device)\n",
    "print(dm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=ChronoNet()\n",
    "ChronoNetModule=ChronoNetModule(model=model,learning_rate=0.001)\n",
    "\n",
    "trainer=L.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "    devices=1, # use all gpus if applicable like value=1 or \"auto\"\n",
    "    default_root_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/',\n",
    "    logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    model=ChronoNetModule,\n",
    "    datamodule=dm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 4/4 [00:39<00:00,  0.10it/s]\n"
     ]
    }
   ],
   "source": [
    "dm=ChronoNetDataModule(pred=val_features,batch_size=33)\n",
    "\n",
    "predictions=trainer.predict(model=ChronoNetModule,datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4965, 0.2688, 0.2347],\n",
      "        [0.3273, 0.2583, 0.4144],\n",
      "        [0.5006, 0.2680, 0.2314],\n",
      "        [0.4999, 0.2684, 0.2317],\n",
      "        [0.4985, 0.2676, 0.2339],\n",
      "        [0.5009, 0.2680, 0.2311],\n",
      "        [0.3749, 0.2693, 0.3558],\n",
      "        [0.5011, 0.2682, 0.2307],\n",
      "        [0.5013, 0.2683, 0.2305],\n",
      "        [0.4955, 0.2667, 0.2378],\n",
      "        [0.5013, 0.2682, 0.2304],\n",
      "        [0.4986, 0.2675, 0.2339],\n",
      "        [0.4995, 0.2677, 0.2328],\n",
      "        [0.4964, 0.2676, 0.2360],\n",
      "        [0.5010, 0.2683, 0.2307],\n",
      "        [0.4113, 0.2699, 0.3188],\n",
      "        [0.1827, 0.2095, 0.6078],\n",
      "        [0.1832, 0.2101, 0.6067],\n",
      "        [0.1985, 0.2183, 0.5832],\n",
      "        [0.2141, 0.2260, 0.5600],\n",
      "        [0.3266, 0.2601, 0.4133],\n",
      "        [0.1779, 0.2066, 0.6154],\n",
      "        [0.3698, 0.2665, 0.3637],\n",
      "        [0.3503, 0.2632, 0.3865],\n",
      "        [0.5013, 0.2683, 0.2305],\n",
      "        [0.5006, 0.2683, 0.2311],\n",
      "        [0.3738, 0.2686, 0.3576],\n",
      "        [0.4989, 0.2675, 0.2336],\n",
      "        [0.5004, 0.2679, 0.2316],\n",
      "        [0.4665, 0.2612, 0.2723],\n",
      "        [0.4928, 0.2662, 0.2410],\n",
      "        [0.4853, 0.2649, 0.2498],\n",
      "        [0.4900, 0.2654, 0.2446],\n",
      "        [0.4909, 0.2658, 0.2433],\n",
      "        [0.4862, 0.2654, 0.2484],\n",
      "        [0.4942, 0.2665, 0.2394],\n",
      "        [0.1815, 0.2087, 0.6098],\n",
      "        [0.1764, 0.2057, 0.6179],\n",
      "        [0.1751, 0.2050, 0.6199],\n",
      "        [0.1787, 0.2072, 0.6141],\n",
      "        [0.1750, 0.2051, 0.6200],\n",
      "        [0.1736, 0.2039, 0.6225],\n",
      "        [0.1732, 0.2038, 0.6230],\n",
      "        [0.1775, 0.2064, 0.6161],\n",
      "        [0.1868, 0.2121, 0.6011],\n",
      "        [0.1912, 0.2142, 0.5946],\n",
      "        [0.1828, 0.2097, 0.6074],\n",
      "        [0.1845, 0.2106, 0.6049],\n",
      "        [0.1846, 0.2111, 0.6043],\n",
      "        [0.1847, 0.2108, 0.6045],\n",
      "        [0.1806, 0.2085, 0.6109],\n",
      "        [0.4858, 0.2648, 0.2494],\n",
      "        [0.4933, 0.2662, 0.2405],\n",
      "        [0.4902, 0.2656, 0.2443],\n",
      "        [0.1815, 0.2088, 0.6098],\n",
      "        [0.2071, 0.2224, 0.5705],\n",
      "        [0.1808, 0.2082, 0.6110],\n",
      "        [0.1868, 0.2120, 0.6012],\n",
      "        [0.1929, 0.2152, 0.5920],\n",
      "        [0.1884, 0.2129, 0.5987],\n",
      "        [0.1843, 0.2107, 0.6050],\n",
      "        [0.1832, 0.2100, 0.6068],\n",
      "        [0.1946, 0.2160, 0.5893],\n",
      "        [0.1890, 0.2132, 0.5978],\n",
      "        [0.1860, 0.2116, 0.6024],\n",
      "        [0.1892, 0.2133, 0.5975],\n",
      "        [0.1913, 0.2154, 0.5933],\n",
      "        [0.1904, 0.2139, 0.5957],\n",
      "        [0.1832, 0.2099, 0.6069],\n",
      "        [0.1856, 0.2113, 0.6030],\n",
      "        [0.1928, 0.2152, 0.5921],\n",
      "        [0.4915, 0.2662, 0.2423],\n",
      "        [0.4894, 0.2655, 0.2451],\n",
      "        [0.3396, 0.2660, 0.3944],\n",
      "        [0.3920, 0.2695, 0.3385],\n",
      "        [0.3254, 0.2597, 0.4148],\n",
      "        [0.3800, 0.2667, 0.3533],\n",
      "        [0.3617, 0.2691, 0.3692],\n",
      "        [0.2909, 0.2530, 0.4561],\n",
      "        [0.2806, 0.2504, 0.4690],\n",
      "        [0.1933, 0.2158, 0.5909],\n",
      "        [0.4919, 0.2661, 0.2420],\n",
      "        [0.4999, 0.2678, 0.2323],\n",
      "        [0.5012, 0.2682, 0.2306],\n",
      "        [0.5012, 0.2682, 0.2307],\n",
      "        [0.2374, 0.2351, 0.5275],\n",
      "        [0.3986, 0.2677, 0.3336],\n",
      "        [0.3209, 0.2589, 0.4202],\n",
      "        [0.4690, 0.2707, 0.2603],\n",
      "        [0.2301, 0.2322, 0.5376],\n",
      "        [0.3600, 0.2650, 0.3750],\n",
      "        [0.1768, 0.2060, 0.6172],\n",
      "        [0.1754, 0.2053, 0.6193],\n",
      "        [0.1756, 0.2054, 0.6190],\n",
      "        [0.1785, 0.2072, 0.6143],\n",
      "        [0.1763, 0.2056, 0.6180],\n",
      "        [0.1754, 0.2051, 0.6195],\n",
      "        [0.1754, 0.2052, 0.6194],\n",
      "        [0.1755, 0.2052, 0.6194],\n",
      "        [0.1850, 0.2110, 0.6040],\n",
      "        [0.1758, 0.2053, 0.6190],\n",
      "        [0.1755, 0.2051, 0.6193],\n",
      "        [0.1752, 0.2050, 0.6198],\n",
      "        [0.1805, 0.2083, 0.6112],\n",
      "        [0.1811, 0.2086, 0.6103],\n",
      "        [0.5012, 0.2682, 0.2306],\n",
      "        [0.4792, 0.2694, 0.2513]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.cat((predictions[0], predictions[1],predictions[2],predictions[3]), dim=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.argmax(result,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "        0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
