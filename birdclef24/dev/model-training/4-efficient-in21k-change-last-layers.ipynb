{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I still try to use efficientNet-in21k pretrained model as feature extractor.\n",
    "\n",
    "But I will modify some codes:\n",
    "\n",
    "1. Try to move some data processing steps to dataloader instead of lightningModelModule\n",
    "\n",
    "2. Use some other layers to replace the attention layer used before\n",
    "\n",
    "For specific experimental steps, please refer to 14-re-organize-dataprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='../../data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# Find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# Initialize training set and validation set\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# Random select a sample from each category to add to the validation set, and the rest to the training set\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# Divide the dataset by index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select 20,000 data from the training set\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Add these samples to the validation set\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# Remove these samples from the training set\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to interpolate missing values ​​for ratings in metadata csv file\n",
    "\n",
    "def rating_value_interplote(df:pd.DataFrame):\n",
    "    '''\n",
    "    interplote Nan values for rating col in metadata csv \n",
    "\n",
    "    parameters:\n",
    "        df: the df of the metadata csv file\n",
    "\n",
    "    rating col means the quality of the corresponding audio file\n",
    "        5 is high quality\n",
    "        1 is low quality\n",
    "        0 is without defined quality level\n",
    "    '''\n",
    "\n",
    "    if df['rating'].isna().sum()>0: \n",
    "        df['rating'].fillna(0, inplace=True)\n",
    "\n",
    "    # Random assign a value to all places where the value is 0, and select from the specified choices\n",
    "    mask = df['rating'] == 0  # Create a boolean mask indicating which positions are 0\n",
    "\n",
    "    choices=np.arange(0.5,5.1,0.5).tolist() # [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "    random_values = np.random.choice(choices, size=mask.sum())  # Generate random numbers for these 0 values  \n",
    "    df.loc[mask, 'rating'] = random_values  # Fill the generated random numbers back into the corresponding positions of the original DataFrame\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weight of each audio file through rating, which is helpful for model training\n",
    "def audio_weight(df):\n",
    "    '''\n",
    "    calculate the weight corresponding to each audio file through the rating value\n",
    "\n",
    "    Because each audio has different quality level, we use weight to affect the inportance of each audio in models,\n",
    "    the lower the quality of the audio, the lower the weight\n",
    "    '''\n",
    "    # Through rating, we calculate the credibility of each audio and express it through weight. \n",
    "    # The purpose of this is to improve the model by increasing the weight of high-quality audio and reducing the weight of low-quality audio.\n",
    "    df[\"audio_weight\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is an unbalanced dataset, the amount of data in each category is very different\n",
    "# So I will calculate the weight of each category here\n",
    "# **(-0.5) The purpose is to reduce the relative influence of high-frequency categories and increase the influence of low-frequency categories, \n",
    "# so as to help the model better learn those uncommon categories\n",
    "# The purpose of calculating this is to build a WeightedRandomSampler, \n",
    "# so that each time a batch is extracted using dataloader, it is more friendly to data of different categories.\n",
    "\n",
    "def sampling_weight(df)->torch.Tensor:\n",
    "    '''\n",
    "    calculate the sampling weight of each audio file\n",
    "\n",
    "    because this is imbalanced dataset\n",
    "    we hope the category with less data has large probability to be picked.\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # Map weights to each row of the original data\n",
    "    sample_weights_map = df['primary_label'].map(sample_weights)\n",
    "\n",
    "    # Convert pandas Series to NumPy array\n",
    "    sample_weights_np = sample_weights_map.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Convert a NumPy array to a PyTorch tensor using torch.from_numpy\n",
    "    sample_weights_tensor = torch.from_numpy(sample_weights_np)\n",
    "\n",
    "    return sample_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_sampler_generate(df):\n",
    "    '''\n",
    "    prepare dataloader for sampler\n",
    "    '''\n",
    "    sample_weights_tensor=sampling_weight(df=df)\n",
    "    # Here we will build an argument sampler that will be used by the dataloader\n",
    "    # It should be noted that the order of weights in the constructed sampler needs to be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "    # Create a sampler based on the newly obtained weight list\n",
    "    sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "    return sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler=dataloader_sampler_generate(df=train_df)\n",
    "val_sampler=dataloader_sampler_generate(df=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CustomCompose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class CustomOneOf:\n",
    "    def __init__(self, transforms: list, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if np.random.rand() < self.p:\n",
    "            n_trns = len(self.transforms)\n",
    "            trns_idx = np.random.choice(n_trns)\n",
    "            trns = self.transforms[trns_idx]\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GaussianNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=40.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class VolumeControl(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode=\"uniform\"):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        assert mode in [\n",
    "            \"uniform\",\n",
    "            \"fade\",\n",
    "            \"fade\",\n",
    "            \"cosine\",\n",
    "            \"sine\",\n",
    "        ], \"`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'\"\n",
    "\n",
    "        self.db_limit = db_limit\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.db_limit, self.db_limit)\n",
    "        if self.mode == \"uniform\":\n",
    "            db_translated = 10 ** (db / 20)\n",
    "        elif self.mode == \"fade\":\n",
    "            lin = np.arange(len(y))[::-1] / (len(y) - 1)\n",
    "            db_translated = 10 ** (db * lin / 20)\n",
    "        elif self.mode == \"cosine\":\n",
    "            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * cosine / 20)\n",
    "        else:\n",
    "            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * sine / 20)\n",
    "        augmented = y * db_translated\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class NoiseInjection(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_level = (0.0, max_noise_level)\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_level = np.random.uniform(*self.noise_level)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_level).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class GaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_rate = max_rate\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "def _db2float(db: float, amplitude=True):\n",
    "    if amplitude:\n",
    "        return 10 ** (db / 20)\n",
    "    else:\n",
    "        return 10 ** (db / 10)\n",
    "\n",
    "\n",
    "def volume_down(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for decreasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to decrease\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with decreased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(-db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "def volume_up(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for increasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to increase\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with increased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "class RandomVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        if db >= 0:\n",
    "            return volume_up(y, db)\n",
    "        else:\n",
    "            return volume_down(y, db)\n",
    "\n",
    "\n",
    "class CosineVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "        dbs = _db2float(cosine * db)\n",
    "        return y * dbs\n",
    "\n",
    "\n",
    "class AddGaussianNoise(AudioTransform):\n",
    "    \"\"\"Add gaussian noise to the samples\"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self, always_apply=False, min_amplitude=0.001, max_amplitude=0.015, p=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_amplitude: Minimum noise amplification factor\n",
    "        :param max_amplitude: Maximum noise amplification factor\n",
    "        :param p:\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        assert min_amplitude > 0.0\n",
    "        assert max_amplitude > 0.0\n",
    "        assert max_amplitude >= min_amplitude\n",
    "        self.min_amplitude = min_amplitude\n",
    "        self.max_amplitude = max_amplitude\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        amplitude = np.random.uniform(self.min_amplitude, self.max_amplitude)\n",
    "        noise = np.random.randn(*samples.shape).astype(np.float32)\n",
    "        samples = samples + amplitude * noise\n",
    "        return samples\n",
    "\n",
    "\n",
    "class AddGaussianSNR(AudioTransform):\n",
    "    \"\"\"\n",
    "    Add gaussian noise to the input. A random Signal to Noise Ratio (SNR) will be picked\n",
    "    uniformly in the decibel scale. This aligns with human hearing, which is more\n",
    "    logarithmic than linear.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        always_apply=False,\n",
    "        min_snr_in_db: float = 5.0,\n",
    "        max_snr_in_db: float = 40.0,\n",
    "        p: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_snr_in_db: Minimum signal-to-noise ratio in dB. A lower number means more noise.\n",
    "        :param max_snr_in_db: Maximum signal-to-noise ratio in dB. A greater number means less noise.\n",
    "        :param p: The probability of applying this transform\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        self.min_snr_in_db = min_snr_in_db\n",
    "        self.max_snr_in_db = max_snr_in_db\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        snr = np.random.uniform(self.min_snr_in_db, self.max_snr_in_db)\n",
    "\n",
    "        clean_rms = np.sqrt(np.mean(np.square(samples)))\n",
    "\n",
    "        a = float(snr) / 20\n",
    "        noise_rms = clean_rms / (10**a)\n",
    "\n",
    "        noise = np.random.normal(0.0, noise_rms, size=samples.shape).astype(np.float32)\n",
    "        return samples + noise\n",
    "\n",
    "\n",
    "class Normalize(AudioTransform):\n",
    "    \"\"\"\n",
    "    Apply a constant amount of gain, so that highest signal level present in the sound becomes\n",
    "    0 dBFS, i.e. the loudest level allowed if all samples must be between -1 and 1. Also known\n",
    "    as peak normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(self, always_apply=False, apply_to: str = \"all\", p: float = 0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        assert apply_to in (\"all\", \"only_too_loud_sounds\")\n",
    "        self.apply_to = apply_to\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        max_amplitude = np.amax(np.abs(samples))\n",
    "        if self.apply_to == \"only_too_loud_sounds\" and max_amplitude < 1.0:\n",
    "            return samples\n",
    "\n",
    "        if max_amplitude > 0:\n",
    "            return samples / max_amplitude\n",
    "        else:\n",
    "            return samples\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order is very important and needs to be matched one by one in the subsequent training, I will save these types here\n",
    "# Save as .npy file\n",
    "np.save(\"./external_files/13-2-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .npy file\n",
    "loaded_array = np.load(\"./external_files/13-2-bird-cates.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weight_generate(df:pd.DataFrame)->torch.Tensor:\n",
    "    '''\n",
    "    would use focal loss in the following, we need to provide the weight of each category to handle unbalanced data sets\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # Convert sample_weights to a DataFrame for easier processing\n",
    "    sample_weights_df = sample_weights.reset_index()\n",
    "    sample_weights_df.columns = ['label', 'weight']\n",
    "\n",
    "    # Convert loaded_array to Categorical type and sort sample_weights_df according to this new order\n",
    "    sample_weights_df['label'] = pd.Categorical(sample_weights_df['label'], categories=loaded_array, ordered=True)\n",
    "\n",
    "\n",
    "    ## Sort the DataFrame according to the new category order\n",
    "    sample_weights_df = sample_weights_df.sort_values('label').reset_index(drop=True)\n",
    "\n",
    "    class_weight=torch.tensor(sample_weights_df['weight'].values,dtype=torch.float16)\n",
    "    \n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_class_weights=class_weight_generate(df=train_df)\n",
    "loss_val_class_weights=class_weight_generate(df=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        bird_category_dir: str,\n",
    "        audio_dir: str = \"../../data/train_audio\",\n",
    "        train: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            df: the dataframe of metadata (train/val)\n",
    "            bird_category_dir: the directory of the bird category array file (npy)\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            train: If the Datset for train set or val set\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # if the Dataset for training or validation\n",
    "        self.train = train\n",
    "        self.raw_df = df\n",
    "\n",
    "        # inperplote nan or 0 value of rating col\n",
    "        self.raw_df = rating_value_interplote(df=self.raw_df)\n",
    "        # Calculate the weight of each audio file by rating\n",
    "        self.raw_df = audio_weight(self.raw_df)\n",
    "\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "        self.bird_cate_array = np.load(bird_category_dir, allow_pickle=True)\n",
    "\n",
    "        self.np_audio_transforms = (\n",
    "            self.setup_transforms()\n",
    "        )  # initialize data augmentation func\n",
    "\n",
    "    def setup_transforms(self):\n",
    "\n",
    "        return CustomCompose(\n",
    "            [\n",
    "                CustomOneOf(\n",
    "                    [\n",
    "                        NoiseInjection(p=1, max_noise_level=0.04),\n",
    "                        GaussianNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        PinkNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        AddGaussianNoise(\n",
    "                            min_amplitude=0.0001, max_amplitude=0.03, p=0.5\n",
    "                        ),\n",
    "                        AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=15, p=0.5),\n",
    "                    ],\n",
    "                    p=0.3,  \n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_audio_path(self, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the audio path of the corresponding index through the provided train metadata csv file. \n",
    "        Since there is only one index, only one path will be returned.\n",
    "\n",
    "        Parameters:\n",
    "            file_name: in format category_type/XC-ID.ogg (asbfly/XC134896.ogg)\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        \"\"\"\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_dir, file_name)\n",
    "\n",
    "    def target_clip(\n",
    "        self, index: int, audio: torch.Tensor, sample_rate: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip\n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "        \"\"\"\n",
    "        # Get the audio start time corresponding to index\n",
    "        clip_start_time = self.raw_df[\"clip_start_time\"].iloc[index]\n",
    "        duration_seconds = self.raw_df[\"duration\"].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time <= duration_seconds:\n",
    "            clip_start_point = clip_start_time * sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence\n",
    "            # The first step is to confirm whether the length is sufficient\n",
    "            # The length is sufficient, no mask is needed\n",
    "            if clip_start_point + segment_duration <= total_samples:\n",
    "                clip = audio[:, clip_start_point : clip_start_point + segment_duration]\n",
    "\n",
    "            # need masks if the length is not enough\n",
    "            else:\n",
    "                padding_length = clip_start_point + segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "\n",
    "                clip = torch.cat((audio[:, clip_start_point:], silence), dim=1)\n",
    "\n",
    "                del silence, padding_length\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The clip start time is out of raw audio length\")\n",
    "\n",
    "        del clip_start_time, segment_duration, total_samples\n",
    "\n",
    "        return clip\n",
    "\n",
    "    def random_audio_augmentation(self, audio: torch.Tensor):\n",
    "        \"\"\"\n",
    "        audio (torch.Tensor): A 2D tensor of audio samples with shape (1, N), where N is the number of samples.\n",
    "        \"\"\"\n",
    "\n",
    "        audio_aug = self.np_audio_transforms(audio[0].numpy())\n",
    "\n",
    "        # tranfer the array to 2D tensor and keep the num channel is 1\n",
    "        # this step is to keep the input and output shape adn type are the same\n",
    "\n",
    "        audio_aug_tensor = torch.from_numpy(audio_aug)\n",
    "        audio_aug_tensor = audio_aug_tensor.unsqueeze(0).to(dtype=torch.float16)\n",
    "\n",
    "        del audio_aug\n",
    "\n",
    "        return audio_aug_tensor\n",
    "\n",
    "    def audio_label_tensor_generator(self, true_label: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a tensor containing all categories based on the given real audio label\n",
    "\n",
    "        Parameters:\n",
    "            true lable: a label string\n",
    "\n",
    "        Return:\n",
    "            If have 10 class, and give a true lable\n",
    "            the return should be tensor([0,1,0,0,0,0,0,0,0,0])\n",
    "        \"\"\"\n",
    "        # Find the index of the target value in the array\n",
    "        idx = np.where(self.bird_cate_array == true_label)[0][0]\n",
    "\n",
    "        # Create a tensor of all zeros with length equal to the length of the array\n",
    "        audio_label_tensor = torch.zeros(len(self.bird_cate_array), dtype=torch.float16)\n",
    "\n",
    "        # Set the value at the corresponding index position to 1\n",
    "        audio_label_tensor[idx] = 1\n",
    "\n",
    "        return audio_label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.raw_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.raw_df.iloc[index]\n",
    "\n",
    "        audio_label = row[\"primary_label\"]\n",
    "        audio_weight = row[\"audio_weight\"]\n",
    "\n",
    "        # Get the path to a single audio file\n",
    "        single_audio_dir = self.get_audio_path(row[\"filename\"])\n",
    "\n",
    "        # Read audio array according to path\n",
    "        audio, sr = read_audio(single_audio_dir)\n",
    "\n",
    "        # augmentation\n",
    "        if self.train:\n",
    "            audio_augmentation = self.random_audio_augmentation(audio=audio)\n",
    "            # Get the audio clip corresponding to index\n",
    "            clip = self.target_clip(index, audio=audio_augmentation, sample_rate=sr)\n",
    "            del audio_augmentation\n",
    "        else:\n",
    "            clip = self.target_clip(index, audio=audio, sample_rate=sr)\n",
    "\n",
    "        # change audio label to one-hot tensor\n",
    "        audio_label_tensor = self.audio_label_tensor_generator(true_label=audio_label)\n",
    "\n",
    "        audio_label_tensor = torch.tensor(audio_label_tensor, dtype=torch.float16)\n",
    "        clip = torch.tensor(clip, dtype=torch.float16)\n",
    "        audio_weight = torch.tensor(audio_weight, dtype=torch.float16)\n",
    "\n",
    "        del audio\n",
    "\n",
    "        return audio_label_tensor, clip, audio_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup_prob, mixup_double):\n",
    "        super(Mixup, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.mixup_double = mixup_double\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0] # Generate a random number p and compare it with mixup_prob to decide whether to mix.\n",
    "        if p < self.mixup_prob:\n",
    "            bs = X.shape[0] # batch size\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs) # Generate a random permutation for randomly selecting samples from the current batch for mixing.\n",
    "\n",
    "            p1 = torch.rand((1,))[0] # If the random number p1 (determines whether to perform double mixing) is less than mixup_double, perform a single mix. Otherwise, perform double mixing:\n",
    "            if p1 < self.mixup_double:\n",
    "                X = X + X[perm]\n",
    "                Y = Y + Y[perm]\n",
    "                Y = torch.clamp(Y, 0, 1) # Use torch.clamp to clamp the values ​​of Y between 0 and 1 (suitable for probabilistic or binary labels).\n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = 0.5 * weight + 0.5 * weight[perm]\n",
    "                    return X, Y, weight\n",
    "            else:\n",
    "                perm2 = torch.randperm(bs)\n",
    "                X = X + X[perm] + X[perm2]\n",
    "                Y = Y + Y[perm] + Y[perm2]\n",
    "                Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = (\n",
    "                        1 / 3 * weight + 1 / 3 * weight[perm] + 1 / 3 * weight[perm2]\n",
    "                    )\n",
    "                    return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.01,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    n_fft = int(window_size * sample_rate)  \n",
    "\n",
    "    hop_length = int(hop_size * sample_rate) \n",
    "\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\") -> torch.Tensor:\n",
    "    \"\"\"Compute delta coefficients of a tensor, usually a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "        win_length (int, optional): The window length used for computing delta (Default: 5)\n",
    "        mode (str, optional): Mode parameter passed to padding (Default: \"replicate\")\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "    \"\"\"\n",
    "    device = specgram.device  \n",
    "    dtype = specgram.dtype\n",
    "\n",
    "    # pack batch\n",
    "    shape = specgram.size()\n",
    "    specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "    assert win_length >= 3\n",
    "    n = (win_length - 1) // 2\n",
    "    denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "    # Create the kernel tensor, making sure it is on the same device as the input tensor\n",
    "    kernel = torch.arange(-n, n + 1, 1, dtype=dtype,device=device).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "    output = (\n",
    "        torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "    )\n",
    "\n",
    "    # unpack batch\n",
    "    output = output.reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def make_delta(input_tensor: torch.Tensor):\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    input_tensor = compute_deltas(input_tensor)\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "def image_delta(x):\n",
    "    delta_1 = make_delta(x)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([x, delta_1, delta_2], dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup2(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup2_prob):\n",
    "        super(Mixup2, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup2_prob = mixup2_prob\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0]\n",
    "        if p < self.mixup2_prob:\n",
    "            bs = X.shape[0]\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs)\n",
    "            coeffs = self.beta_distribution.rsample(torch.Size((bs,)))\n",
    "\n",
    "            if n_dims == 2:\n",
    "                X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n",
    "            elif n_dims == 3:\n",
    "                X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n",
    "            else:\n",
    "                X = (\n",
    "                    coeffs.view(-1, 1, 1, 1) * X\n",
    "                    + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n",
    "                )\n",
    "            Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n",
    "            # Y = Y + Y[perm]\n",
    "            # Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n",
    "                return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_layer = Mixup(mix_beta=5, mixup_prob=0.7, mixup_double=0.5)\n",
    "mixup2_layer = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "audio_transforms = Compose(\n",
    "    [\n",
    "        # AddColoredNoise(p=0.5),\n",
    "        PitchShift(\n",
    "            min_transpose_semitones=-4,\n",
    "            max_transpose_semitones=4,\n",
    "            sample_rate=32000,\n",
    "            p=0.4,\n",
    "        ),\n",
    "        Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model = timm.create_model('tf_efficientnetv2_s_in21k', pretrained=True,in_chans=3) # You can change the data channel accepted by the pre-trained model by passing in argument in_chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume model is the loaded complete EfficientNet model\n",
    "# Use the output of the first set of InvertedResidual\n",
    "feature_extractor = torch.nn.Sequential(\n",
    "    *list(model.children())[:-3]  # Remove the last three layers, which needs to be adjusted according to the actual model structure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to separate feature extractor from lightningmodule and add it to dataloader as part of data processing\n",
    "\n",
    "def trainloader_collate(batch):\n",
    "    \"\"\"\n",
    "    When creating data batches, define how each batch should be stacked\n",
    "    parameters:\n",
    "        batch: is a list of tuples with (labels, clip, weights)\n",
    "        feature_extractor: use a pretrained model as a feature extractor\n",
    "    \"\"\"\n",
    "    # Unpack each individual sample in the batch\n",
    "    labels, clips, weights = zip(*batch)\n",
    "\n",
    "    # Stack the data into new batches\n",
    "    labels = torch.stack(labels).float()\n",
    "    clips = torch.stack(clips).float()\n",
    "\n",
    "    weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "    clips, labels, weights = mixup_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # Use Compose to combine multiple audio transformation operations. These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "    clips = audio_transforms(clips, sample_rate=32000)\n",
    "\n",
    "    # convert audio to mel spectrogram\n",
    "    clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "    clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "    # generalization\n",
    "    clips = (clips + 80) / 80\n",
    "\n",
    "    # Random masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "    clips = torchaudio.transforms.TimeMasking(\n",
    "        time_mask_param=20, iid_masks=True, p=0.3\n",
    "    )(clips)\n",
    "\n",
    "    # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "    clips = image_delta(clips)\n",
    "\n",
    "    # mix audio up\n",
    "    clips, labels,weights = mixup2_layer(X=clips, Y=labels, weight=weights)\n",
    "\n",
    "    # feature extractor\n",
    "    # Use torch.no_grad() to ensure feature extraction does not preserve gradients\n",
    "    with torch.no_grad():\n",
    "        clips=feature_extractor(clips)\n",
    "\n",
    "    return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to separate feature extractor from lightningmodule and add it to dataloader as part of data processing.\n",
    "\n",
    "\n",
    "def valloader_collate(batch):\n",
    "    \"\"\"\n",
    "    When creating data batches, define how each batch should be stacked\n",
    "    parameters:\n",
    "        batch: is a list of tuples with (labels, clip, weights)\n",
    "        feature_extractor: use a pretrained model as a feature extractor\n",
    "    \"\"\"\n",
    "    # Unpack each individual sample in the batch\n",
    "    labels, clips, weights = zip(*batch)\n",
    "\n",
    "    # Stack the data into new batches\n",
    "    labels = torch.stack(labels).float()\n",
    "    clips = torch.stack(clips).float()\n",
    "\n",
    "    weights = torch.stack(weights) if weights[0] is not None else None\n",
    "\n",
    "    # Convert audio data into mel spectrogram\n",
    "    clips = mel_transform(sample_rate=32000, audio=clips)\n",
    "\n",
    "    ##Convert the amplitude of Mel Spectrogram to decibel (Decibel, dB)\n",
    "    clips = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)(clips)\n",
    "\n",
    "    # generalization\n",
    "    clips = (clips + 80) / 80\n",
    "\n",
    "    # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "    clips = image_delta(clips)\n",
    "\n",
    "    # feature extractor\n",
    "    # Use torch.no_grad() to ensure feature extraction does not preserve gradients\n",
    "    with torch.no_grad():\n",
    "        clips = feature_extractor(clips)\n",
    "\n",
    "    return clips, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_sampler,\n",
    "        val_sampler,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        bird_category_dir: str,\n",
    "        audio_dir: str = \"data/audio\",\n",
    "        batch_size: int = 128,\n",
    "        workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.bird_category_dir = bird_category_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_sampler = train_sampler\n",
    "        self.val_sampler = val_sampler\n",
    "        self.workers = workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            df=self.train_df,\n",
    "            bird_category_dir=self.bird_category_dir,\n",
    "            audio_dir=self.audio_dir,\n",
    "            train=True,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.train_sampler,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=trainloader_collate\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            df=self.val_df,\n",
    "            bird_category_dir=self.bird_category_dir,\n",
    "            audio_dir=self.audio_dir,\n",
    "            train=False,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.val_sampler,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=valloader_collate\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self,class_nums:int=182):\n",
    "        super().__init__()\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=1280, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=128, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru3 = nn.GRU(\n",
    "            input_size=256, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru4 = nn.GRU(\n",
    "            input_size=384, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=class_nums)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        gru_out1, _ = self.gru1(x)\n",
    "        x1 = self.bn1(gru_out1)\n",
    "        gru_out2, _ = self.gru2(x1)\n",
    "        x2 = self.bn2(gru_out2)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x3 = torch.cat((x1, x2), dim=2)\n",
    "        gru_out3, _ = self.gru3(x3)\n",
    "        x4 = self.bn3(gru_out3)\n",
    "        x5 = torch.cat((x1, x2, x4), dim=2)\n",
    "        gru_out4, _ = self.gru4(x5)\n",
    "        x6 = self.dropout1(gru_out4[:, -1, :]) \n",
    "        out = self.fc1(x6)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, sample_weight=None,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  \n",
    "        self.sample_weight=sample_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
    "        p_t = torch.exp(-ce_loss) # Modulating Factor\n",
    "        loss = (1 - p_t) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.sample_weight is not None:\n",
    "            loss *= self.sample_weight\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_auc(preds, targets):\n",
    "    preds = torch.sigmoid(preds)  # Assuming binary or multi-label classification\n",
    "    preds = preds.detach().cpu().numpy()  # Detach and convert to numpy\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute ROC-AUC on a per-class basis and average\n",
    "    auc_scores = []\n",
    "    for i in range(targets.shape[1]):  # Loop through classes\n",
    "        if targets[:, i].sum() > 0:  # Only score classes with positive labels\n",
    "            auc = roc_auc_score(targets[:, i], preds[:, i])\n",
    "            auc_scores.append(auc)\n",
    "    \n",
    "    if len(auc_scores) > 0:\n",
    "        return sum(auc_scores) / len(auc_scores)  # Return macro average\n",
    "    else:\n",
    "        return 0.0  # Handle cases where no class has positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_class_weight: torch.Tensor,\n",
    "        val_class_weight: torch.Tensor,\n",
    "        sample_rate: int = 32000,\n",
    "        class_num: int = 182,\n",
    "        lr: float = 0.001\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model: the defined model module\n",
    "            train_class_weight: the argument is used for Focal Loss Function, focal loss needs a sequence of class weights to calculate the loss\n",
    "            val_class_weight: the argument is also used for Focal loss function, for validation step\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "        self.train_class_weight = train_class_weight.to(device)\n",
    "        self.val_class_weight = val_class_weight.to(device)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.class_num = class_num\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, clips):\n",
    "\n",
    "        return self.model(clips)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # predictions\n",
    "        # target_pred=self(clip.to(device))\n",
    "        target_pred = self(clips)\n",
    "        # print(\"train\", weights.shape)\n",
    "        # initialize loss fn\n",
    "        loss_fn = FocalLoss(weight=self.train_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_roc_auc\",\n",
    "            roc_auc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        # clean memory\n",
    "        del labels, clips, weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # predictions\n",
    "        target_pred = self(clips).detach()\n",
    "\n",
    "        # initialize loss fn\n",
    "        print(\"val\", weights.shape)\n",
    "        loss_fn = FocalLoss(weight=self.val_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"val_roc_auc\",\n",
    "            roc_auc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        # clean memory\n",
    "        del labels, clips, weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.lr,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | ChronoNet | 1.0 M \n",
      "------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'BirdclefDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    # logger = WandbLogger(project='BirdClef-2024', name='sef_s21_v1')\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\", \n",
    "        dirpath=\"models/checkpoints\",\n",
    "        filename=\"sed_s21k_v1-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,  \n",
    "        mode=\"min\",  \n",
    "        auto_insert_metric_name=False,  \n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", \n",
    "        min_delta=0.00,\n",
    "        patience=3, \n",
    "        verbose=True,\n",
    "        mode=\"min\",  \n",
    "    )\n",
    "\n",
    "    # Previously we used a separate dataloader to feed the model\n",
    "    # Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "    bdm = BirdclefDatasetModule(\n",
    "        train_sampler=train_sampler,\n",
    "        val_sampler=val_sampler,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        bird_category_dir=\"./external_files/13-2-bird-cates.npy\",\n",
    "        audio_dir=\"../../data/train_audio\",\n",
    "        batch_size=64,\n",
    "        workers=0,\n",
    "    )\n",
    "\n",
    "    class_num = len(np.load(\"external_files/13-2-bird-cates.npy\", allow_pickle=True))\n",
    "    # initilize model\n",
    "    chrononet = ChronoNet(class_nums=class_num)\n",
    "\n",
    "    BirdModelModule = BirdModelModule(\n",
    "        model=chrononet,\n",
    "        train_class_weight=loss_train_class_weights,\n",
    "        val_class_weight=loss_val_class_weights,\n",
    "        class_num=6,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        # enable mixed precision\n",
    "        precision=16,\n",
    "        # Set up Trainer, use gradient accumulation, and update parameters after accumulating gradients every 512 batches\n",
    "        accumulate_grad_batches=512,\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir=\"models/model_training\",\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        # logger=logger,  # use MLflow logger\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],  \n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
