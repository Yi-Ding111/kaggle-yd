{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import pydub\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "import joblib\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torchmetrics\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping,Callback\n",
    "\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240420\n",
      "Is CUDA available:  False\n",
      "Is Metal available:  True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Is Metal available: \", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory='../../data/train'\n",
    "\n",
    "sub_folders=glob.glob(os.path.join(parent_directory,'*'))\n",
    "\n",
    "files=[]\n",
    "\n",
    "for folder_path in sub_folders:\n",
    "    first_file=glob.glob(os.path.join(folder_path,'*.ogg'))\n",
    "    files.extend(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected files: ['../../data/train/ashpri1/XC116338.ogg', '../../data/train/asikoe2/XC115086.ogg', '../../data/train/ashwoo2/XC169810.ogg', '../../data/train/ashdro1/XC114598.ogg', '../../data/train/asiope1/XC194954.ogg', '../../data/train/asbfly/XC164848.ogg']\n",
      "Remaining files: ['../../data/train/ashpri1/XC116339.ogg', '../../data/train/asikoe2/XC138196.ogg', '../../data/train/ashwoo2/XC125152.ogg', '../../data/train/ashdro1/XC114599.ogg', '../../data/train/asiope1/XC397761.ogg', '../../data/train/asbfly/XC134896.ogg']\n"
     ]
    }
   ],
   "source": [
    "# split train and val paths\n",
    "# i decide to randomly choose one single file from each category folder as val path, and the left part as train paths\n",
    "\n",
    "parent_directory='../../data/train'\n",
    "\n",
    "sub_folders=glob.glob(os.path.join(parent_directory,'*'))\n",
    "\n",
    "\n",
    "# Used to store randomly selected file paths\n",
    "random_files = []\n",
    "# Path to store the remaining files\n",
    "files=[] \n",
    "\n",
    "# iterate each subfolder\n",
    "for folder in sub_folders:\n",
    "    # Get all file paths in subfolders\n",
    "    all_files = glob.glob(os.path.join(folder, '*'))\n",
    "    if all_files:  # Make sure the folder is not empty\n",
    "        # Random select a file from the file list\n",
    "        chosen_file = random.choice(all_files)\n",
    "        # Add to random file list\n",
    "        random_files.append(chosen_file)\n",
    "        # Add the remaining files to another list\n",
    "        files.extend([file for file in all_files if file != chosen_file])\n",
    "\n",
    "\n",
    "print(\"Randomly selected files:\", random_files)\n",
    "print(\"Remaining files:\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path:str)->pydub.audio_segment.AudioSegment:\n",
    "    \"\"\"\n",
    "    read ogg file as pydub.audio_segment.AudioSegment for the following steps\n",
    "\n",
    "    parametere:\n",
    "        path: *.ogg file path\n",
    "\n",
    "    return\n",
    "        audio: the readed audio data\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(path, format=\"ogg\")\n",
    "\n",
    "    return audio    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regarding the data of a single audio, some audio information needs to be paid attention to, such as audio duration, sampling rate, bit rate and number of channels.\n",
    "\n",
    "def audio_info(audio:pydub.audio_segment.AudioSegment):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio\n",
    "\n",
    "    Parameters:\n",
    "        Audio: the readed audio data\n",
    "\n",
    "    Return:\n",
    "        the information of the audio\n",
    "    \"\"\"\n",
    "    # the audio duration time (seconds)\n",
    "    duration_seconds=len(audio)/1000.0\n",
    "\n",
    "    # the audio sampling rate\n",
    "    sr=audio.frame_rate\n",
    "\n",
    "    # the num of channels\n",
    "    num_channels=audio.channels\n",
    "\n",
    "    #bit rate\n",
    "    bit_rate=audio.sample_width * 8\n",
    "\n",
    "    return duration_seconds, sr, num_channels, bit_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert audio data into array\n",
    "\n",
    "def audio2array(audio_slices:list)->np.array:\n",
    "    \"\"\"\n",
    "    transform audio segments to arrays\n",
    "    \"\"\"\n",
    "    audio_arrays=np.array([np.array(audio_slice.get_array_of_samples()) for audio_slice in audio_slices])\n",
    "\n",
    "    return audio_arrays\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_audio_5_align(audio:pydub.audio_segment.AudioSegment)->List[pydub.audio_segment.AudioSegment]:\n",
    "    \"\"\"\n",
    "    Slice the complete audio into multiple 5 seconds length,\n",
    "    keep all slice have the same length, especially for the last slice\n",
    "\n",
    "    Return the list of all audio segments\n",
    "    \"\"\"\n",
    "    # set up the segment duration\n",
    "    # Set up the segment duration\n",
    "    segment_duration = 5 * 1000  # 5 seconds in milliseconds\n",
    "\n",
    "    # Check if the audio is less than 5 seconds\n",
    "    if len(audio) < segment_duration:\n",
    "        # Calculate the required padding length\n",
    "        padding_length = segment_duration - len(audio)\n",
    "        # Create a silent audio segment for padding\n",
    "        silence = AudioSegment.silent(duration=padding_length)\n",
    "        # Pad the audio with silence\n",
    "        padded_audio = audio + silence\n",
    "        return [padded_audio]  # Return the padded audio as a single segment\n",
    "\n",
    "    # If the audio is 5 seconds or longer, proceed as normal\n",
    "    segments = [audio[i:i + segment_duration] for i in range(0, len(audio), segment_duration)]\n",
    "\n",
    "    # Ensure the last segment is exactly 5 seconds long\n",
    "    if len(segments[-1]) != segment_duration:\n",
    "        last_segment = audio[-segment_duration:]  # Get the last 5 seconds of the audio\n",
    "        segments[-1] = last_segment  # Replace the last segment with a full 5-second segment\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_bitrate_norm(bit_rate:float,audio_array:np.array):\n",
    "    \"\"\"\n",
    "    because the .ogg file readed through pydub would based off the audio original bit rate,\n",
    "    we want the value of the audio keep small, \n",
    "    so do normalization based off the bit rate.\n",
    "\n",
    "    Parameters:\n",
    "        bit_rate: the bit rate of the audio\n",
    "        audio_array: the data in array form for each single slice\n",
    "    \"\"\"\n",
    "    audio_array_norm = audio_array / float(2**(bit_rate-1))\n",
    "\n",
    "    return audio_array_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_random_sampling_2(total_samples:int,audio_segment:pydub.audio_segment.AudioSegment)-> List[pydub.audio_segment.AudioSegment]:\n",
    "    \"\"\"\n",
    "    Randomly extract audio clips and combine them into 2 seconds of audio\n",
    "\n",
    "    Parameters:\n",
    "        total_samples: the number of randomly synthesized audio clips\n",
    "        audio_segment: the single audio segment in form `pydub.audio_segment.AudioSegment`\n",
    "\n",
    "    Return:\n",
    "        The list of all random extract audio clips in form `pydub.audio_segment.AudioSegment`\n",
    "\n",
    "    \"\"\"\n",
    "    random_clips=[]\n",
    "    clip_num=0\n",
    "\n",
    "    # Our goal is to randomly extract a total of 2 seconds of audio\n",
    "    total_duration_ms = 2*1000\n",
    "\n",
    "    while clip_num<total_samples:\n",
    "        #Store the extracted fragments\n",
    "        extracted_segments = AudioSegment.silent(duration=0)  # Create a silent segment for subsequent splicing\n",
    "\n",
    "        # Continue looping when the total length of the extracted segments is less than 2 seconds\n",
    "        while extracted_segments.duration_seconds < 2:\n",
    "            # Random choose a starting point\n",
    "            start_ms = random.randint(0, len(audio_segment) - 1)\n",
    "            # Calculate the maximum duration that can be extracted\n",
    "            max_extract_ms = total_duration_ms - int(extracted_segments.duration_seconds * 1000)\n",
    "            # Random determine the duration of this draw\n",
    "            extract_duration_ms = random.randint(1, max_extract_ms)\n",
    "            # Random extracted fragments\n",
    "            extract = audio_segment[start_ms:start_ms+extract_duration_ms]\n",
    "            # concat the extracted fragments\n",
    "            extracted_segments += extract\n",
    "\n",
    "        # Final discontinuous random 2 seconds of audio data\n",
    "        random_two_seconds = extracted_segments\n",
    "\n",
    "        random_clips.append(random_two_seconds)\n",
    "\n",
    "        clip_num+=1\n",
    "\n",
    "    return random_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeD2preqD(normalized_clip:np.array,sr:int):\n",
    "    \"\"\"\n",
    "    transform audio from time domain to frequency domain\n",
    "\n",
    "    Parameters:\n",
    "        normalized_clip: the single clip in array format (data)\n",
    "        sr: sampling rate\n",
    "\n",
    "    Return:\n",
    "        Due to symmetry, only half the spectrum is needed.\n",
    "        And becuase the range of frequencies depends on the sampling rate of the audio signal.\n",
    "        we do not need the frequency info, all of them are the same.\n",
    "    \"\"\"\n",
    "    fft = np.fft.fft(normalized_clip)\n",
    "    magnitude = np.abs(fft)\n",
    "    frequency = np.linspace(0, sr, len(magnitude))\n",
    "\n",
    "    half_len = len(magnitude) // 2\n",
    "    frequency=frequency[:half_len]\n",
    "    magnitude=magnitude[:half_len]\n",
    "\n",
    "    return magnitude,frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/train/ashpri1/XC116339.ogg\n",
      "../../data/train/asikoe2/XC138196.ogg\n",
      "../../data/train/ashwoo2/XC125152.ogg\n",
      "../../data/train/ashdro1/XC114599.ogg\n",
      "../../data/train/asiope1/XC397761.ogg\n",
      "../../data/train/asbfly/XC134896.ogg\n"
     ]
    }
   ],
   "source": [
    "labels_list=[]\n",
    "all_audios_magnitude=[]\n",
    "for path in files:\n",
    "    print(path)\n",
    "    labels=[]\n",
    "    # extract label\n",
    "    label=path.split('/')[-2]\n",
    "\n",
    "    # read audio\n",
    "    audio=read_audio(path)\n",
    "\n",
    "    # grab audio information\n",
    "    duration_secconds,sr,num_channels,bit_rate=audio_info(audio)\n",
    "\n",
    "    # slice audio into multi 5 seconds\n",
    "    slice_5_all=slice_audio_5_align(audio)\n",
    "\n",
    "    for single_slice in slice_5_all:\n",
    "\n",
    "        # slice audio on each 5 sec long audio and generate multi 2 sec clips\n",
    "        random_clips=audio_random_sampling_2(total_samples=5,audio_segment=single_slice)\n",
    "\n",
    "        # convert all 2 sec clips to array format\n",
    "        audio_arrays_2sec=audio2array(random_clips)\n",
    "\n",
    "        # normalize each 2 sec audio array \n",
    "        arrays_2sec_norm=[]\n",
    "        for i in audio_arrays_2sec:\n",
    "            array_2sec_norm=audio_bitrate_norm(bit_rate=bit_rate,audio_array=i)\n",
    "            arrays_2sec_norm.append(array_2sec_norm)\n",
    "        \n",
    "        # arrays_2sec_norm=np.array(arrays_2sec_norm)\n",
    "\n",
    "        # conver audio from time domain to frequency domain\n",
    "        audios_magnitude=[]\n",
    "        for i in arrays_2sec_norm:\n",
    "            magnitude,frequency=timeD2preqD(normalized_clip=i,sr=sr)\n",
    "            audios_magnitude.append(magnitude)\n",
    "\n",
    "        # audios_magnitude=np.array(audios_magnitude)\n",
    "\n",
    "        labels.append(label)\n",
    "        all_audios_magnitude.append(audios_magnitude)\n",
    "\n",
    "\n",
    "    # all_audios_magnitude=np.array(all_audios_magnitude)\n",
    "\n",
    "    \n",
    "    labels_list.append(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/train/ashpri1/XC116338.ogg\n",
      "../../data/train/asikoe2/XC115086.ogg\n",
      "../../data/train/ashwoo2/XC169810.ogg\n",
      "../../data/train/ashdro1/XC114598.ogg\n",
      "../../data/train/asiope1/XC194954.ogg\n",
      "../../data/train/asbfly/XC164848.ogg\n"
     ]
    }
   ],
   "source": [
    "# same step for val-set prepare\n",
    "\n",
    "val_labels_list=[]\n",
    "val_all_audios_magnitude=[]\n",
    "for path in random_files:\n",
    "    print(path)\n",
    "    labels=[]\n",
    "    # extract label\n",
    "    label=path.split('/')[-2]\n",
    "\n",
    "    # read audio\n",
    "    audio=read_audio(path)\n",
    "\n",
    "    # grab audio information\n",
    "    duration_secconds,sr,num_channels,bit_rate=audio_info(audio)\n",
    "\n",
    "    # slice audio into multi 5 seconds\n",
    "    slice_5_all=slice_audio_5_align(audio)\n",
    "\n",
    "    for single_slice in slice_5_all:\n",
    "\n",
    "        # slice audio on each 5 sec long audio and generate multi 2 sec clips\n",
    "        random_clips=audio_random_sampling_2(total_samples=5,audio_segment=single_slice)\n",
    "\n",
    "        # convert all 2 sec clips to array format\n",
    "        audio_arrays_2sec=audio2array(random_clips)\n",
    "\n",
    "        # normalize each 2 sec audio array \n",
    "        arrays_2sec_norm=[]\n",
    "        for i in audio_arrays_2sec:\n",
    "            array_2sec_norm=audio_bitrate_norm(bit_rate=bit_rate,audio_array=i)\n",
    "            arrays_2sec_norm.append(array_2sec_norm)\n",
    "        \n",
    "        # arrays_2sec_norm=np.array(arrays_2sec_norm)\n",
    "\n",
    "        # conver audio from time domain to frequency domain\n",
    "        audios_magnitude=[]\n",
    "        for i in arrays_2sec_norm:\n",
    "            magnitude,frequency=timeD2preqD(normalized_clip=i,sr=sr)\n",
    "            audios_magnitude.append(magnitude)\n",
    "\n",
    "        # audios_magnitude=np.array(audios_magnitude)\n",
    "\n",
    "        labels.append(label)\n",
    "        val_all_audios_magnitude.append(audios_magnitude)\n",
    "\n",
    "\n",
    "    # all_audios_magnitude=np.array(all_audios_magnitude)\n",
    "\n",
    "    \n",
    "    val_labels_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ashpri1', 'ashpri1'],\n",
       " ['asikoe2', 'asikoe2', 'asikoe2', 'asikoe2', 'asikoe2'],\n",
       " ['ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2',\n",
       "  'ashwoo2'],\n",
       " ['ashdro1',\n",
       "  'ashdro1',\n",
       "  'ashdro1',\n",
       "  'ashdro1',\n",
       "  'ashdro1',\n",
       "  'ashdro1',\n",
       "  'ashdro1',\n",
       "  'ashdro1'],\n",
       " ['asiope1'],\n",
       " ['asbfly', 'asbfly', 'asbfly', 'asbfly', 'asbfly', 'asbfly']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ashpri1', 'ashpri1', 'asikoe2', 'asikoe2', 'asikoe2', 'asikoe2', 'asikoe2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'asiope1', 'asbfly', 'asbfly', 'asbfly', 'asbfly', 'asbfly', 'asbfly']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "labels_flatten_list = [element for sublist in labels_list for element in sublist]\n",
    "print(labels_flatten_list)\n",
    "print(len(labels_flatten_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audios_magnitude=np.array(all_audios_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 5, 32000)\n"
     ]
    }
   ],
   "source": [
    "print(all_audios_magnitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.81686464e-02 4.81870678e+00 4.33423088e+00 ... 5.71471008e-02\n",
      "   7.89477954e-02 8.27030752e-02]\n",
      "  [2.41087334e+00 1.01080087e+01 3.89218539e+00 ... 3.94646991e-02\n",
      "   3.66282951e-02 2.98537665e-02]\n",
      "  [4.65034673e+00 6.72856101e+00 1.37018789e+01 ... 1.30813245e-01\n",
      "   8.74947668e-02 1.27252597e-01]\n",
      "  [2.42534146e+00 5.45813414e+00 6.52445159e+00 ... 6.41737195e-02\n",
      "   7.28165364e-02 7.87558942e-02]\n",
      "  [5.29594434e+00 4.89940171e+00 4.45367478e+00 ... 8.42250725e-02\n",
      "   1.07353951e-01 8.76623899e-02]]\n",
      "\n",
      " [[1.24898088e+00 6.13831839e+00 4.70692573e+00 ... 1.39811849e-01\n",
      "   1.01204803e-01 6.18743821e-02]\n",
      "  [6.33152206e+00 2.73318003e+00 1.72171257e+00 ... 1.92442911e-01\n",
      "   5.04104599e-02 1.88744907e-01]\n",
      "  [5.15870596e+00 4.54021180e+00 2.64509008e+00 ... 4.54308252e-02\n",
      "   8.92498460e-02 1.37793069e-01]\n",
      "  [7.49060958e-01 2.76398303e+00 6.64030817e+00 ... 6.61474730e-02\n",
      "   7.56880047e-02 6.89109598e-02]\n",
      "  [8.13311935e+00 3.95740002e+00 7.31788068e+00 ... 1.81223302e-01\n",
      "   1.39337953e-01 2.33314238e-01]]\n",
      "\n",
      " [[1.57695617e-01 4.05367348e-01 3.99438528e-01 ... 4.40094213e-02\n",
      "   3.83088787e-02 1.23933984e-01]\n",
      "  [2.15471224e-01 1.72293618e-01 7.91486340e-02 ... 1.72440694e-01\n",
      "   1.88173252e-01 1.76177821e-01]\n",
      "  [3.72200009e-01 7.84587614e-01 7.87984308e-01 ... 1.02102836e-01\n",
      "   9.64980138e-02 1.19631031e-01]\n",
      "  [1.64399067e+00 3.52473117e-01 1.63752731e+00 ... 1.47475280e-01\n",
      "   4.37109992e-02 1.45709332e-01]\n",
      "  [1.06257827e+00 1.11717170e+00 1.11771107e+00 ... 6.62623585e-02\n",
      "   4.64601420e-02 2.09701229e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.63411403e-02 4.29272775e-02 3.49959797e-02 ... 4.06269697e-02\n",
      "   3.97576813e-02 3.71240421e-02]\n",
      "  [1.15571190e-02 7.39302944e-03 4.09477254e-03 ... 4.54982168e-03\n",
      "   1.46833115e-03 5.18742346e-03]\n",
      "  [7.35011990e-02 4.78401348e-02 5.10794362e-02 ... 1.82884629e-02\n",
      "   1.91323834e-02 2.79436126e-02]\n",
      "  [2.72988598e-02 2.73385792e-02 2.60646215e-02 ... 4.61815336e-02\n",
      "   4.70946557e-02 3.81176888e-02]\n",
      "  [4.19242587e-03 2.16803760e-03 1.42276814e-03 ... 3.33449853e-03\n",
      "   4.01120104e-03 5.82816163e-03]]\n",
      "\n",
      " [[8.77072848e-02 8.23002778e-02 6.91249923e-02 ... 7.80045665e-02\n",
      "   7.23115425e-02 5.51717773e-02]\n",
      "  [1.00347507e-02 1.25121204e-02 1.77063632e-02 ... 3.12366264e-02\n",
      "   3.16934812e-02 3.20005116e-02]\n",
      "  [1.31986244e-03 9.12991932e-04 6.11824639e-04 ... 5.60617061e-04\n",
      "   1.55033814e-03 1.68399200e-03]\n",
      "  [1.04134809e-03 2.69153517e-03 6.61454778e-03 ... 4.05230311e-03\n",
      "   3.16750626e-03 5.37672017e-03]\n",
      "  [6.02443842e-03 6.00055605e-03 6.09371389e-03 ... 5.99557089e-03\n",
      "   4.76277743e-03 7.24000334e-03]]\n",
      "\n",
      " [[6.20382731e-02 6.68200050e-02 6.61920581e-02 ... 7.56801239e-02\n",
      "   6.83957408e-02 6.90062390e-02]\n",
      "  [6.98671304e-03 6.13969962e-03 4.01287905e-03 ... 5.25150824e-03\n",
      "   5.04009446e-03 3.15447566e-03]\n",
      "  [2.64993869e-05 4.39705789e-04 8.51796738e-04 ... 3.34580788e-03\n",
      "   3.18480741e-03 2.94812411e-03]\n",
      "  [2.84709968e-04 3.57097332e-03 5.82635344e-03 ... 2.02320352e-02\n",
      "   2.23165638e-02 1.31256041e-02]\n",
      "  [5.48721943e-03 5.91737741e-03 5.22252208e-03 ... 2.39131548e-03\n",
      "   3.26534867e-03 5.41741164e-03]]]\n"
     ]
    }
   ],
   "source": [
    "print(all_audios_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ashpri1', 'ashpri1', 'ashpri1', 'ashpri1', 'ashpri1', 'ashpri1', 'asikoe2', 'asikoe2', 'asikoe2', 'asikoe2', 'asikoe2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'asiope1', 'asiope1', 'asiope1', 'asbfly', 'asbfly', 'asbfly', 'asbfly']\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "# same step for val set\n",
    "\n",
    "val_labels_flatten_list = [element for sublist in val_labels_list for element in sublist]\n",
    "print(val_labels_flatten_list)\n",
    "print(len(val_labels_flatten_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 5, 32000)\n"
     ]
    }
   ],
   "source": [
    "val_all_audios_magnitude=np.array(val_all_audios_magnitude)\n",
    "\n",
    "print(val_all_audios_magnitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize label encoder\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "# use Labelencoder to transform labels\n",
    "encoded_labels=encoder.fit_transform(labels_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded labels: [2 2 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 5 0 0 0 0 0 0]\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print('encoded labels:', encoded_labels)\n",
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to integer mapping: {'asbfly': 0, 'ashdro1': 1, 'ashpri1': 2, 'ashwoo2': 3, 'asikoe2': 4, 'asiope1': 5}\n"
     ]
    }
   ],
   "source": [
    "# If needed, you can view the mapping of original labels to encodings\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Label to integer mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Labels: ['ashpri1' 'ashpri1' 'ashpri1' 'ashpri1' 'ashpri1' 'ashpri1' 'ashpri1'\n",
      " 'ashpri1' 'asikoe2' 'asikoe2' 'asikoe2' 'asikoe2' 'asikoe2' 'asikoe2'\n",
      " 'asikoe2' 'asikoe2' 'asikoe2' 'asikoe2' 'ashwoo2' 'ashwoo2' 'ashwoo2'\n",
      " 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashwoo2'\n",
      " 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashwoo2' 'ashdro1' 'ashdro1'\n",
      " 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1'\n",
      " 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1'\n",
      " 'ashdro1' 'ashdro1' 'ashdro1' 'ashdro1' 'asiope1' 'asiope1' 'asiope1'\n",
      " 'asiope1' 'asbfly' 'asbfly' 'asbfly' 'asbfly' 'asbfly' 'asbfly' 'asbfly'\n",
      " 'asbfly' 'asbfly' 'asbfly']\n"
     ]
    }
   ],
   "source": [
    "# # decoding\n",
    "# decoded_labels = encoder.inverse_transform(encoded_labels)  # 解码前10个标签作为示例\n",
    "# print(\"Decoded Labels:\", decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./pickles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pickles/label_encoder.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the label encoder to a file\n",
    "joblib.dump(encoder, './pickles/label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ashpri1', 'ashpri1', 'asikoe2', 'asikoe2', 'asikoe2', 'asikoe2',\n",
       "       'asikoe2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2',\n",
       "       'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2', 'ashwoo2',\n",
       "       'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1', 'ashdro1',\n",
       "       'ashdro1', 'ashdro1', 'asiope1', 'asbfly', 'asbfly', 'asbfly',\n",
       "       'asbfly', 'asbfly', 'asbfly'], dtype='<U7')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load label encoder from file\n",
    "loaded_label_encoder = joblib.load('./pickles/label_encoder.joblib')\n",
    "\n",
    "loaded_label_encoder.inverse_transform(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then use the loaded encoder to encode val set labels\n",
    "\n",
    "val_encoded_labels=loaded_label_encoder.fit_transform(val_labels_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded labels: [2 2 2 2 2 2 4 4 4 4 4 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 5 5 5 0 0 0 0]\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print('encoded labels:', val_encoded_labels)\n",
    "print(len(val_encoded_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "all_audios_magnitude_norm=scaler.fit_transform(all_audios_magnitude.reshape(-1,all_audios_magnitude.shape[-1])).reshape(all_audios_magnitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 5, 32000)\n",
      "[[[-0.60150175  1.75064311  1.38202133 ... -0.18650158  0.2927888\n",
      "    0.2458972 ]\n",
      "  [ 0.48014446  4.45496844  1.17155445 ... -0.44697244 -0.45597294\n",
      "   -0.58597602]\n",
      "  [ 1.49252092  2.72711736  5.84214968 ...  0.89863856  0.44401094\n",
      "    0.94712784]\n",
      "  [ 0.48668492  2.07757086  2.4248301  ... -0.08299588  0.18430803\n",
      "    0.1837667 ]\n",
      "  [ 1.7843698   1.79190098  1.43889101 ...  0.21237085  0.79538081\n",
      "    0.32395917]]\n",
      "\n",
      " [[-0.04510071  2.42533688  1.55946895 ...  1.03119262  0.68658353\n",
      "   -0.08195632]\n",
      "  [ 2.25251312  0.68435064  0.13814806 ...  1.80647519 -0.21212418\n",
      "    1.91504601]\n",
      "  [ 1.72232981  1.60825361  0.57778693 ... -0.35908834  0.47506367\n",
      "    1.11303988]\n",
      "  [-0.27109453  0.70009966  2.47999178 ... -0.05392148  0.23511311\n",
      "    0.02880275]\n",
      "  [ 3.06694328  1.31027239  2.80259788 ...  1.64120459  1.3612759\n",
      "    2.61658844]]\n",
      "\n",
      " [[-0.53842718 -0.50581829 -0.49141359 ... -0.38002635 -0.42623826\n",
      "    0.89489129]\n",
      "  [-0.51230913 -0.62498472 -0.64391015 ...  1.51183225  2.2253221\n",
      "    1.7172341 ]\n",
      "  [-0.44145831 -0.31192976 -0.30641902 ...  0.47571949  0.60330599\n",
      "    0.82716076]\n",
      "  [ 0.13346744 -0.53286217  0.09806574 ...  1.14407889 -0.33065819\n",
      "    1.23764562]\n",
      "  [-0.12936588 -0.14188548 -0.14942938 ... -0.05222915 -0.28201742\n",
      "   -0.72580879]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.58876609 -0.69112741 -0.66493213 ... -0.4298516  -0.4006045\n",
      "   -0.47153843]\n",
      "  [-0.60449055 -0.70929543 -0.67964482 ... -0.96128651 -1.07806053\n",
      "   -0.97423598]\n",
      "  [-0.57648811 -0.68861555 -0.65727446 ... -0.75890927 -0.76552928\n",
      "   -0.61604275]\n",
      "  [-0.59737434 -0.69909763 -0.66918453 ... -0.34803002 -0.27079092\n",
      "   -0.45589796]\n",
      "  [-0.60781984 -0.71196688 -0.68091702 ... -0.97918885 -1.03306937\n",
      "   -0.96415046]]\n",
      "\n",
      " [[-0.57006611 -0.6709967  -0.64868261 ...  0.1207396   0.17537314\n",
      "   -0.18745851]\n",
      "  [-0.60517876 -0.70667813 -0.67316406 ... -0.56817618 -0.54328493\n",
      "   -0.55218524]\n",
      "  [-0.60911841 -0.71260856 -0.68130312 ... -1.02004955 -1.07660958\n",
      "   -1.02938166]\n",
      "  [-0.60924431 -0.71169922 -0.6784451  ... -0.96861522 -1.04799692\n",
      "   -0.97125636]\n",
      "  [-0.60699166 -0.71000738 -0.67869308 ... -0.93998989 -1.01977168\n",
      "   -0.9419274 ]]\n",
      "\n",
      " [[-0.58167004 -0.67891149 -0.65007904 ...  0.08649937  0.1060906\n",
      "    0.03030249]\n",
      "  [-0.60655665 -0.70993624 -0.67968381 ... -0.95095031 -1.01486509\n",
      "   -1.00623554]\n",
      "  [-0.60970309 -0.71285054 -0.68118887 ... -0.97902226 -1.04769081\n",
      "   -1.00948362]\n",
      "  [-0.60958636 -0.71124958 -0.67882038 ... -0.73027945 -0.70919136\n",
      "   -0.84928525]\n",
      "  [-0.60723451 -0.71004991 -0.67910788 ... -0.99308242 -1.04626578\n",
      "   -0.97061586]]]\n"
     ]
    }
   ],
   "source": [
    "print(all_audios_magnitude_norm.shape)\n",
    "print(all_audios_magnitude_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pickles/scaler_model.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save scaler\n",
    "\n",
    "joblib.dump(scaler, './pickles/scaler_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, load the StandardScaler instance when deploying the model\n",
    "loaded_scaler = joblib.load('./pickles/scaler_model.pkl')\n",
    "\n",
    "val_all_audios_magnitude_norm = loaded_scaler.transform(val_all_audios_magnitude.reshape(-1, val_all_audios_magnitude.shape[-1])).reshape(val_all_audios_magnitude.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 5, 32000)\n",
      "[[[-2.96661618e-01 -6.21576370e-01  1.35383039e-01 ...  4.46073585e+00\n",
      "    5.01726441e+00  2.89779311e+00]\n",
      "  [-3.21112425e-01 -1.77822865e-02 -2.88892756e-01 ... -1.42808933e-01\n",
      "   -3.35280570e-01 -6.65739535e-01]\n",
      "  [-3.58124485e-01 -4.89836755e-01  3.89714426e-01 ...  2.76594705e-01\n",
      "   -6.83490813e-02 -9.66597405e-02]\n",
      "  [ 5.22984045e-01  2.27902500e-01  2.30623069e-01 ... -2.40654137e-02\n",
      "   -3.07987943e-01 -4.64070870e-01]\n",
      "  [-3.80511734e-01 -3.21570032e-01  2.21361356e-01 ...  1.95712279e+00\n",
      "    1.83632179e+00  3.29184347e+00]]\n",
      "\n",
      " [[ 3.01381659e-01  3.35694978e-01 -1.43715363e-01 ...  1.07712162e+00\n",
      "    7.79471764e-01  3.03887798e-01]\n",
      "  [-4.05915720e-01 -2.13267223e-01 -1.13360247e-01 ...  2.32126216e+00\n",
      "    1.77975424e+00 -7.59200042e-01]\n",
      "  [-1.88137944e-01 -2.74027318e-01 -3.58961878e-01 ...  1.89930823e+00\n",
      "    1.72475294e+00 -1.76090712e-01]\n",
      "  [ 2.88734198e-02  4.88467463e-03  1.85810327e-01 ...  2.44931403e-01\n",
      "    1.78712298e+00  1.41414016e+00]\n",
      "  [ 8.01468570e-02  2.01008664e-01 -3.34673542e-01 ... -3.53940561e-01\n",
      "   -5.46491219e-01  2.79300740e-01]]\n",
      "\n",
      " [[-1.93112226e-01 -2.16653855e-01  2.57624156e-01 ...  1.86317311e+00\n",
      "   -3.83312427e-01  2.98390120e+00]\n",
      "  [ 3.56176547e-01  4.48714427e-01  7.35876113e-01 ...  1.54970408e+00\n",
      "   -6.84005855e-01  8.96612475e-01]\n",
      "  [-4.10350754e-01 -4.88071361e-01 -4.18741456e-01 ...  2.03270410e+00\n",
      "    2.63378544e+00  2.27903075e+00]\n",
      "  [ 1.52341512e-01  3.38860922e-01 -4.10467675e-02 ...  1.08153114e+00\n",
      "    6.50863893e-01 -8.90911691e-02]\n",
      "  [-7.51004243e-02 -1.05751964e-01  9.91619906e-02 ...  3.26514762e+00\n",
      "    4.27738050e+00  3.77096741e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.30525814e-01 -2.99134455e-01  4.50525607e-02 ... -8.73621505e-01\n",
      "   -8.40641125e-01 -8.39064391e-01]\n",
      "  [-5.11326663e-01 -3.23060335e-01 -4.49511441e-01 ... -8.45890234e-01\n",
      "   -8.41547326e-01 -8.40828330e-01]\n",
      "  [-5.78430342e-01 -5.37340036e-01 -1.66765681e-01 ... -9.68005415e-01\n",
      "   -1.02772322e+00 -9.40243134e-01]\n",
      "  [-4.23713222e-01 -2.69630574e-01 -3.94932793e-01 ... -9.60211870e-01\n",
      "   -1.04658989e+00 -1.02923216e+00]\n",
      "  [-5.45430645e-01 -6.30878928e-01 -4.19963662e-01 ... -8.28877783e-01\n",
      "   -8.20073361e-01 -8.78052115e-01]]\n",
      "\n",
      " [[-6.06607696e-01 -3.55917756e-01 -9.41406531e-02 ... -9.21877485e-01\n",
      "   -9.69922658e-01 -9.33919073e-01]\n",
      "  [-5.76874931e-01 -2.78626069e-01  9.16808574e-02 ... -7.87542963e-01\n",
      "   -8.64319162e-01 -8.57121809e-01]\n",
      "  [-2.67539110e-01 -1.04292995e-01 -5.21200420e-01 ... -8.33999633e-01\n",
      "   -1.04711623e+00 -9.28022171e-01]\n",
      "  [-2.60651822e-01 -1.89145180e-01 -1.66331654e-01 ... -1.00564044e+00\n",
      "   -1.08778853e+00 -1.02225558e+00]\n",
      "  [-4.17786738e-01  1.51503515e-01  1.41364505e-01 ... -9.70012059e-01\n",
      "   -1.07537929e+00 -9.76905303e-01]]\n",
      "\n",
      " [[-3.22564875e-01 -4.53097656e-02 -2.58830892e-01 ... -1.01122834e+00\n",
      "   -1.07766919e+00 -1.03877986e+00]\n",
      "  [-4.39999167e-01 -4.64668256e-01 -5.25064006e-01 ... -9.97138119e-01\n",
      "   -1.06736465e+00 -1.02447706e+00]\n",
      "  [-4.23958842e-01 -1.65781160e-03 -4.30878333e-02 ... -9.99208187e-01\n",
      "   -1.07122672e+00 -1.04426356e+00]\n",
      "  [-5.89615009e-01 -1.98892516e-01 -2.86418484e-01 ... -8.92475053e-01\n",
      "   -8.31174512e-01 -8.27206261e-01]\n",
      "  [-3.70193361e-01 -4.00839665e-01 -3.31914607e-01 ... -8.75998524e-01\n",
      "   -9.09720960e-01 -8.44248202e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print(val_all_audios_magnitude_norm.shape)\n",
    "print(val_all_audios_magnitude_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm ./pickles/*\n",
    "! rmdir ./pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build up neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.Tensor(all_audios_magnitude_norm)\n",
    "val_features = torch.Tensor(val_all_audios_magnitude_norm)\n",
    "train_labels = torch.Tensor(encoded_labels)\n",
    "val_labels = torch.Tensor(val_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 5, 32000])\n",
      "torch.Size([34, 5, 32000])\n",
      "torch.Size([33])\n",
      "torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(val_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=5,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=5,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "        self.conv3=nn.Conv1d(in_channels=5,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "    def forward(self,x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x)\n",
    "        x3=self.conv3(x)\n",
    "        # The length of the input data shape is 32000, and the stride is 2, so after conv1d, the length becomes 16000\n",
    "        # The number of output channels of each conv1d layer is 32, so for the shape of the entire output, regardless of batchsize, it is 32*16000\n",
    "        # Because of the chrononet architecture, we need to connect the outputs of the three layers to become 96*16000 output data.\n",
    "        x=torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 96, 16000])\n",
      "tensor([[[ 3.2082e-01, -5.7716e-01, -6.0971e-01,  ..., -8.2652e-01,\n",
      "          -5.5966e-01, -7.3094e-01],\n",
      "         [ 2.3690e+00,  2.6382e+00,  1.8766e+00,  ...,  5.4351e-01,\n",
      "           5.5883e-01,  3.7526e-01],\n",
      "         [-1.0461e+00, -2.5734e+00, -1.7035e+00,  ..., -8.7640e-01,\n",
      "          -7.3219e-01, -4.8908e-01],\n",
      "         ...,\n",
      "         [-8.2224e-01,  7.9491e-01, -4.8606e-01,  ..., -2.6135e-01,\n",
      "           8.7546e-04, -4.2487e-02],\n",
      "         [ 1.1410e+00,  1.2414e+00,  8.9403e-01,  ..., -2.9899e-01,\n",
      "          -3.1044e-01, -2.2978e-01],\n",
      "         [ 3.7328e-02,  7.5953e-02, -1.2442e-01,  ..., -2.8588e-01,\n",
      "           3.2260e-01,  1.3019e-01]],\n",
      "\n",
      "        [[ 4.3485e-01, -8.5247e-01,  1.1724e+00,  ...,  3.1057e-01,\n",
      "           4.0219e-01, -1.0443e-01],\n",
      "         [ 1.1806e+00,  4.9628e-01,  9.9513e-01,  ...,  1.0207e+00,\n",
      "           2.7607e-01,  5.4837e-01],\n",
      "         [-7.0068e-01, -1.7329e+00, -1.4784e+00,  ..., -6.4664e-01,\n",
      "          -9.8242e-01, -1.5394e+00],\n",
      "         ...,\n",
      "         [ 9.8461e-01, -1.5152e-01,  1.6415e+00,  ...,  4.6369e-01,\n",
      "           7.3557e-02, -4.5420e-02],\n",
      "         [-2.2529e-01, -1.6040e-01,  1.1423e+00,  ..., -1.2750e+00,\n",
      "          -3.1265e-01,  3.8642e-01],\n",
      "         [-4.9982e-02,  1.2051e-03,  8.5799e-01,  ..., -3.0067e-01,\n",
      "          -2.6407e-01, -3.3509e-01]],\n",
      "\n",
      "        [[-3.3971e-01, -3.0336e-01, -3.2737e-01,  ...,  3.2199e-01,\n",
      "           3.4796e-01,  5.7818e-01],\n",
      "         [-1.0632e-01, -1.9844e-02, -2.0276e-02,  ...,  3.6603e-01,\n",
      "           4.4810e-01,  7.1753e-01],\n",
      "         [-4.9229e-01, -4.9223e-01, -3.8988e-01,  ...,  7.9921e-01,\n",
      "           3.3109e-01,  8.0203e-01],\n",
      "         ...,\n",
      "         [-1.4089e-01, -2.1721e-01, -4.0429e-02,  ..., -9.5365e-02,\n",
      "           1.3292e-01, -2.9462e-01],\n",
      "         [-3.7304e-03,  5.4888e-02,  1.3852e-01,  ..., -6.3664e-02,\n",
      "           7.2425e-01, -2.6772e-01],\n",
      "         [-1.8002e-01, -1.6181e-01, -2.4511e-01,  ...,  6.0059e-01,\n",
      "          -4.8009e-01,  1.2823e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.8116e-01, -3.0283e-01, -2.9957e-01,  ..., -5.8581e-01,\n",
      "          -5.1248e-01, -6.0012e-01],\n",
      "         [-2.6007e-01, -2.8293e-01, -2.9871e-01,  ..., -1.9164e-01,\n",
      "          -2.2462e-01, -3.2114e-01],\n",
      "         [-1.3165e-01, -1.0618e-01, -1.0184e-01,  ...,  1.0632e-01,\n",
      "           9.1927e-02,  1.3975e-01],\n",
      "         ...,\n",
      "         [-1.0443e-02, -3.9712e-01, -4.2288e-02,  ..., -1.7269e-01,\n",
      "           7.8124e-02, -2.3255e-01],\n",
      "         [-6.6592e-02, -9.2852e-02,  2.6421e-02,  ..., -1.8741e-01,\n",
      "           3.2363e-02,  1.5833e-01],\n",
      "         [-2.3018e-01, -1.5655e-01, -1.8495e-01,  ...,  5.9017e-02,\n",
      "          -1.9356e-01, -1.6707e-01]],\n",
      "\n",
      "        [[-2.7681e-01, -2.8998e-01, -2.8887e-01,  ..., -3.0917e-01,\n",
      "          -2.8462e-01, -2.5084e-01],\n",
      "         [-2.7715e-01, -2.9901e-01, -3.0819e-01,  ..., -6.2539e-01,\n",
      "          -6.0692e-01, -6.3227e-01],\n",
      "         [-1.1974e-01, -9.9230e-02, -1.0155e-01,  ...,  5.7645e-01,\n",
      "           4.8219e-01,  4.3530e-01],\n",
      "         ...,\n",
      "         [-4.1792e-03, -3.8992e-01, -3.4886e-02,  ...,  5.3956e-02,\n",
      "           1.4356e-01, -2.7922e-01],\n",
      "         [-7.2769e-02, -9.5535e-02,  3.3998e-02,  ..., -2.3272e-01,\n",
      "          -2.1248e-01,  6.4008e-03],\n",
      "         [-2.2340e-01, -1.5663e-01, -1.9469e-01,  ..., -1.3842e-01,\n",
      "          -4.7610e-01, -2.0993e-01]],\n",
      "\n",
      "        [[-2.7646e-01, -2.9424e-01, -2.9783e-01,  ..., -5.1954e-01,\n",
      "          -4.7752e-01, -5.1110e-01],\n",
      "         [-2.7644e-01, -3.0003e-01, -3.1115e-01,  ..., -5.9718e-01,\n",
      "          -5.4247e-01, -6.0423e-01],\n",
      "         [-1.2325e-01, -9.8618e-02, -9.3213e-02,  ...,  4.1962e-01,\n",
      "           4.1962e-01,  3.8498e-01],\n",
      "         ...,\n",
      "         [-3.8892e-03, -3.8997e-01, -3.5330e-02,  ..., -1.4176e-02,\n",
      "           1.8743e-01, -2.3401e-01],\n",
      "         [-7.2656e-02, -1.0002e-01,  2.0989e-02,  ..., -3.9267e-01,\n",
      "          -2.6088e-01, -4.0272e-02],\n",
      "         [-2.2319e-01, -1.5316e-01, -1.9063e-01,  ..., -2.7641e-02,\n",
      "          -3.3411e-01, -1.2483e-01]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## check output shape\n",
    "\n",
    "model =ConvBlock1()\n",
    "\n",
    "output_test_1=model(train_features)\n",
    "\n",
    "print(output_test_1.shape)\n",
    "print(output_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "        self.conv3=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x)\n",
    "        x3=self.conv3(x)\n",
    "        # From the output of ConvBlock1, we know that the input shape of convBlock2 is 96*16000\n",
    "        # After the calculation of this block, the output will become 96*8000\n",
    "        x=torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 96, 8000])\n",
      "tensor([[[-1.8299e-02, -3.7645e-01, -1.4992e-01,  ..., -3.0809e-01,\n",
      "          -2.3014e-01, -1.5019e-01],\n",
      "         [ 3.7378e-01,  2.6526e-01,  3.0702e-01,  ...,  1.0680e-01,\n",
      "          -5.2213e-02,  1.6615e-02],\n",
      "         [-3.0561e-01, -2.7692e-01, -3.4077e-01,  ...,  9.5736e-02,\n",
      "           5.5747e-02,  7.0089e-02],\n",
      "         ...,\n",
      "         [ 2.9844e-01, -1.6223e-01,  2.1274e-01,  ...,  4.3536e-01,\n",
      "           3.8831e-01,  3.1669e-01],\n",
      "         [-6.6846e-01, -1.2972e+00, -1.0175e+00,  ..., -6.7008e-01,\n",
      "          -4.7979e-01, -3.5018e-01],\n",
      "         [-7.3458e-02,  7.3898e-01,  2.8393e-01,  ..., -1.1900e-01,\n",
      "          -4.9675e-03, -1.8927e-02]],\n",
      "\n",
      "        [[-6.0680e-02,  1.0096e-02,  2.9825e-01,  ...,  5.2853e-01,\n",
      "          -2.3376e-01, -2.9571e-01],\n",
      "         [-8.7680e-02,  2.2218e-01, -5.9301e-02,  ...,  1.5120e-02,\n",
      "          -2.6125e-01, -2.3533e-01],\n",
      "         [-3.4706e-01, -5.6057e-01, -5.4021e-01,  ...,  1.0120e-01,\n",
      "          -3.8656e-01, -4.3388e-01],\n",
      "         ...,\n",
      "         [ 2.6190e-01,  1.7876e-03, -9.3299e-01,  ...,  1.9800e-01,\n",
      "          -3.6182e-01, -9.3470e-02],\n",
      "         [-1.8134e-01, -7.5882e-01, -1.4673e-01,  ..., -1.1254e-01,\n",
      "          -1.5440e-01, -6.6186e-01],\n",
      "         [ 7.5352e-01,  6.7629e-01,  1.7824e-01,  ...,  5.0097e-01,\n",
      "          -4.5434e-01, -2.4478e-02]],\n",
      "\n",
      "        [[-1.4765e-02,  7.8139e-02,  1.4469e-01,  ..., -4.4981e-03,\n",
      "          -3.9654e-01, -2.9861e-02],\n",
      "         [-2.3072e-01, -2.4998e-01, -2.7088e-01,  ..., -2.4933e-01,\n",
      "          -5.7114e-02, -2.5965e-01],\n",
      "         [ 2.0062e-01,  2.4935e-01,  1.8946e-01,  ..., -4.4870e-01,\n",
      "          -4.5044e-01, -1.5126e-01],\n",
      "         ...,\n",
      "         [ 1.1615e-01,  1.7771e-01,  2.9529e-01,  ..., -3.4772e-02,\n",
      "          -2.1074e-01, -1.6965e-01],\n",
      "         [ 8.2501e-02,  8.0614e-02,  3.2422e-02,  ..., -1.0990e-01,\n",
      "          -1.6844e-01,  3.8159e-02],\n",
      "         [-2.1433e-02, -1.1911e-01, -1.3861e-01,  ...,  2.0779e-01,\n",
      "           1.9416e-01,  9.8499e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4146e-02,  1.4932e-01,  1.7015e-01,  ...,  9.6209e-02,\n",
      "           5.7038e-02,  2.1234e-01],\n",
      "         [-3.2791e-01, -3.0756e-01, -2.9554e-01,  ..., -1.9688e-01,\n",
      "          -1.7480e-01, -2.1947e-01],\n",
      "         [ 2.5415e-01,  2.1723e-01,  1.9528e-01,  ...,  4.3995e-01,\n",
      "           3.9315e-01,  4.6602e-01],\n",
      "         ...,\n",
      "         [ 2.4767e-02,  1.8263e-01,  2.9991e-01,  ...,  3.9283e-01,\n",
      "           4.6588e-01,  4.8741e-01],\n",
      "         [ 5.4926e-02,  1.6375e-01,  1.3526e-01,  ...,  3.8288e-03,\n",
      "          -6.8790e-04, -8.2337e-02],\n",
      "         [-7.0417e-02, -2.1085e-01, -2.3636e-01,  ..., -2.2442e-01,\n",
      "           1.8201e-02,  1.1973e-02]],\n",
      "\n",
      "        [[ 2.8457e-02,  1.5172e-01,  1.7240e-01,  ...,  1.7130e-01,\n",
      "           1.7996e-01,  3.2121e-01],\n",
      "         [-3.3189e-01, -3.0963e-01, -2.9714e-01,  ..., -2.7990e-01,\n",
      "          -2.4638e-01, -2.9344e-01],\n",
      "         [ 2.4927e-01,  2.0912e-01,  1.9083e-01,  ...,  2.0644e-01,\n",
      "           2.1498e-01,  2.4437e-01],\n",
      "         ...,\n",
      "         [ 1.9233e-02,  1.8119e-01,  2.9179e-01,  ...,  2.9819e-01,\n",
      "           2.6061e-01,  3.4121e-01],\n",
      "         [ 5.7850e-02,  1.7186e-01,  1.3772e-01,  ...,  1.1248e-01,\n",
      "           4.3294e-02,  9.0195e-03],\n",
      "         [-7.4110e-02, -2.1206e-01, -2.4292e-01,  ..., -3.4427e-01,\n",
      "          -2.3001e-01, -8.3282e-02]],\n",
      "\n",
      "        [[ 2.8687e-02,  1.5031e-01,  1.7017e-01,  ...,  1.7555e-01,\n",
      "           1.5385e-01,  2.9026e-01],\n",
      "         [-3.2683e-01, -3.0737e-01, -2.9751e-01,  ..., -2.2597e-01,\n",
      "          -2.2152e-01, -2.2494e-01],\n",
      "         [ 2.4779e-01,  2.1316e-01,  1.9313e-01,  ...,  3.3375e-01,\n",
      "           2.9781e-01,  3.6400e-01],\n",
      "         ...,\n",
      "         [ 2.1972e-02,  1.7979e-01,  2.9726e-01,  ...,  3.2581e-01,\n",
      "           3.7460e-01,  4.4272e-01],\n",
      "         [ 5.7355e-02,  1.6951e-01,  1.3469e-01,  ...,  1.9778e-02,\n",
      "           2.5678e-02, -8.5510e-02],\n",
      "         [-7.5720e-02, -2.1577e-01, -2.4353e-01,  ..., -4.5535e-01,\n",
      "          -2.3424e-01, -9.9301e-02]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## check output shape\n",
    "\n",
    "model =ConvBlock2()\n",
    "\n",
    "output_test_2=model(output_test_1)\n",
    "\n",
    "print(output_test_2.shape)\n",
    "print(output_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "        self.conv2=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "        self.conv3=nn.Conv1d(in_channels=96,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x)\n",
    "        x3=self.conv3(x)\n",
    "        # From the output of ConvBlock1, we know that the input shape of convBlock2 is 96*8000\n",
    "        # After the calculation of this block, the output will become 96*4000\n",
    "        x=torch.cat((x1,x2,x3),dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 96, 4000])\n",
      "tensor([[[ 2.1763e-01,  1.6819e-03, -2.9034e-01,  ...,  2.4235e-01,\n",
      "           2.4384e-01,  1.3891e-01],\n",
      "         [-1.9164e-01, -3.7283e-01, -2.2973e-01,  ...,  1.0915e-01,\n",
      "           3.3382e-02,  2.8995e-02],\n",
      "         [-1.2335e-01, -1.3528e-01,  3.1715e-01,  ..., -1.6945e-01,\n",
      "          -1.9558e-01, -9.9069e-02],\n",
      "         ...,\n",
      "         [-2.3842e-01, -4.7206e-01, -4.7228e-01,  ...,  5.5285e-02,\n",
      "           1.6305e-02,  9.8824e-02],\n",
      "         [-5.2599e-01, -9.0719e-02,  1.1435e-01,  ..., -1.3734e-01,\n",
      "          -1.2618e-01,  4.1087e-02],\n",
      "         [ 3.0367e-01, -3.1348e-01,  5.4315e-02,  ..., -3.1884e-02,\n",
      "           3.1888e-02,  8.4352e-02]],\n",
      "\n",
      "        [[-3.3234e-01, -7.4199e-03, -2.3013e-01,  ..., -1.0078e-01,\n",
      "          -1.8817e-01,  4.0805e-04],\n",
      "         [-4.8083e-01, -1.3919e-01, -3.0645e-01,  ..., -1.4822e-01,\n",
      "          -6.0931e-02, -1.7554e-01],\n",
      "         [ 3.1605e-01,  1.7554e-01, -1.8008e-02,  ...,  3.0727e-01,\n",
      "           9.0435e-02,  8.0738e-02],\n",
      "         ...,\n",
      "         [-2.7921e-01, -5.9720e-01, -3.2259e-01,  ..., -5.2278e-01,\n",
      "          -3.2950e-01, -3.0997e-01],\n",
      "         [-2.5926e-01, -1.3802e-01,  2.8156e-01,  ..., -1.4140e-01,\n",
      "           1.8625e-02, -2.4004e-01],\n",
      "         [ 2.6967e-01, -2.5502e-01, -7.8777e-02,  ...,  1.7015e-02,\n",
      "           1.1518e-01,  3.0706e-01]],\n",
      "\n",
      "        [[ 1.9125e-01,  1.8642e-01,  1.6420e-01,  ..., -2.9086e-02,\n",
      "          -8.6362e-02, -1.2023e-03],\n",
      "         [ 1.8186e-01,  1.7672e-01,  1.5443e-01,  ..., -1.3473e-01,\n",
      "          -1.0335e-01, -1.6193e-01],\n",
      "         [-8.0028e-02, -8.0798e-02, -8.5757e-02,  ...,  3.4799e-02,\n",
      "          -5.7688e-02,  1.6804e-01],\n",
      "         ...,\n",
      "         [ 8.7651e-02,  1.5328e-01,  1.6346e-01,  ..., -3.4787e-01,\n",
      "          -2.3232e-01, -1.2650e-01],\n",
      "         [-2.8701e-02,  1.2087e-02,  1.6010e-02,  ...,  1.8552e-01,\n",
      "          -1.4573e-01,  3.2735e-02],\n",
      "         [-1.2242e-02,  6.9848e-02, -6.0709e-03,  ...,  6.1553e-01,\n",
      "           3.9992e-01, -1.7683e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.2843e-01,  2.1238e-01,  2.0705e-01,  ...,  1.9007e-01,\n",
      "           2.4056e-01,  1.5812e-01],\n",
      "         [ 1.7199e-01,  1.7822e-01,  1.5470e-01,  ...,  2.2973e-01,\n",
      "           2.6016e-01,  1.7990e-01],\n",
      "         [-9.9497e-02, -5.5602e-02, -5.3114e-02,  ..., -5.6927e-02,\n",
      "          -2.7266e-02, -1.1979e-01],\n",
      "         ...,\n",
      "         [ 1.1131e-01,  2.2873e-01,  2.0037e-01,  ...,  2.0358e-01,\n",
      "           1.6259e-01,  1.2257e-01],\n",
      "         [ 2.1784e-02,  5.2360e-02,  4.3504e-02,  ...,  1.0191e-01,\n",
      "           4.3512e-02,  6.7930e-02],\n",
      "         [-4.6693e-02,  3.1186e-02,  7.0700e-03,  ...,  6.4973e-02,\n",
      "          -1.9408e-02, -2.8113e-02]],\n",
      "\n",
      "        [[ 2.2612e-01,  2.0961e-01,  2.0562e-01,  ...,  1.4444e-01,\n",
      "           1.4816e-01,  6.3914e-02],\n",
      "         [ 1.6884e-01,  1.7602e-01,  1.5607e-01,  ...,  1.7561e-01,\n",
      "           1.4225e-01,  9.0861e-02],\n",
      "         [-9.6700e-02, -5.4977e-02, -5.0890e-02,  ..., -4.0879e-02,\n",
      "          -3.9836e-02, -1.0900e-01],\n",
      "         ...,\n",
      "         [ 1.0823e-01,  2.2540e-01,  1.9647e-01,  ...,  2.5175e-02,\n",
      "           2.4390e-02,  3.2921e-02],\n",
      "         [ 2.2304e-02,  5.2041e-02,  4.0703e-02,  ...,  2.2197e-02,\n",
      "           2.7973e-02,  3.5518e-02],\n",
      "         [-5.0257e-02,  3.1213e-02,  5.4091e-03,  ...,  4.1729e-02,\n",
      "          -5.8118e-02, -7.3187e-02]],\n",
      "\n",
      "        [[ 2.2787e-01,  2.0981e-01,  2.0592e-01,  ...,  1.7949e-01,\n",
      "           1.8746e-01,  1.0575e-01],\n",
      "         [ 1.7171e-01,  1.7767e-01,  1.5531e-01,  ...,  2.3798e-01,\n",
      "           2.2485e-01,  1.6547e-01],\n",
      "         [-9.9031e-02, -5.4864e-02, -5.0548e-02,  ..., -3.4568e-02,\n",
      "          -6.4009e-02, -1.1873e-01],\n",
      "         ...,\n",
      "         [ 1.0876e-01,  2.2530e-01,  1.9804e-01,  ...,  7.8551e-02,\n",
      "           5.7887e-02,  6.3033e-02],\n",
      "         [ 2.2138e-02,  5.2866e-02,  4.1975e-02,  ...,  4.5664e-02,\n",
      "           3.5442e-02,  3.5523e-02],\n",
      "         [-4.9618e-02,  3.1234e-02,  6.5878e-03,  ...,  5.6714e-02,\n",
      "          -2.8233e-02, -5.2357e-02]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## check output shape\n",
    "\n",
    "model =ConvBlock3()\n",
    "\n",
    "output_test_3=model(output_test_2)\n",
    "\n",
    "print(output_test_3.shape)\n",
    "print(output_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "\n",
    "        return gru_out1,_\n",
    "        \n",
    "        \n",
    "# The above is used to view the output after adding a gru\n",
    "# Next I need to add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 4000, 32])\n",
      "torch.Size([1, 33, 32])\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_4,_=model(train_features)\n",
    "\n",
    "print(output_test_4.shape)\n",
    "\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# The above is used to view the output after adding a gru\n",
    "# Next I need to add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 4000, 64])\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_5=model(train_features)\n",
    "\n",
    "print(output_test_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# The above is used to view the output after adding a gru\n",
    "# Next I need to add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 4000, 96])\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_6=model(train_features)\n",
    "\n",
    "print(output_test_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru4=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "        gru_out4,_=self.gru4(x)\n",
    "\n",
    "        return gru_out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 4000, 32])\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_7=model(train_features)\n",
    "\n",
    "print(output_test_7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, add a fully connected layer\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru4=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.fc1=nn.Linear(in_features=32,out_features=64)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "        gru_out4,_=self.gru4(x)\n",
    "        x = self.fc1(gru_out4[:, -1, :])  #Usually take the final output of GRU\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 64])\n",
      "tensor([[-0.0006,  0.1203,  0.0623,  ...,  0.1123,  0.0983,  0.0780],\n",
      "        [ 0.1523,  0.0730,  0.3505,  ...,  0.0644,  0.1266,  0.2473],\n",
      "        [ 0.0691,  0.1145,  0.2082,  ...,  0.1363,  0.1202,  0.2101],\n",
      "        ...,\n",
      "        [-0.0378,  0.0938, -0.0893,  ...,  0.1510,  0.0767, -0.0551],\n",
      "        [-0.0487,  0.0690, -0.0497,  ...,  0.1291,  0.1071, -0.0163],\n",
      "        [-0.0473,  0.0794, -0.0766,  ...,  0.1403,  0.0967, -0.0395]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_7=model(train_features)\n",
    "\n",
    "print(output_test_7.shape)\n",
    "print(output_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru4=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.fc1=nn.Linear(in_features=32,out_features=64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "        gru_out4,_=self.gru4(x)\n",
    "        x = self.fc1(gru_out4[:, -1, :])  #Usually take the final output of GRU\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 64])\n",
      "tensor([[-3.7648e-01, -0.0000e+00,  3.7256e-01,  ..., -3.2188e-01,\n",
      "          1.4680e-02, -7.5296e-02],\n",
      "        [-5.5064e-01, -0.0000e+00,  3.8999e-01,  ..., -5.1643e-01,\n",
      "          0.0000e+00, -0.0000e+00],\n",
      "        [-5.3573e-01, -0.0000e+00,  0.0000e+00,  ..., -4.0909e-01,\n",
      "          0.0000e+00, -2.9524e-04],\n",
      "        ...,\n",
      "        [-0.0000e+00, -3.0714e-01,  1.3834e-01,  ..., -2.1615e-01,\n",
      "         -0.0000e+00, -2.0412e-01],\n",
      "        [-2.5025e-01, -3.4895e-01,  0.0000e+00,  ..., -1.8650e-01,\n",
      "         -4.0053e-02, -1.8042e-01],\n",
      "        [-0.0000e+00, -3.4395e-01,  1.5104e-01,  ..., -1.6353e-01,\n",
      "         -0.0000e+00, -2.0822e-01]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_8=model(train_features)\n",
    "\n",
    "print(output_test_8.shape)\n",
    "print(output_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the last fully connected layer\n",
    "# Add the regularization layer dropput\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru4=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.fc1=nn.Linear(in_features=32,out_features=64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 6)  # num_classes is the number of categories\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "        gru_out4,_=self.gru4(x)\n",
    "        x = self.fc1(gru_out4[:, -1, :])  #Usually take the final output of GRU\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x=F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 6])\n",
      "tensor([[0.1862, 0.1498, 0.1662, 0.1691, 0.1825, 0.1462],\n",
      "        [0.1911, 0.1891, 0.1450, 0.1723, 0.1360, 0.1665],\n",
      "        [0.2297, 0.1708, 0.1398, 0.1566, 0.1499, 0.1532],\n",
      "        [0.1923, 0.1709, 0.1590, 0.1591, 0.1710, 0.1477],\n",
      "        [0.1692, 0.1335, 0.1750, 0.1552, 0.1695, 0.1976],\n",
      "        [0.1951, 0.1838, 0.1524, 0.1526, 0.1534, 0.1628],\n",
      "        [0.1731, 0.1320, 0.1542, 0.1698, 0.1773, 0.1936],\n",
      "        [0.1623, 0.1599, 0.1777, 0.1471, 0.1757, 0.1772],\n",
      "        [0.1559, 0.1662, 0.1422, 0.1757, 0.2008, 0.1592],\n",
      "        [0.1855, 0.1543, 0.1747, 0.1551, 0.1675, 0.1628],\n",
      "        [0.1689, 0.1798, 0.1451, 0.1769, 0.1628, 0.1665],\n",
      "        [0.1653, 0.1640, 0.1516, 0.1478, 0.1903, 0.1810],\n",
      "        [0.1671, 0.1755, 0.1746, 0.1641, 0.1665, 0.1521],\n",
      "        [0.1746, 0.1474, 0.1773, 0.1644, 0.1582, 0.1781],\n",
      "        [0.1639, 0.1833, 0.1390, 0.1947, 0.1774, 0.1417],\n",
      "        [0.1890, 0.1785, 0.1561, 0.1610, 0.1402, 0.1753],\n",
      "        [0.1662, 0.1704, 0.1778, 0.1740, 0.1393, 0.1723],\n",
      "        [0.1946, 0.1541, 0.1700, 0.1689, 0.1506, 0.1619],\n",
      "        [0.1787, 0.1380, 0.1536, 0.2069, 0.1664, 0.1564],\n",
      "        [0.1737, 0.1628, 0.1645, 0.1494, 0.1861, 0.1635],\n",
      "        [0.1826, 0.1473, 0.1627, 0.1481, 0.1628, 0.1964],\n",
      "        [0.1812, 0.1980, 0.1260, 0.1836, 0.1459, 0.1653],\n",
      "        [0.1855, 0.1440, 0.1484, 0.1890, 0.1822, 0.1509],\n",
      "        [0.1774, 0.1826, 0.1456, 0.1882, 0.1595, 0.1466],\n",
      "        [0.1991, 0.1792, 0.1658, 0.1281, 0.1492, 0.1786],\n",
      "        [0.1893, 0.1825, 0.1626, 0.1602, 0.1527, 0.1527],\n",
      "        [0.2019, 0.1743, 0.1344, 0.1681, 0.1579, 0.1635],\n",
      "        [0.1685, 0.1960, 0.1447, 0.1733, 0.1551, 0.1625],\n",
      "        [0.1886, 0.1543, 0.1390, 0.1818, 0.1476, 0.1888],\n",
      "        [0.1923, 0.1496, 0.1560, 0.1600, 0.1682, 0.1739],\n",
      "        [0.1890, 0.1559, 0.1888, 0.1434, 0.1517, 0.1713],\n",
      "        [0.1743, 0.1811, 0.1618, 0.1343, 0.1693, 0.1792],\n",
      "        [0.1978, 0.1550, 0.1555, 0.1491, 0.1676, 0.1750]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_9=model(train_features)\n",
    "\n",
    "print(output_test_9.shape)\n",
    "print(output_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33])\n",
      "tensor([0, 0, 0, 0, 5, 0, 5, 2, 4, 0, 1, 4, 1, 5, 3, 0, 2, 0, 3, 4, 5, 1, 3, 3,\n",
      "        0, 0, 0, 1, 5, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Use torch.argmax to get the category index with the highest probability\n",
    "predicted_classes = torch.argmax(output_test_9, dim=1)\n",
    "\n",
    "print(predicted_classes.shape)\n",
    "print(predicted_classes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1862, 0.1911, 0.2297, 0.1923, 0.1976, 0.1951, 0.1936, 0.1777, 0.2008,\n",
      "        0.1855, 0.1798, 0.1903, 0.1755, 0.1781, 0.1947, 0.1890, 0.1778, 0.1946,\n",
      "        0.2069, 0.1861, 0.1964, 0.1980, 0.1890, 0.1882, 0.1991, 0.1893, 0.2019,\n",
      "        0.1960, 0.1888, 0.1923, 0.1890, 0.1811, 0.1978],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "tensor([0, 0, 0, 0, 5, 0, 5, 2, 4, 0, 1, 4, 1, 5, 3, 0, 2, 0, 3, 4, 5, 1, 3, 3,\n",
      "        0, 0, 0, 1, 5, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Use torch.max to get the probability value and the corresponding category index at the same time\n",
    "max_probs, predicted_classes = torch.max(output_test_9, dim=1)\n",
    "\n",
    "print(max_probs)       # Print the highest probability value\n",
    "print(predicted_classes)  # Print the corresponding category index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we need to train the model first, we need to calculate the loss\n",
    "# For multi-classification problems, if you choose to use nn.crossentropylss, you need to remove F.softmax(),\n",
    "# Because this loss function combines Log-Softmax and NLL Loss (Negative Log Likelihood Loss).\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1=ConvBlock1()\n",
    "        self.block2=ConvBlock2()\n",
    "        self.block3=ConvBlock3()\n",
    "        self.gru1=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru2=nn.GRU(input_size=32,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru3=nn.GRU(input_size=64,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.gru4=nn.GRU(input_size=96,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.fc1=nn.Linear(in_features=32,out_features=64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 6)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x=x.permute(0,2,1)\n",
    "        gru_out1,_=self.gru1(x)\n",
    "        gru_out2,_=self.gru2(gru_out1)\n",
    "        # According to the chrononet architecture, we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x=torch.cat((gru_out1,gru_out2),dim=2)\n",
    "        gru_out3,_=self.gru3(x)\n",
    "        x=torch.cat((gru_out1,gru_out2,gru_out3),dim=2)\n",
    "        gru_out4,_=self.gru4(x)\n",
    "        x = self.fc1(gru_out4[:, -1, :])  #Usually take the final output of GRU\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 6])\n",
      "tensor([[-1.2378e-01,  2.0501e-01, -2.0715e-01, -2.4703e-02, -1.1928e-01,\n",
      "          1.7309e-02],\n",
      "        [-2.3920e-02,  3.7598e-01,  2.4398e-01, -5.1875e-02, -2.4172e-01,\n",
      "         -1.3735e-02],\n",
      "        [-2.7396e-02,  2.0630e-01, -8.7438e-02,  3.5428e-02, -2.1047e-01,\n",
      "          2.2017e-01],\n",
      "        [-4.1249e-02,  6.7034e-02,  3.2156e-02, -1.6259e-02, -2.5943e-01,\n",
      "         -2.0576e-02],\n",
      "        [ 7.8136e-02,  1.3621e-01, -1.2375e-01, -9.5075e-02, -9.3135e-02,\n",
      "          2.4104e-01],\n",
      "        [-1.7018e-01,  2.0280e-01, -2.4220e-01,  3.2697e-04, -1.2233e-01,\n",
      "         -1.3607e-01],\n",
      "        [ 4.3858e-03,  1.9670e-01, -1.6418e-01,  2.5328e-02, -2.3579e-01,\n",
      "          9.0969e-02],\n",
      "        [ 2.0663e-02,  1.9062e-01, -7.9694e-02,  7.0192e-02, -2.3042e-01,\n",
      "          1.1258e-01],\n",
      "        [-2.9393e-01,  1.7228e-01, -1.0757e-01,  1.3902e-01, -7.7695e-02,\n",
      "         -6.2754e-02],\n",
      "        [-2.0392e-02, -9.3418e-02, -2.5637e-01,  4.9833e-02, -2.5574e-01,\n",
      "          2.6642e-02],\n",
      "        [-4.1795e-02,  1.1969e-01, -1.3126e-01,  7.6483e-02, -1.8405e-01,\n",
      "         -1.2786e-01],\n",
      "        [-2.4057e-02,  3.0353e-01, -8.8835e-02,  1.9072e-01, -3.9700e-01,\n",
      "          9.0624e-02],\n",
      "        [-1.5424e-01,  1.9151e-01, -1.6818e-01,  1.3345e-01, -1.9973e-01,\n",
      "         -1.0985e-01],\n",
      "        [ 3.1216e-02,  1.5294e-01, -4.9443e-02, -1.5604e-02, -6.6795e-02,\n",
      "          8.1769e-02],\n",
      "        [-1.8503e-01,  1.7426e-01, -1.1628e-01,  5.6644e-02, -1.0821e-01,\n",
      "         -5.6911e-02],\n",
      "        [ 2.9470e-02,  9.7176e-02,  1.1119e-01, -1.5739e-02, -2.1568e-01,\n",
      "         -6.3378e-02],\n",
      "        [-6.8113e-02,  1.3676e-01, -1.6001e-01, -6.8050e-02, -2.1570e-01,\n",
      "         -4.1406e-02],\n",
      "        [-1.0458e-01,  2.5945e-01, -9.3578e-02,  1.1731e-01, -1.9732e-01,\n",
      "          1.2972e-03],\n",
      "        [-1.7244e-01,  1.8316e-01, -8.5640e-02,  1.5423e-02, -1.1209e-01,\n",
      "         -1.1534e-01],\n",
      "        [-9.7289e-02,  2.4968e-01, -5.1337e-02,  1.9047e-01, -2.4351e-01,\n",
      "          5.8238e-02],\n",
      "        [ 9.4818e-02,  2.5117e-01, -6.1383e-02,  4.3377e-02, -1.9630e-01,\n",
      "          2.8028e-02],\n",
      "        [ 5.0700e-02,  3.4846e-01,  7.9771e-02,  2.5027e-01, -1.0277e-01,\n",
      "          1.1444e-01],\n",
      "        [-2.8244e-01,  2.0149e-01,  9.4789e-02,  3.9389e-02, -1.6154e-01,\n",
      "         -9.5428e-02],\n",
      "        [-1.6898e-01,  9.7642e-02,  1.9247e-01,  4.8670e-02, -1.4402e-01,\n",
      "         -1.3890e-01],\n",
      "        [ 4.2444e-02,  9.9284e-02, -8.9875e-02, -1.7315e-02, -1.6452e-01,\n",
      "          5.4732e-02],\n",
      "        [ 4.9698e-02,  3.9909e-01,  5.0524e-03,  1.2549e-01, -2.5430e-01,\n",
      "          1.5882e-01],\n",
      "        [ 3.5750e-02,  1.2834e-01, -2.9738e-02, -4.3409e-02, -2.7715e-01,\n",
      "         -1.5476e-02],\n",
      "        [-3.3961e-02,  1.4254e-01, -1.5083e-01,  1.6744e-01, -1.2757e-01,\n",
      "          2.2549e-02],\n",
      "        [ 8.7138e-02,  1.0240e-01, -1.5035e-01,  1.4140e-02, -1.3328e-01,\n",
      "          7.2049e-02],\n",
      "        [-6.3875e-03,  2.0179e-01, -6.2232e-02,  1.0456e-01, -1.7550e-01,\n",
      "          1.6935e-01],\n",
      "        [ 5.3971e-03,  1.8742e-01, -1.6061e-01,  1.4341e-01, -2.0890e-01,\n",
      "         -3.0698e-02],\n",
      "        [ 3.1693e-02,  1.3235e-01, -2.1241e-01,  4.3052e-02, -9.1175e-02,\n",
      "          1.2368e-01],\n",
      "        [-6.0196e-02,  1.1718e-01, -1.0951e-01,  7.2600e-03, -2.3815e-01,\n",
      "          8.9494e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "output_test_10=model(train_features)\n",
    "\n",
    "print(output_test_10.shape)\n",
    "print(output_test_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use lightningmodule to organize my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because in this part I want to verify whether organize model code into LightningModule is successful, I need to prepare the data first\n",
    "# The data here is only used temporarily, and the data processing flow will be re-integrated later\n",
    "feature_dataset=TensorDataset(train_features,train_labels)\n",
    "\n",
    "# This is for batch shuffuling, so that the same batch can be obtained each time\n",
    "torch.manual_seed(123)\n",
    "train_loader= DataLoader(feature_dataset, batch_size=32,num_workers=12,persistent_workers=True, shuffle=True)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "val_dataset=TensorDataset(val_features,val_labels)\n",
    "val_loader= DataLoader(val_dataset, batch_size=32, num_workers=12,persistent_workers=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNetModule(L.LightningModule):\n",
    "    def __init__(self,model,learning_rate):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        self.lr=learning_rate\n",
    "        self.train_acc=torchmetrics.Accuracy(task='multiclass',num_classes=6)\n",
    "        self.val_acc=torchmetrics.Accuracy(task='multiclass',num_classes=6)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x: feature data for training \n",
    "\n",
    "        This is the part of the neural model that is used to read or build\n",
    "        define the computation performed at every call define the computation performed at every call\n",
    "\n",
    "        return:\n",
    "            model's output\n",
    "        '''\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        we need to train the model right here\n",
    "        including provide the loss step, acc calculation step\n",
    "\n",
    "        This function will perform the following operations:\n",
    "        1. Calculate the loss value for each training batch\n",
    "        2. Perform optimization and gradient descent (automatically performed by lightningModule)\n",
    "        3. Update parameters (automatically performed by lightningModule)\n",
    "        https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#training\n",
    "        '''\n",
    "        # read batch data\n",
    "        features,labels=batch\n",
    "\n",
    "        # Send data to GPU for training\n",
    "        features=features.to(self.device)\n",
    "        labels=labels.to(self.device)\n",
    "        \n",
    "        # feeding feature to the model\n",
    "        # Only self() is used here because the forward() function is called automatically\n",
    "        # forward propagation\n",
    "        out=self(features)\n",
    "\n",
    "        # After getting the output of the model, you need to calculate the loss function\n",
    "        loss=F.cross_entropy(out, labels)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # After adding self.train_acc=torchmetrics.Accuracy(task='multiclass',num_classes=6)\n",
    "        # You can add the steps to calculate accuracy below\n",
    "        # Because we use cross_entropy() as the loss function\n",
    "        # So we need to use argmax to convert to normal values ​​for accuracy calculation\n",
    "        # predicted_labels=torch.argmax(out)\n",
    "        # But torchmetrics.Accuracy is already configured to handle logits for multi-class classification problems. \n",
    "        # It will apply softmax (or log_softmax) and calculate argmax internally to determine the most likely category.\n",
    "        acc=self.train_acc(out,labels)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "\n",
    "        # In training_step(), we only calculate and return the loss. \n",
    "        # The optimization part does not belong to this part, \n",
    "        # and the optimization method will be defined in configure_optimizers.\n",
    "        return loss # this is passed to the optimizer for training\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        The val step is not used in training, but only in validation.\n",
    "        '''\n",
    "        features,labels=batch\n",
    "\n",
    "        # Send data to GPU for training\n",
    "        features=features.to(self.device)\n",
    "        labels=labels.to(self.device)\n",
    "        \n",
    "        out=self(features)\n",
    "        loss=F.cross_entropy(out, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        acc=self.val_acc(out,labels)\n",
    "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "\n",
    "        The optimizer defined here will be automatically called by lightningModule\n",
    "        Used in the training step\n",
    "        '''\n",
    "        optimizer=torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # If you have only one tensor (feature) in your TensorDataset, batch will be a tuple containing a tensor and an empty tuple (since there are no labels)\n",
    "        features= batch[0]\n",
    "        features=features.to(self.device)\n",
    "        predictions = self(features)\n",
    "        # Because what our model ultimately wants is the probability of an object corresponding to all categories, so add the softmax function here\n",
    "        probabilities = torch.softmax(predictions, dim=1)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ChronoNet          | 131 K \n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "131 K     Trainable params\n",
      "0         Non-trainable params\n",
      "131 K     Total params\n",
      "0.527     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "model=ChronoNet()\n",
    "ChronoNetModule=ChronoNetModule(model=model,learning_rate=0.1)\n",
    "\n",
    "trainer=L.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "    devices=1, # use all gpus if applicable like value=1 or \"auto\"\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    model=ChronoNetModule,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup lightning data loader module for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x161c588e0>\n",
      "First sample features: tensor([[-0.6015,  1.7506,  1.3820,  ..., -0.1865,  0.2928,  0.2459],\n",
      "        [ 0.4801,  4.4550,  1.1716,  ..., -0.4470, -0.4560, -0.5860],\n",
      "        [ 1.4925,  2.7271,  5.8421,  ...,  0.8986,  0.4440,  0.9471],\n",
      "        [ 0.4867,  2.0776,  2.4248,  ..., -0.0830,  0.1843,  0.1838],\n",
      "        [ 1.7844,  1.7919,  1.4389,  ...,  0.2124,  0.7954,  0.3240]])\n",
      "First sample label: tensor(2.)\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x1620a2740>\n",
      "Batch 1:\n",
      "Features: tensor([[[-0.5475, -0.3557, -0.3218,  ...,  1.2892,  1.7215,  1.4805],\n",
      "         [-0.1687, -0.3946, -0.3593,  ..., -0.7355, -0.7527, -0.9198],\n",
      "         [-0.4361, -0.6789, -0.5990,  ...,  0.1586,  0.2221,  0.1589],\n",
      "         [-0.4438, -0.5252, -0.5615,  ..., -0.3533, -0.6874,  0.1831],\n",
      "         [-0.5679, -0.2651, -0.3601,  ..., -0.2300, -0.0471, -0.8231]],\n",
      "\n",
      "        [[-0.6069, -0.7100, -0.6757,  ..., -0.8249, -0.8882, -0.9098],\n",
      "         [-0.5898, -0.6902, -0.6601,  ..., -0.7868, -0.8398, -0.8637],\n",
      "         [-0.6073, -0.7098, -0.6785,  ..., -0.8794, -0.9743, -0.9259],\n",
      "         [-0.6088, -0.7112, -0.6803,  ..., -0.9651, -1.0769, -1.0200],\n",
      "         [-0.6057, -0.7020, -0.6625,  ..., -0.5707, -0.4604, -0.4290]],\n",
      "\n",
      "        [[ 0.1371,  0.2473, -0.0370,  ...,  1.4090,  1.3841,  0.2122],\n",
      "         [-0.2047,  0.2919,  0.6170,  ...,  0.0557, -0.5296, -0.1005],\n",
      "         [ 1.7479,  1.0662,  0.6010,  ..., -0.3213, -0.4131,  0.8754],\n",
      "         [ 0.0725,  0.4889,  0.6152,  ..., -0.4720, -0.5679, -0.5186],\n",
      "         [ 0.9111,  0.7254, -0.1911,  ...,  0.5905,  2.0391,  1.2875]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4419, -0.5587, -0.5200,  ...,  0.5037, -0.0745, -0.4411],\n",
      "         [-0.4258, -0.5566, -0.4142,  ..., -0.2473, -0.4744, -0.1998],\n",
      "         [-0.4266, -0.5922, -0.5286,  ..., -0.6694, -0.5957, -0.5454],\n",
      "         [-0.5322, -0.6090, -0.6426,  ..., -0.5597, -0.5423, -0.3351],\n",
      "         [-0.5332, -0.5915, -0.6055,  ..., -0.6855, -0.8432, -0.9546]],\n",
      "\n",
      "        [[-0.5888, -0.6911, -0.6649,  ..., -0.4299, -0.4006, -0.4715],\n",
      "         [-0.6045, -0.7093, -0.6796,  ..., -0.9613, -1.0781, -0.9742],\n",
      "         [-0.5765, -0.6886, -0.6573,  ..., -0.7589, -0.7655, -0.6160],\n",
      "         [-0.5974, -0.6991, -0.6692,  ..., -0.3480, -0.2708, -0.4559],\n",
      "         [-0.6078, -0.7120, -0.6809,  ..., -0.9792, -1.0331, -0.9642]],\n",
      "\n",
      "        [[ 2.3616,  2.0694,  1.2680,  ..., -0.4895, -0.2716,  0.0218],\n",
      "         [ 0.6789,  0.8673,  0.5359,  ...,  1.1339, -0.2932, -0.3814],\n",
      "         [ 0.9216,  1.0516,  0.8245,  ...,  1.5157,  0.2796,  1.9018],\n",
      "         [-0.1260, -0.0819, -0.0470,  ...,  2.4673,  1.2380,  2.9817],\n",
      "         [-0.4737,  0.3919,  0.8619,  ...,  2.0421,  2.6700,  2.4665]]])\n",
      "Labels: tensor([3., 0., 1., 1., 0., 2., 3., 0., 1., 3., 1., 4., 3., 3., 3., 1., 2., 0.,\n",
      "        1., 3., 3., 3., 4., 4., 4., 0., 3., 1., 5., 4., 0., 1.])\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x1619a9ea0>\n",
      "First sample features: (tensor([[-0.6015,  1.7506,  1.3820,  ..., -0.1865,  0.2928,  0.2459],\n",
      "        [ 0.4801,  4.4550,  1.1716,  ..., -0.4470, -0.4560, -0.5860],\n",
      "        [ 1.4925,  2.7271,  5.8421,  ...,  0.8986,  0.4440,  0.9471],\n",
      "        [ 0.4867,  2.0776,  2.4248,  ..., -0.0830,  0.1843,  0.1838],\n",
      "        [ 1.7844,  1.7919,  1.4389,  ...,  0.2124,  0.7954,  0.3240]]),)\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x2ccfd2380>\n",
      "Batch 1:\n",
      "Features: [tensor([[[ 0.1371,  0.2473, -0.0370,  ...,  1.4090,  1.3841,  0.2122],\n",
      "         [-0.2047,  0.2919,  0.6170,  ...,  0.0557, -0.5296, -0.1005],\n",
      "         [ 1.7479,  1.0662,  0.6010,  ..., -0.3213, -0.4131,  0.8754],\n",
      "         [ 0.0725,  0.4889,  0.6152,  ..., -0.4720, -0.5679, -0.5186],\n",
      "         [ 0.9111,  0.7254, -0.1911,  ...,  0.5905,  2.0391,  1.2875]],\n",
      "\n",
      "        [[-0.4569, -0.5263, -0.5566,  ..., -0.8449, -0.8989, -0.9070],\n",
      "         [-0.5542, -0.4786, -0.4001,  ..., -0.0298,  0.2295, -0.1898],\n",
      "         [-0.4863, -0.5029, -0.2924,  ..., -0.9072, -0.8452, -0.9599],\n",
      "         [-0.2487, -0.4547, -0.4418,  ..., -0.6548, -0.7244, -0.7643],\n",
      "         [-0.4240, -0.6555, -0.6315,  ..., -0.6191, -0.4269, -0.6378]],\n",
      "\n",
      "        [[-0.4444, -0.6591, -0.5558,  ..., -0.4350, -0.1834,  0.1719],\n",
      "         [-0.5636, -0.5422, -0.5940,  ..., -0.4830, -0.6132, -0.8364],\n",
      "         [-0.5488, -0.4596, -0.5171,  ...,  2.5667, -0.5969,  2.7372],\n",
      "         [-0.5288, -0.5957, -0.3675,  ..., -0.8872, -0.7017, -0.5625],\n",
      "         [-0.3085, -0.5812, -0.5024,  ..., -0.7475, -0.8525, -0.7907]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4937, -0.2892,  0.0046,  ..., -0.1068, -0.3962, -0.0074],\n",
      "         [-0.5284, -0.5860, -0.5922,  ..., -0.8989, -0.4749,  0.0496],\n",
      "         [-0.5974, -0.5901, -0.4942,  ..., -0.4255, -0.1683, -0.3736],\n",
      "         [-0.5372, -0.5762, -0.3096,  ...,  0.2986, -0.0162, -0.1972],\n",
      "         [-0.3748, -0.6193, -0.3851,  ..., -0.1194, -0.2251, -0.3783]],\n",
      "\n",
      "        [[ 0.9160,  0.1412,  0.0904,  ...,  0.3772,  1.5838,  0.3127],\n",
      "         [ 2.3358,  2.4154,  0.9792,  ..., -0.2934,  0.7099,  0.6657],\n",
      "         [ 1.2084,  1.0846,  1.0642,  ...,  1.8594,  1.7990,  0.5036],\n",
      "         [ 1.1650,  0.8667,  1.0895,  ...,  0.6630,  1.3646,  0.8117],\n",
      "         [-0.0355, -0.0035, -0.6203,  ...,  0.5187,  1.8913,  0.0114]],\n",
      "\n",
      "        [[-0.6015, -0.3455, -0.3304,  ..., -0.2872,  0.1242,  0.2577],\n",
      "         [-0.5310, -0.6592, -0.3561,  ..., -0.6742, -0.8243, -0.8655],\n",
      "         [-0.2735, -0.5754, -0.5677,  ..., -0.8604,  0.1520, -0.7797],\n",
      "         [-0.2801, -0.5588, -0.5981,  ...,  0.9865,  0.8443, -0.0759],\n",
      "         [-0.4545, -0.6269, -0.5745,  ..., -0.7782, -0.7701, -0.7376]]])]\n"
     ]
    }
   ],
   "source": [
    "# Here I want to verify the role of tensordataset and dataloader\n",
    "\n",
    "dataset=TensorDataset(train_features,train_labels)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Access and print the first sample\n",
    "first_data, first_label = dataset[0]\n",
    "print(\"First sample features:\", first_data)\n",
    "print(\"First sample label:\", first_label)\n",
    "\n",
    "loader= DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(loader)\n",
    "\n",
    "# In PyTorch, the DataLoader object itself is not a structure for direct visualization or direct printing of contents, but an iterable object,\n",
    "# used to generate data in batches in each iteration. If you want to see the contents of the data loaded from the DataLoader, \n",
    "# you can print out the contents of one or more batches of data by iterating it.\n",
    "\n",
    "# View data contents by iterating DataLoader\n",
    "for i, (data, label) in enumerate(loader):\n",
    "    print(f\"Batch {i + 1}:\")\n",
    "    print(\"Features:\", data)  # print feature\n",
    "    print(\"Labels:\", label)   # print label\n",
    "    # If you only want to see the first batch of data, you can add a break statement here\n",
    "    break  # Uncomment this line and only print the first batch of data\n",
    "\n",
    "dataset=TensorDataset(train_features)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Access and print the first sample\n",
    "first_data= dataset[0]\n",
    "print(\"First sample features:\", first_data)\n",
    "\n",
    "loader= DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "loader= DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(loader)\n",
    "\n",
    "# View data contents by iterating DataLoader\n",
    "for i, data in enumerate(loader):\n",
    "    print(f\"Batch {i + 1}:\")\n",
    "    print(\"Features:\", data) \n",
    "    break \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNetDataModule(L.LightningDataModule):\n",
    "    def __init__(self,train=None,train_label=None,val=None,val_label=None,pred=None,batch_size:int=32,num_workers:int=12):\n",
    "        super().__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.num_workers=num_workers\n",
    "        self.train=train\n",
    "        self.train_label=train_label\n",
    "        self.val=val\n",
    "        self.val_label=val_label\n",
    "        self.pred=pred\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # You need to create a tensor to include the data and labels together\n",
    "        # The length of the first dimension of the two must be equal\n",
    "        dataset=TensorDataset(self.train,self.train_label)\n",
    "        loader= DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True, shuffle=True)\n",
    "\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset=TensorDataset(self.val,self.val_label)\n",
    "        loader= DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True,shuffle=False)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataset=TensorDataset(self.pred)\n",
    "        loader=DataLoader(dataset,batch_size=self.batch_size,shuffle=False)\n",
    "\n",
    "        return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ChronoNet          | 131 K \n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "131 K     Trainable params\n",
      "0         Non-trainable params\n",
      "131 K     Total params\n",
      "0.527     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ChronoNetDataModule object at 0x1623807c0>\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:48<00:00,  0.02it/s, v_num=9, train_loss_step=1.800, train_acc_step=0.182, val_loss_step=2.870, val_acc_step=0.000, val_loss_epoch=3.340, val_acc_epoch=0.147, train_loss_epoch=1.800, train_acc_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:48<00:00,  0.02it/s, v_num=9, train_loss_step=1.800, train_acc_step=0.182, val_loss_step=2.870, val_acc_step=0.000, val_loss_epoch=3.340, val_acc_epoch=0.147, train_loss_epoch=1.800, train_acc_epoch=0.182]\n"
     ]
    }
   ],
   "source": [
    "# Previously we used a separate dataloader to feed the model\n",
    "# Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "dm=ChronoNetDataModule(train=train_features,train_label=train_labels,val=val_features,val_label=val_labels,batch_size=33)\n",
    "print(dm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=ChronoNet()\n",
    "ChronoNetModule=ChronoNetModule(model=model,learning_rate=0.1)\n",
    "\n",
    "trainer=L.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "    devices=1, # use all gpus if applicable like value=1 or \"auto\"\n",
    "    default_root_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/',\n",
    "    logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    model=ChronoNetModule,\n",
    "    datamodule=dm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:18<00:00,  0.11it/s]\n"
     ]
    }
   ],
   "source": [
    "dm=ChronoNetDataModule(pred=val_features,batch_size=33)\n",
    "\n",
    "\n",
    "predictions=trainer.predict(model=ChronoNetModule,datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "2\n",
      "1\n",
      "[tensor([[5.6575e-01, 1.0018e-03, 4.6197e-03, 3.2325e-01, 9.1726e-02, 1.3648e-02],\n",
      "        [5.4818e-01, 4.8104e-04, 5.5269e-03, 2.7513e-01, 1.5051e-01, 2.0169e-02],\n",
      "        [5.4210e-01, 5.1319e-04, 5.4765e-03, 2.8120e-01, 1.5046e-01, 2.0249e-02],\n",
      "        [3.0845e-01, 1.9877e-01, 3.2014e-02, 4.1173e-01, 2.9122e-02, 1.9915e-02],\n",
      "        [5.3649e-01, 2.9894e-03, 8.7351e-03, 3.3508e-01, 9.7630e-02, 1.9080e-02],\n",
      "        [4.6169e-01, 5.7414e-03, 9.5079e-03, 3.9689e-01, 1.0934e-01, 1.6827e-02],\n",
      "        [1.6857e-01, 8.0688e-02, 5.3463e-02, 2.4675e-01, 3.3104e-01, 1.1950e-01],\n",
      "        [3.5587e-03, 9.4635e-01, 2.1553e-02, 2.3794e-02, 1.4574e-03, 3.2845e-03],\n",
      "        [1.0027e-01, 1.6685e-01, 1.0053e-01, 2.0234e-01, 2.9135e-01, 1.3866e-01],\n",
      "        [9.3389e-02, 1.6281e-01, 1.2310e-01, 1.1841e-01, 2.8799e-01, 2.1430e-01],\n",
      "        [3.9036e-01, 6.5489e-03, 1.0268e-02, 2.4627e-01, 2.9684e-01, 4.9722e-02],\n",
      "        [4.8367e-03, 9.1764e-01, 3.9673e-02, 2.5409e-02, 3.2278e-03, 9.2131e-03],\n",
      "        [8.3479e-02, 3.2066e-01, 1.2967e-01, 1.4709e-01, 1.6473e-01, 1.5438e-01],\n",
      "        [2.7313e-02, 5.2377e-01, 1.1541e-01, 6.6613e-02, 1.3102e-01, 1.3588e-01],\n",
      "        [2.3028e-02, 6.6500e-01, 6.1188e-02, 7.7492e-02, 9.4303e-02, 7.8985e-02],\n",
      "        [2.3668e-01, 3.3913e-01, 2.6268e-02, 3.5324e-01, 2.6426e-02, 1.8253e-02],\n",
      "        [4.7598e-02, 2.3659e-01, 1.2000e-01, 7.7913e-02, 3.2452e-01, 1.9338e-01],\n",
      "        [5.2860e-01, 4.9137e-03, 7.6877e-03, 3.9528e-01, 5.1745e-02, 1.1774e-02],\n",
      "        [1.3305e-01, 1.0724e-01, 7.3430e-02, 1.6088e-01, 3.6384e-01, 1.6156e-01],\n",
      "        [2.0907e-01, 3.9668e-02, 3.7971e-02, 1.7643e-01, 4.2155e-01, 1.1531e-01],\n",
      "        [3.4764e-02, 4.7706e-01, 1.7388e-01, 5.8586e-02, 1.2676e-01, 1.2895e-01],\n",
      "        [8.5206e-03, 8.3192e-01, 7.0193e-02, 5.5140e-02, 1.5224e-02, 1.8997e-02],\n",
      "        [4.8122e-01, 3.4001e-02, 2.7294e-02, 3.9878e-01, 3.8699e-02, 2.0006e-02],\n",
      "        [5.0270e-01, 3.1407e-03, 6.0532e-03, 2.4050e-01, 2.1437e-01, 3.3240e-02],\n",
      "        [3.2021e-01, 3.4719e-02, 2.0002e-01, 2.5017e-01, 1.0316e-01, 9.1729e-02],\n",
      "        [5.8280e-01, 2.0919e-04, 3.7964e-03, 2.3891e-01, 1.5304e-01, 2.1256e-02],\n",
      "        [6.4742e-02, 2.5304e-01, 1.2606e-01, 1.2853e-01, 2.7879e-01, 1.4884e-01],\n",
      "        [6.0337e-01, 1.9059e-03, 6.4897e-03, 3.3563e-01, 4.4271e-02, 8.3335e-03],\n",
      "        [4.4235e-01, 2.5057e-03, 7.9291e-03, 1.9468e-01, 3.0436e-01, 4.8183e-02],\n",
      "        [6.3393e-02, 1.8906e-01, 1.3785e-01, 1.1922e-01, 2.8152e-01, 2.0896e-01],\n",
      "        [1.3329e-03, 9.7273e-01, 1.1484e-02, 7.4972e-03, 2.1491e-03, 4.8027e-03],\n",
      "        [4.1996e-03, 9.0407e-01, 6.4034e-02, 1.7054e-02, 2.7059e-03, 7.9338e-03],\n",
      "        [1.6671e-01, 2.8565e-01, 7.6819e-02, 2.7095e-01, 1.1354e-01, 8.6328e-02]]), tensor([[0.0565, 0.3001, 0.0957, 0.1704, 0.2469, 0.1303]])]\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions[0]))\n",
    "print(len(predictions))\n",
    "print(len(predictions[1]))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 6])\n",
      "tensor([[5.6575e-01, 1.0018e-03, 4.6197e-03, 3.2325e-01, 9.1726e-02, 1.3648e-02],\n",
      "        [5.4818e-01, 4.8104e-04, 5.5269e-03, 2.7513e-01, 1.5051e-01, 2.0169e-02],\n",
      "        [5.4210e-01, 5.1319e-04, 5.4765e-03, 2.8120e-01, 1.5046e-01, 2.0249e-02],\n",
      "        [3.0845e-01, 1.9877e-01, 3.2014e-02, 4.1173e-01, 2.9122e-02, 1.9915e-02],\n",
      "        [5.3649e-01, 2.9894e-03, 8.7351e-03, 3.3508e-01, 9.7630e-02, 1.9080e-02],\n",
      "        [4.6169e-01, 5.7414e-03, 9.5079e-03, 3.9689e-01, 1.0934e-01, 1.6827e-02],\n",
      "        [1.6857e-01, 8.0688e-02, 5.3463e-02, 2.4675e-01, 3.3104e-01, 1.1950e-01],\n",
      "        [3.5587e-03, 9.4635e-01, 2.1553e-02, 2.3794e-02, 1.4574e-03, 3.2845e-03],\n",
      "        [1.0027e-01, 1.6685e-01, 1.0053e-01, 2.0234e-01, 2.9135e-01, 1.3866e-01],\n",
      "        [9.3389e-02, 1.6281e-01, 1.2310e-01, 1.1841e-01, 2.8799e-01, 2.1430e-01],\n",
      "        [3.9036e-01, 6.5489e-03, 1.0268e-02, 2.4627e-01, 2.9684e-01, 4.9722e-02],\n",
      "        [4.8367e-03, 9.1764e-01, 3.9673e-02, 2.5409e-02, 3.2278e-03, 9.2131e-03],\n",
      "        [8.3479e-02, 3.2066e-01, 1.2967e-01, 1.4709e-01, 1.6473e-01, 1.5438e-01],\n",
      "        [2.7313e-02, 5.2377e-01, 1.1541e-01, 6.6613e-02, 1.3102e-01, 1.3588e-01],\n",
      "        [2.3028e-02, 6.6500e-01, 6.1188e-02, 7.7492e-02, 9.4303e-02, 7.8985e-02],\n",
      "        [2.3668e-01, 3.3913e-01, 2.6268e-02, 3.5324e-01, 2.6426e-02, 1.8253e-02],\n",
      "        [4.7598e-02, 2.3659e-01, 1.2000e-01, 7.7913e-02, 3.2452e-01, 1.9338e-01],\n",
      "        [5.2860e-01, 4.9137e-03, 7.6877e-03, 3.9528e-01, 5.1745e-02, 1.1774e-02],\n",
      "        [1.3305e-01, 1.0724e-01, 7.3430e-02, 1.6088e-01, 3.6384e-01, 1.6156e-01],\n",
      "        [2.0907e-01, 3.9668e-02, 3.7971e-02, 1.7643e-01, 4.2155e-01, 1.1531e-01],\n",
      "        [3.4764e-02, 4.7706e-01, 1.7388e-01, 5.8586e-02, 1.2676e-01, 1.2895e-01],\n",
      "        [8.5206e-03, 8.3192e-01, 7.0193e-02, 5.5140e-02, 1.5224e-02, 1.8997e-02],\n",
      "        [4.8122e-01, 3.4001e-02, 2.7294e-02, 3.9878e-01, 3.8699e-02, 2.0006e-02],\n",
      "        [5.0270e-01, 3.1407e-03, 6.0532e-03, 2.4050e-01, 2.1437e-01, 3.3240e-02],\n",
      "        [3.2021e-01, 3.4719e-02, 2.0002e-01, 2.5017e-01, 1.0316e-01, 9.1729e-02],\n",
      "        [5.8280e-01, 2.0919e-04, 3.7964e-03, 2.3891e-01, 1.5304e-01, 2.1256e-02],\n",
      "        [6.4742e-02, 2.5304e-01, 1.2606e-01, 1.2853e-01, 2.7879e-01, 1.4884e-01],\n",
      "        [6.0337e-01, 1.9059e-03, 6.4897e-03, 3.3563e-01, 4.4271e-02, 8.3335e-03],\n",
      "        [4.4235e-01, 2.5057e-03, 7.9291e-03, 1.9468e-01, 3.0436e-01, 4.8183e-02],\n",
      "        [6.3393e-02, 1.8906e-01, 1.3785e-01, 1.1922e-01, 2.8152e-01, 2.0896e-01],\n",
      "        [1.3329e-03, 9.7273e-01, 1.1484e-02, 7.4972e-03, 2.1491e-03, 4.8027e-03],\n",
      "        [4.1996e-03, 9.0407e-01, 6.4034e-02, 1.7054e-02, 2.7059e-03, 7.9338e-03],\n",
      "        [1.6671e-01, 2.8565e-01, 7.6819e-02, 2.7095e-01, 1.1354e-01, 8.6328e-02],\n",
      "        [5.6503e-02, 3.0012e-01, 9.5731e-02, 1.7040e-01, 2.4693e-01, 1.3032e-01]])\n"
     ]
    }
   ],
   "source": [
    "## Because the dataloader to be predicted is divided into two batches, we need to stack all batch outputs together here\n",
    "\n",
    "result = torch.cat((predictions[0], predictions[1]), dim=0)\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup callbacks for saving checkpoint and earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ChronoNet          | 131 K \n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "131 K     Trainable params\n",
      "0         Non-trainable params\n",
      "131 K     Total params\n",
      "0.527     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[337], line 31\u001b[0m\n\u001b[1;32m     21\u001b[0m trainer\u001b[38;5;241m=\u001b[39mL\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     22\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop_callback, checkpoint_callback],\n\u001b[1;32m     23\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     logger\u001b[38;5;241m=\u001b[39mCSVLogger(save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/\u001b[39m\u001b[38;5;124m'\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchrononet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChronoNetModule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dm可以自动从object里面找到对应的dataloader进行训练，不需要指定\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:110\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@_no_grad_context\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_OUT_DICT]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:166\u001b[0m, in \u001b[0;36m_EvaluationLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m stage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stage\n\u001b[1;32m    165\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_source\n\u001b[0;32m--> 166\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m \u001b[43m_request_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;241m.\u001b[39mdataloader_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dataloader()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataloaders, CombinedLoader):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:342\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    The requested dataloader\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:309\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_datamodule_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:179\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[332], line 22\u001b[0m, in \u001b[0;36mChronoNetDataModule.val_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mval_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     loader\u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers, persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loader\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/torch/utils/data/dataset.py:203\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/birdclef/lib/python3.10/site-packages/torch/utils/data/dataset.py:204\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 204\u001b[0m         \u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    205\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "dm=ChronoNetDataModule(train=train_features,train_label=train_labels,val=val_features,val_label=val_labels,batch_size=33)\n",
    "\n",
    "# Set model checkpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss', # Monitor validation loss\n",
    "    dirpath='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/checkpoints/',\n",
    "    filename='chrononet-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1, # Save the model with the lowest validation loss\n",
    "    mode='min',  # Minimize monitoring metrics\n",
    "    auto_insert_metric_name=False  # Prevent automatic addition of metric names to paths\n",
    ")\n",
    "\n",
    "## Set early stopping point\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_acc',\n",
    "   min_delta=0.0,\n",
    "   patience=3,\n",
    "   verbose=True,\n",
    "   mode='max'\n",
    ")\n",
    "\n",
    "\n",
    "trainer=L.Trainer(\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "    devices=1, # use all gpus if applicable like value=1 or \"auto\"\n",
    "    default_root_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/',\n",
    "    logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    model=ChronoNetModule,\n",
    "    datamodule=dm\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
