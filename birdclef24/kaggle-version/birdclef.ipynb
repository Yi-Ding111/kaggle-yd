{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f98f5b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-19T07:04:54.980318Z",
     "iopub.status.busy": "2024-05-19T07:04:54.979904Z",
     "iopub.status.idle": "2024-05-19T07:05:55.816207Z",
     "shell.execute_reply": "2024-05-19T07:05:55.814665Z"
    },
    "papermill": {
     "duration": 60.850234,
     "end_time": "2024-05-19T07:05:55.819667",
     "exception": false,
     "start_time": "2024-05-19T07:04:54.969433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q /kaggle/input/birdclef-extra/python_packages/lightning-2.2.0-py3-none-any.whl --no-deps\n",
    "\n",
    "!pip install -q /kaggle/input/birdclef-extra/python_packages/colorednoise-2.2.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffff239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:05:55.838041Z",
     "iopub.status.busy": "2024-05-19T07:05:55.837643Z",
     "iopub.status.idle": "2024-05-19T07:06:10.325808Z",
     "shell.execute_reply": "2024-05-19T07:06:10.323843Z"
    },
    "papermill": {
     "duration": 14.501819,
     "end_time": "2024-05-19T07:06:10.329873",
     "exception": false,
     "start_time": "2024-05-19T07:05:55.828054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bfee8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:10.348199Z",
     "iopub.status.busy": "2024-05-19T07:06:10.347749Z",
     "iopub.status.idle": "2024-05-19T07:06:10.361083Z",
     "shell.execute_reply": "2024-05-19T07:06:10.359666Z"
    },
    "papermill": {
     "duration": 0.025498,
     "end_time": "2024-05-19T07:06:10.363753",
     "exception": false,
     "start_time": "2024-05-19T07:06:10.338255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "    \n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f850f1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:10.381749Z",
     "iopub.status.busy": "2024-05-19T07:06:10.381346Z",
     "iopub.status.idle": "2024-05-19T07:06:10.388189Z",
     "shell.execute_reply": "2024-05-19T07:06:10.387010Z"
    },
    "papermill": {
     "duration": 0.019103,
     "end_time": "2024-05-19T07:06:10.390721",
     "exception": false,
     "start_time": "2024-05-19T07:06:10.371618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    Initialize the parameters of the fully connected layer\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # Initialize the weights and biases of the network layer\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # Check if the layer has a bias attribute\n",
    "        if layer.bias is not None: # and bis is not none\n",
    "            layer.bias.data.fill_(0.0) # If a bias exists, initialize it to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a34c36e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:10.408919Z",
     "iopub.status.busy": "2024-05-19T07:06:10.408530Z",
     "iopub.status.idle": "2024-05-19T07:06:10.443390Z",
     "shell.execute_reply": "2024-05-19T07:06:10.442138Z"
    },
    "papermill": {
     "duration": 0.047069,
     "end_time": "2024-05-19T07:06:10.445898",
     "exception": false,
     "start_time": "2024-05-19T07:06:10.398829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "\n",
    "        # load  pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=False,in_chans=3)\n",
    "        pretrained_model.load_state_dict(torch.load('/kaggle/input/birdclef-extra/backbones/tf-efficientnetv2_s_in21k/tf_efficientnetv2_s_in21k_weights.pth'))\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer.\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers) \n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features \n",
    "\n",
    "        # dense layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True)\n",
    "\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\")\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "        # freeze\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.encoder.eval()\n",
    "        # self.fc1.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # Use the pre-trained model (excluding the last two layers) for calculation\n",
    "        clip=self.encoder(clip) # feature extractor\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them to compress the dimension\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "\n",
    "        # mix audio up\n",
    "        mixup = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "        clip, audio_label,audio_weights=mixup(X=clip,Y=audio_label,weight=audio_weights)\n",
    "\n",
    "        # Use Compose to combine multiple audio transformation operations. \n",
    "        # These operations are applied to the input audio data to enhance the generalization and robustness of the model.\n",
    "        # clip=self.audio_transforms(clip,sample_rate=self.sample_rate)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip)\n",
    "\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Random mask part of the Spectrogram, which helps the model learn to be robust when information is missing in certain time periods.\n",
    "\n",
    "        time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)\n",
    "\n",
    "        clip = time_mask_transform(clip)\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip)\n",
    "\n",
    "        # mix audio up\n",
    "        mixup2 = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "        clip, audio_label,audio_weights = mixup2(clip, audio_label, audio_weights)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip)\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        audio_label=audio_label\n",
    "        clip=clip\n",
    "        audio_weights=audio_weights\n",
    "\n",
    "        # convert audio to mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip)\n",
    "\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip)\n",
    "\n",
    "        #generalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip)\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # If you have only one tensor (feature) in your TensorDataset, batch will be a tuple containing a tensor and an empty tuple (since there are no labels)\n",
    "        features= batch\n",
    "        features=features\n",
    "        predictions = self(features)\n",
    "        # Because what our model ultimately wants is the probability of an object corresponding to all categories, \n",
    "        # the sigmoid function is used here because we want to treat each class as a separate probability, so softmax is not needed\n",
    "        probabilities = predictions.sigmoid().detach()\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f9ee21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:10.463745Z",
     "iopub.status.busy": "2024-05-19T07:06:10.463082Z",
     "iopub.status.idle": "2024-05-19T07:06:15.014427Z",
     "shell.execute_reply": "2024-05-19T07:06:15.013132Z"
    },
    "papermill": {
     "duration": 4.563491,
     "end_time": "2024-05-19T07:06:15.017332",
     "exception": false,
     "start_time": "2024-05-19T07:06:10.453841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.2.2, which is newer than your current Lightning version: v2.2.0\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnetv2_s_in21k to current tf_efficientnetv2_s.in21k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "# 1. load checkpoint\n",
    "class_num=len(np.load('/kaggle/input/birdclef-extra/external_files/3-bird-cates.npy',allow_pickle=True))\n",
    "\n",
    "model = BirdModelModule.load_from_checkpoint(\n",
    "    checkpoint_path=\"/kaggle/input/birdclef-extra/attention-05-19/sed_s21k_43-44-450.98.ckpt\",\n",
    "    class_num=class_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bff37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.035849Z",
     "iopub.status.busy": "2024-05-19T07:06:15.035453Z",
     "iopub.status.idle": "2024-05-19T07:06:15.143065Z",
     "shell.execute_reply": "2024-05-19T07:06:15.141801Z"
    },
    "papermill": {
     "duration": 0.120071,
     "end_time": "2024-05-19T07:06:15.146073",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.026002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_audio_dir = '/kaggle/input/birdclef-2024/test_soundscapes/'\n",
    "pred_files = [test_audio_dir+f for f in sorted(os.listdir(test_audio_dir))]\n",
    "if len(pred_files) == 1:\n",
    "    test_audio_dir = '/kaggle/input/birdclef-2024/unlabeled_soundscapes/'\n",
    "    pred_files = [test_audio_dir+f for f in sorted(os.listdir(test_audio_dir))][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a04fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.163951Z",
     "iopub.status.busy": "2024-05-19T07:06:15.163540Z",
     "iopub.status.idle": "2024-05-19T07:06:15.172178Z",
     "shell.execute_reply": "2024-05-19T07:06:15.171055Z"
    },
    "papermill": {
     "duration": 0.020446,
     "end_time": "2024-05-19T07:06:15.174681",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.154235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_audio(audio: torch.Tensor, segment_length:int):\n",
    "\n",
    "    '''\n",
    "    split raw audio tensor into multiple clips with 5 seconds long.\n",
    "\n",
    "    Parameters:\n",
    "        audio: the raw audio tensor\n",
    "        segment_length: the audio length of each 5 seconds\n",
    "\n",
    "    return:\n",
    "        parts: list includes all clips\n",
    "        end_time_list: the list of all clips' end time in seconds\n",
    "    '''\n",
    "\n",
    "    length_audio = audio.shape[1]\n",
    "    parts = []\n",
    "    end_time_list=[]\n",
    "    end_time=5\n",
    "    for i in range(0, length_audio, segment_length):\n",
    "        part = audio[0][i:i + segment_length]\n",
    "        if len(part) == segment_length:  # Ensure the fragment lengths are consistent\n",
    "            parts.append(part)  #Store the raw bytes of audio data\n",
    "            end_time_list.append(end_time)\n",
    "            end_time+=5\n",
    "\n",
    "        \n",
    "\n",
    "    return parts,end_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3c757e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.192796Z",
     "iopub.status.busy": "2024-05-19T07:06:15.192368Z",
     "iopub.status.idle": "2024-05-19T07:06:15.199463Z",
     "shell.execute_reply": "2024-05-19T07:06:15.198295Z"
    },
    "papermill": {
     "duration": 0.019263,
     "end_time": "2024-05-19T07:06:15.202221",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.182958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regarding the data of a single audio, some audio information needs to be paid attention to, such as audio duration and number of channels.\n",
    "\n",
    "\n",
    "def audio_info(audio: torch.Tensor, sample_rate: int):\n",
    "    \"\"\"\n",
    "    Grab all information of the input audio loaded by torchaudio.\n",
    "\n",
    "    Parameters:\n",
    "        audio: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "\n",
    "    Return:\n",
    "        duration_seconds: Duration of the audio in seconds\n",
    "        num_channels: Number of audio channels\n",
    "    \"\"\"\n",
    "    # The audio duration time (seconds)\n",
    "    duration_seconds = audio.shape[1] / sample_rate\n",
    "\n",
    "    # The number of channels\n",
    "    num_channels = audio.shape[0]\n",
    "\n",
    "\n",
    "    return duration_seconds, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2e248f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.220292Z",
     "iopub.status.busy": "2024-05-19T07:06:15.219844Z",
     "iopub.status.idle": "2024-05-19T07:06:15.226164Z",
     "shell.execute_reply": "2024-05-19T07:06:15.224930Z"
    },
    "papermill": {
     "duration": 0.018327,
     "end_time": "2024-05-19T07:06:15.228749",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.210422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_audio(path: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        path: Path to the .ogg file\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4104fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.247946Z",
     "iopub.status.busy": "2024-05-19T07:06:15.247513Z",
     "iopub.status.idle": "2024-05-19T07:06:15.257504Z",
     "shell.execute_reply": "2024-05-19T07:06:15.256214Z"
    },
    "papermill": {
     "duration": 0.021966,
     "end_time": "2024-05-19T07:06:15.260132",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.238166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_transform(batch):\n",
    "    \"\"\"\n",
    "    Transform audio data into normalized mel spectrogram in decibel scale.\n",
    "    \"\"\"\n",
    "#     print('1111')\n",
    "    n_fft = int(0.04 * 32000)  # Convert window size to sample points\n",
    "    hop_length = int(0.02 * 32000)  # Convert hop size to sample points\n",
    "    n_mels = 40  # Number of Mel filters\n",
    "\n",
    "    # Create Mel Spectrogram transformer\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=32000,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    )\n",
    "    \n",
    "    # Create dB transformer\n",
    "    db_transform = AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "    melspec_list = []\n",
    "\n",
    "    for audio_clip in batch:\n",
    "        # Convert audio clip to tensor and add a new dimension\n",
    "        audio_clip = audio_clip.unsqueeze(0)\n",
    "\n",
    "        # Generate Mel Spectrogram\n",
    "        melspec = mel_transformer(audio_clip)\n",
    "        \n",
    "        # Convert Mel Spectrogram to dB\n",
    "        db_melspec = db_transform(melspec)\n",
    "        \n",
    "        # Normalize the spectrogram\n",
    "        normalized_melspec = (db_melspec + 80) / 80\n",
    "#         print(normalized_melspec.shape)\n",
    "\n",
    "        melspec_list.append(normalized_melspec)\n",
    "\n",
    "    # Stack the list of tensors into a single tensor\n",
    "    stacked_melspecs = torch.stack(melspec_list)\n",
    "    \n",
    "    return stacked_melspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8780d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.279392Z",
     "iopub.status.busy": "2024-05-19T07:06:15.277942Z",
     "iopub.status.idle": "2024-05-19T07:06:15.291052Z",
     "shell.execute_reply": "2024-05-19T07:06:15.289828Z"
    },
    "papermill": {
     "duration": 0.025648,
     "end_time": "2024-05-19T07:06:15.293788",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.268140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_deltas(specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\") -> torch.Tensor:\n",
    "    \"\"\"Compute delta coefficients of a tensor, usually a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "        win_length (int, optional): The window length used for computing delta (Default: 5)\n",
    "        mode (str, optional): Mode parameter passed to padding (Default: \"replicate\")\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "    \"\"\"\n",
    "    device = specgram.device  # Get the device of the input tensor\n",
    "    dtype = specgram.dtype\n",
    "\n",
    "    # pack batch\n",
    "    shape = specgram.size()\n",
    "    specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "    assert win_length >= 3\n",
    "    n = (win_length - 1) // 2\n",
    "    denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "    # Create the kernel tensor, making sure it is on the same device as the input tensor\n",
    "    kernel = torch.arange(-n, n + 1, 1, dtype=dtype,device=device).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "    output = (\n",
    "        torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "    )\n",
    "\n",
    "    # unpack batch\n",
    "    output = output.reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def make_delta(input_tensor: torch.Tensor):\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    input_tensor = compute_deltas(input_tensor)\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f421b12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.311581Z",
     "iopub.status.busy": "2024-05-19T07:06:15.311177Z",
     "iopub.status.idle": "2024-05-19T07:06:15.316741Z",
     "shell.execute_reply": "2024-05-19T07:06:15.315648Z"
    },
    "papermill": {
     "duration": 0.017705,
     "end_time": "2024-05-19T07:06:15.319562",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.301857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_delta(batch):\n",
    "#     batch=torch.stack(batch)\n",
    "    delta_1 = make_delta(batch)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([batch, delta_1, delta_2], dim=1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2c0ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.338057Z",
     "iopub.status.busy": "2024-05-19T07:06:15.336995Z",
     "iopub.status.idle": "2024-05-19T07:06:15.342635Z",
     "shell.execute_reply": "2024-05-19T07:06:15.341510Z"
    },
    "papermill": {
     "duration": 0.01736,
     "end_time": "2024-05-19T07:06:15.344961",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.327601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04974791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.363656Z",
     "iopub.status.busy": "2024-05-19T07:06:15.363232Z",
     "iopub.status.idle": "2024-05-19T07:06:15.387891Z",
     "shell.execute_reply": "2024-05-19T07:06:15.386652Z"
    },
    "papermill": {
     "duration": 0.037549,
     "end_time": "2024-05-19T07:06:15.390830",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.353281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(columns=['row_id']+np.load('/kaggle/input/birdclef-extra/ChroNet-05-17/13-2-bird-cates-preprepared.npy',allow_pickle=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "264b0c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:15.409357Z",
     "iopub.status.busy": "2024-05-19T07:06:15.408963Z",
     "iopub.status.idle": "2024-05-19T07:06:16.141291Z",
     "shell.execute_reply": "2024-05-19T07:06:16.139876Z"
    },
    "papermill": {
     "duration": 0.745012,
     "end_time": "2024-05-19T07:06:16.144268",
     "exception": false,
     "start_time": "2024-05-19T07:06:15.399256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 3. Using the model to make predictions\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"auto\", \n",
    "        devices='auto'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2227d680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:16.162901Z",
     "iopub.status.busy": "2024-05-19T07:06:16.162500Z",
     "iopub.status.idle": "2024-05-19T07:06:33.311933Z",
     "shell.execute_reply": "2024-05-19T07:06:33.310608Z"
    },
    "papermill": {
     "duration": 17.162239,
     "end_time": "2024-05-19T07:06:33.314933",
     "exception": false,
     "start_time": "2024-05-19T07:06:16.152694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Missing logger folder: /kaggle/working/lightning_logs\n",
      "2024-05-19 07:06:19.058687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-19 07:06:19.058818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-19 07:06:19.237283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5d59939689434bbd76d9b6d27002d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_18/2682359517.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  submission=pd.concat([submission,df],ignore_index=True)\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc2b7e364f048f3b8414bfe2cd2f6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "# audio_clips_list=[]\n",
    "# clip_names_list=[]\n",
    "\n",
    "for path in pred_files:\n",
    "    # read audio as tensor\n",
    "    audio,sr=read_audio(path=path)\n",
    "\n",
    "    # get audio corresponding informatino\n",
    "    duration_seconds,num_channels=audio_info(audio=audio,sample_rate=sr)\n",
    "\n",
    "    # split audio into multi clips with 5 seconds\n",
    "    audio_clips,end_time_list=split_audio(audio=audio,segment_length=5*sr)\n",
    "\n",
    "    # generate each label name for each clip\n",
    "    soundscape_id=path.split('/')[-1].split('.')[0]\n",
    "    clip_name=[f'{soundscape_id}_{end_time}' for end_time in end_time_list]\n",
    "\n",
    "#     audio_clips_list.extend(audio_clips)\n",
    "    \n",
    "#     clip_names_list.extend(clip_name)\n",
    "    \n",
    "#     print(audio_clips)\n",
    "#     print(type(audio_clips))\n",
    "    \n",
    "#     audio_clips = torch.stack(audio_clips)\n",
    "#     print(type(audio_clips))\n",
    "\n",
    "    \n",
    "    melspec_list=pred_transform(audio_clips)\n",
    "    \n",
    "#     print(melspec_list.shape)\n",
    "#     print(len(melspec_list))\n",
    "#     print(type(melspec_list))\n",
    "#     print(type(melspec_list[0][0]))\n",
    "    \n",
    "    x=image_delta(melspec_list)\n",
    "    \n",
    "#     print(x.shape)\n",
    "#     print(type(x))\n",
    "    \n",
    "#     ds=TensorDataset(x)\n",
    "    \n",
    "    dataloader = DataLoader(dataset=x, batch_size=8, shuffle=False, num_workers=3)\n",
    "    \n",
    "#     dataset = TensorDataset(x)  # Assuming 'x' is a tensor\n",
    "#     for i in range(min(len(dataset), 3)):  # Check the first few elements\n",
    "#         sample = dataset[i]\n",
    "#         print(type(sample), [s.shape for s in sample])\n",
    "\n",
    "#     for batch in dataloader:\n",
    "#         if isinstance(batch, tuple):\n",
    "#             print([b.shape for b in batch])  # If it's a tuple of tensors\n",
    "#         else:\n",
    "#             print(type(batch), batch.shape)\n",
    "#         break\n",
    "    \n",
    "    # prediction\n",
    "    predictions = trainer.predict(model, dataloaders=dataloader)\n",
    "    \n",
    "    # Convert each tensor to a NumPy array and use them as rows of the DataFrame\n",
    "    data_frames = [pd.DataFrame(tensor.numpy()) for tensor in predictions]\n",
    "    \n",
    "    # Merge all DataFrames into one big DataFrame\n",
    "    # Each tensor forms a block of the DataFrame\n",
    "    df = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    df.columns=np.load('/kaggle/input/birdclef-extra/ChroNet-05-17/13-2-bird-cates-preprepared.npy',allow_pickle=True).tolist()\n",
    "    \n",
    "    # create a new Series\n",
    "    new_column = pd.Series(clip_name, name='row_id')\n",
    "    \n",
    "    df.insert(0,'row_id',new_column)\n",
    "    \n",
    "    submission=pd.concat([submission,df],ignore_index=True)\n",
    "    \n",
    "#     display(submission)\n",
    "    \n",
    "    del df, new_column, data_frames, predictions, dataloader, x, melspec_list,clip_name, audio_clips, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "499f6bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:33.336224Z",
     "iopub.status.busy": "2024-05-19T07:06:33.335783Z",
     "iopub.status.idle": "2024-05-19T07:06:33.376502Z",
     "shell.execute_reply": "2024-05-19T07:06:33.375461Z"
    },
    "papermill": {
     "duration": 0.054528,
     "end_time": "2024-05-19T07:06:33.379213",
     "exception": false,
     "start_time": "2024-05-19T07:06:33.324685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "738d5b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T07:06:33.399800Z",
     "iopub.status.busy": "2024-05-19T07:06:33.399381Z",
     "iopub.status.idle": "2024-05-19T07:06:33.404444Z",
     "shell.execute_reply": "2024-05-19T07:06:33.403361Z"
    },
    "papermill": {
     "duration": 0.018208,
     "end_time": "2024-05-19T07:06:33.406839",
     "exception": false,
     "start_time": "2024-05-19T07:06:33.388631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d922d0a",
   "metadata": {
    "papermill": {
     "duration": 0.009185,
     "end_time": "2024-05-19T07:06:33.425454",
     "exception": false,
     "start_time": "2024-05-19T07:06:33.416269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4898300,
     "sourceId": 8455746,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 105.23781,
   "end_time": "2024-05-19T07:06:37.037837",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-19T07:04:51.800027",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1c5d59939689434bbd76d9b6d27002d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_740629ad9789459c9b0cb86195b2d0b8",
        "IPY_MODEL_86a4754b047647ff9b66a48b2973aa49",
        "IPY_MODEL_5175f2fd6ba945779b90af3f0ed90a94"
       ],
       "layout": "IPY_MODEL_208c29a2fa57472aacbd2214cc7d93b0"
      }
     },
     "208c29a2fa57472aacbd2214cc7d93b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "28089ba3c5a94d6ab22746e24d113316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2b4295991166468aa5a96235060d9e36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_48bfa2354e634677a3837b390fb5f50d",
       "max": 6,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b3f54bb44d364f2ebdc68714cd0208fa",
       "value": 6
      }
     },
     "3a3517a990894909a2c6fb293d30d348": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "48bfa2354e634677a3837b390fb5f50d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a8419dbeefd4675b28659cb0dc41a0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c70ea1498ed44f17b8cd91c9ff44c8af",
       "placeholder": "​",
       "style": "IPY_MODEL_b68351fff6e94723a4dc0c0a8302c22c",
       "value": " 6/6 [00:01&lt;00:00,  4.53it/s]"
      }
     },
     "5175f2fd6ba945779b90af3f0ed90a94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c77312664de24ff2b0a1c9055cee1a27",
       "placeholder": "​",
       "style": "IPY_MODEL_922e7dc4e0204bbc9de340d619179be0",
       "value": " 6/6 [00:01&lt;00:00,  4.06it/s]"
      }
     },
     "740629ad9789459c9b0cb86195b2d0b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86e6e30a2ad14885ace3a3a4a5df2407",
       "placeholder": "​",
       "style": "IPY_MODEL_28089ba3c5a94d6ab22746e24d113316",
       "value": "Predicting DataLoader 0: 100%"
      }
     },
     "86a4754b047647ff9b66a48b2973aa49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8d41e3c47a54b8691dd9ebc0b944009",
       "max": 6,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c0f6e1be762f4e39b7d6a659804d4d3c",
       "value": 6
      }
     },
     "86e6e30a2ad14885ace3a3a4a5df2407": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f2b462b219a4cf59e277c4523eaeaab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "922e7dc4e0204bbc9de340d619179be0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b3f54bb44d364f2ebdc68714cd0208fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b68351fff6e94723a4dc0c0a8302c22c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bf1c014a300947bfb5e94aa220cd57dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8f2b462b219a4cf59e277c4523eaeaab",
       "placeholder": "​",
       "style": "IPY_MODEL_f405946ef7154c419bd29732aecbb25e",
       "value": "Predicting DataLoader 0: 100%"
      }
     },
     "c0f6e1be762f4e39b7d6a659804d4d3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c70ea1498ed44f17b8cd91c9ff44c8af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c77312664de24ff2b0a1c9055cee1a27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8d41e3c47a54b8691dd9ebc0b944009": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efc2b7e364f048f3b8414bfe2cd2f6c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bf1c014a300947bfb5e94aa220cd57dd",
        "IPY_MODEL_2b4295991166468aa5a96235060d9e36",
        "IPY_MODEL_4a8419dbeefd4675b28659cb0dc41a0b"
       ],
       "layout": "IPY_MODEL_3a3517a990894909a2c6fb293d30d348"
      }
     },
     "f405946ef7154c419bd29732aecbb25e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
