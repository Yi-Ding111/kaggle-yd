{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use workers to speed up data processing, I separated some blocks of 3-efficient-in21k-feature-extractor into separate packages\n",
    "\n",
    "Specifically, you can find the package in common/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q packages/torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl\n",
    "!pip install -q packages/lightning-2.2.0-py3-none-any.whl\n",
    "!pip install -q packages/colorednoise-2.2.0-py3-none-any.whl\n",
    "!pip install -q packages/librosa-0.10.2-py3-none-any.whl\n",
    "!pip install -q packages/torch_audiomentations-0.11.1-py3-none-any.whl\n",
    "!pip install -q packages/torchinfo-1.8.0-py3-none-any.whl\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-05-11 14:01:51.055250: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-11 14:01:51.102700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.audiotransform import rating_value_interplote,audio_weight, Mixup,image_delta,Mixup2,mel_transform\n",
    "from common.audioprocess import read_audio \n",
    "from common.audioprocess import CustomCompose,CustomOneOf,NoiseInjection,GaussianNoise,PinkNoise,AddGaussianNoise,AddGaussianSNR\n",
    "from common.audiodatasets import BirdclefDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Number of CUDA devices: 1\n",
      "CUDA device 0 cores: 40\n"
     ]
    }
   ],
   "source": [
    "# check cuda and select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "if device == \"cuda\":\n",
    "    # get the num of devices\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_devices)\n",
    "\n",
    "    # Iterate over each CUDA device and print its core count\n",
    "    for i in range(num_devices):\n",
    "        print(\"CUDA device\", i, \"cores:\", torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# initilize tran and val sets\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "# random select a sample into val set and other part into train set.\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "# split dataset based off index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 20000 pieces data from trainset\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# add these selected data into valset\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# drop these data out of trainset\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197556, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20182, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is an unbalanced dataset, the amount of data in each category is very different\n",
    "# So I will calculate the weight of each category here\n",
    "# **(-0.5) The purpose is to reduce the relative influence of high-frequency categories and increase the influence of low-frequency categories, so as to help the model better learn those uncommon categories\n",
    "# The purpose of calculating this is to build a WeightedRandomSampler, so that each time a batch is extracted using dataloader, it is more friendly to data of different categories.\n",
    "\n",
    "def sampling_weight(df)->torch.Tensor:\n",
    "    '''\n",
    "    calculate the sampling weight of each audio file\n",
    "\n",
    "    because this is imbalanced dataset\n",
    "    we hope the category with less data has large probability to be picked.\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # Map weights to each row of the original data\n",
    "    sample_weights_map = df['primary_label'].map(sample_weights)\n",
    "\n",
    "    # Convert a pandas Series to a NumPy array\n",
    "    sample_weights_np = sample_weights_map.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors using torch.from_numpy\n",
    "    sample_weights_tensor = torch.from_numpy(sample_weights_np)\n",
    "\n",
    "    return sample_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x7fcff408fdf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_csv(metadata_path,header=0)\n",
    "sample_weights_tensor=sampling_weight(df=train_df)\n",
    "# Here we will build an argument sampler that dataloader will use\n",
    "# It should be noted that the order of weights in the constructed sampler needs to be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "#Create a sampler based on the newly obtained weight list\n",
    "sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order of this is very important and needs to be matched one by one in subsequent training, I will save these categories here\n",
    "# save as .npy file\n",
    "np.save(\"./external/3-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self,sampler,train_df:pd.DataFrame,val_df:pd.DataFrame,bird_category_dir:str,audio_dir: str = 'data/audio',batch_size:int=128,workers=4):\n",
    "        super().__init__()\n",
    "        self.train_df=train_df\n",
    "        self.val_df=val_df\n",
    "        self.bird_category_dir=bird_category_dir\n",
    "        self.audio_dir=audio_dir\n",
    "        self.batch_size=batch_size\n",
    "        self.sampler=sampler\n",
    "        self.workers=workers\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.train_df,bird_category_dir=self.bird_category_dir,audio_dir=self.audio_dir,train=True)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, sampler=self.sampler, pin_memory=True,num_workers=self.workers)\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.val_df,bird_category_dir=self.bird_category_dir,audio_dir=self.audio_dir,train=False)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, pin_memory=True,num_workers=self.workers)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    initilize dense layer parameters\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # initilize net layers weight and bias\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # check if layer has bias value\n",
    "        if layer.bias is not None: # and bias is not none\n",
    "            layer.bias.data.fill_(0.0) # if existing bias, set as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we want to put the high dimentional fetures grabbed into a attention block\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "    # x： This is the final output after processing by the attention weighting and classification layer\n",
    "    # shape：(n_samples, out_features)。Since the time dimension is compressed by summation, we end up with a single value for each sample and each output feature.\n",
    "    # norm_att: This is the output of the attention layer (att) processed with softmax and tanh functions, which shows which parts of the input sequence the model should focus on. Normalization ensures that the attention weights of all time steps add up to 1, which makes it easier to intuitively interpret the importance of each time step.\n",
    "    # shape：(n_samples, out_features, n_time)，where out_features is the number of output features of the att convolutional layer, which is the same as the out_features parameter of the input. Each time step and each output feature has a normalized weight.\n",
    "    # cla: This is the output of the classification layer (cla), which is obtained by processing the input features through another 1D convolutional layer. This output layer is usually used to directly predict task-related outputs, such as the probability of a class label.\n",
    "    # shape：(n_samples, out_features, n_time)，Same shape as norm_att . This means that each output feature at each time step has a value processed by the activation function.\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        self.audio_transforms = Compose(\n",
    "            [\n",
    "                # AddColoredNoise(p=0.5),\n",
    "                PitchShift(\n",
    "                    min_transpose_semitones=-4,\n",
    "                    max_transpose_semitones=4,\n",
    "                    sample_rate=32000,\n",
    "                    p=0.4,\n",
    "                ),\n",
    "                Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # load pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer.\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers).to(device) # Encapsulate multiple layers in sequence\n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # create a fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "        # freeze part parameters\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.encoder.eval()\n",
    "        # self.fc1.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        return\n",
    "    \n",
    "\n",
    "    def preprocess(self, clip):\n",
    "        clip = mel_transform(sample_rate=self.sample_rate, audio=clip)\n",
    "        clip = torchaudio.transforms.AmplitudeToDB()(clip)\n",
    "        clip = (clip + 80) / 80  # normalization\n",
    "        clip = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)(clip)\n",
    "        clip = image_delta(clip)\n",
    "        return clip\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "        # use pre-trained model (exclude the last two layers) for computation\n",
    "        clip=self.encoder(clip.to(device)) # feature extractor\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them Dimensionality compression\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "        # audio_label=audio_label.to(device)\n",
    "        # clip=clip.to(device)\n",
    "        # audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # mixup audio\n",
    "        mixup = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "        clip, audio_label,audio_weights=mixup(X=clip,Y=audio_label,weight=audio_weights)\n",
    "\n",
    "        # Use Compose to combine multiple audio transformation operations. \n",
    "        # These operations are applied to the input audio data to improve the generalization and robustness of the model.\n",
    "        # clip=self.audio_transforms(clip,sample_rate=self.sample_rate)\n",
    "\n",
    "        # # Convert audio data into mel spectrogram\n",
    "        # clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        # ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "        # db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        # clip=db_transform(clip).to(device)\n",
    "\n",
    "        # #normalization\n",
    "        # clip=(clip+80)/80\n",
    "\n",
    "        # # Randomly masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "\n",
    "        # time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)\n",
    "\n",
    "        # clip = time_mask_transform(clip)\n",
    "\n",
    "        # # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        # clip= image_delta(clip.to(device))\n",
    "\n",
    "        clip=self.preprocess(clip)\n",
    "\n",
    "        # audio mix up\n",
    "        mixup2 = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "        clip, audio_label,audio_weights = mixup2(clip, audio_label, audio_weights)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # clean up memory\n",
    "        del audio_label, clip, audio_weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        # audio_label=audio_label.to(device)\n",
    "        # clip=clip.to(device)\n",
    "        # audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip)\n",
    "\n",
    "        ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip)\n",
    "\n",
    "        # normalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip).detach()\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # clean up memory\n",
    "        del audio_label, clip, audio_weights, target_pred\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/sagemaker-user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240511_140203-61uicrjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dydifferent/BirdClef-2024/runs/61uicrjf' target=\"_blank\">sef_s21_v1</a></strong> to <a href='https://wandb.ai/dydifferent/BirdClef-2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dydifferent/BirdClef-2024' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dydifferent/BirdClef-2024/runs/61uicrjf' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-2024/runs/61uicrjf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type              | Params\n",
      "-------------------------------------------------------\n",
      "0 | audio_transforms | Compose           | 0     \n",
      "1 | encoder          | Sequential        | 20.2 M\n",
      "2 | fc1              | Linear            | 1.6 M \n",
      "3 | att_block        | AttBlockV2        | 466 K \n",
      "4 | loss_function    | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "20.2 M    Non-trainable params\n",
      "22.3 M    Total params\n",
      "89.134    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb52519efe344f88e542ada453241bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss = nan is not finite. Previous best value was inf. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    logger = WandbLogger(project='BirdClef-2024', name='sef_s21_v1')\n",
    "\n",
    "    # set up checkpoint（ModelCheckpoint）\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',  # monitor val loss\n",
    "        dirpath='models/checkpoints',\n",
    "        filename='sed_s21k_v1-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=1,  # Only save the best model, the one with the lowest validation loss\n",
    "        mode='min',  # Specifying ‘min’ means the smaller the better, e.g. the smaller the loss the better\n",
    "        auto_insert_metric_name=False  # Prevent automatic insertion of index names into path names\n",
    "    )\n",
    "\n",
    "    # EarlyStopping\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00,\n",
    "        patience=3,  # If the validation set loss does not improve within 3 epochs, stop training early\n",
    "        verbose=True,\n",
    "        mode='min'  # 'min' is valid for loss, if you are monitoring indicators such as accuracy, you should use 'max'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Previously we used a separate dataloader to feed the model\n",
    "    # Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "    bdm=BirdclefDatasetModule(sampler=sampler,train_df=train_df,val_df=val_df,bird_category_dir='external_files/3-bird-cates.npy',batch_size=64,workers=8)\n",
    "\n",
    "\n",
    "    class_num=len(np.load('external_files/3-bird-cates.npy',allow_pickle=True))\n",
    "    BirdModelModule=BirdModelModule(class_num=class_num).to(device)\n",
    "\n",
    "\n",
    "    trainer=L.Trainer(\n",
    "        # setup Trainer，Enable mixed precision\n",
    "        precision=16,\n",
    "        # Set up Trainer, use gradient accumulation, and update parameters after accumulating gradients every 128 batches\n",
    "        accumulate_grad_batches=256,\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir='models/model_training',\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        logger=logger, \n",
    "        callbacks=[checkpoint_callback, early_stop_callback], \n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm # DM can automatically find the corresponding dataloader from the object for training without specifying\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
