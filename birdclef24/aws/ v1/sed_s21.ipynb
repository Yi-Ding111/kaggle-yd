{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7605fd-8a24-4f81-8993-4a7c6244d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dash 2.16.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.16.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.16.1 requires dash-table==5.0.0, which is not installed.\n",
      "jupyter-ai 2.12.0 requires faiss-cpu, which is not installed.\n",
      "autogluon-multimodal 0.8.2 requires torch<2.1,>=1.13, but you have torch 2.3.0 which is incompatible.\n",
      "autogluon-timeseries 0.8.2 requires torch<2.1,>=1.13, but you have torch 2.3.0 which is incompatible.\n",
      "fastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "tensorflow 2.12.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mProcessing ./packages/lightning-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (6.0.1)\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (2023.6.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (0.11.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (1.26.4)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (23.2)\n",
      "Requirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (2.3.0)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (1.0.3)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (4.11.0)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.0) (2.0.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (3.9.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning==2.2.0) (69.5.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (3.13.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.13.0->lightning==2.2.0) (12.4.127)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning==2.2.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning==2.2.0) (1.3.0)\n",
      "Installing collected packages: lightning\n",
      "Successfully installed lightning-2.2.0\n",
      "Processing ./packages/colorednoise-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from colorednoise==2.2.0) (1.26.4)\n",
      "Installing collected packages: colorednoise\n",
      "Successfully installed colorednoise-2.2.0\n",
      "Processing ./packages/librosa-0.10.2-py3-none-any.whl\n",
      "Collecting audioread>=2.1.9 (from librosa==0.10.2)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (1.4.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (0.59.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa==0.10.2)\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Collecting pooch>=1.1 (from librosa==0.10.2)\n",
      "  Using cached pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa==0.10.2)\n",
      "  Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (4.11.0)\n",
      "Collecting lazy-loader>=0.1 (from librosa==0.10.2)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.10.2) (1.0.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa==0.10.2) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa==0.10.2) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa==0.10.2) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa==0.10.2) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa==0.10.2) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa==0.10.2) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa==0.10.2) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa==0.10.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa==0.10.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa==0.10.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa==0.10.2) (2024.2.2)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: soxr, lazy-loader, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 lazy-loader-0.4 librosa-0.10.2 pooch-1.8.1 soundfile-0.12.1 soxr-0.3.7\n",
      "Processing ./packages/torch_audiomentations-0.11.1-py3-none-any.whl\n",
      "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations==0.11.1)\n",
      "  Using cached julius-0.2.7-py3-none-any.whl\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from torch-audiomentations==0.11.1) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from torch-audiomentations==0.11.1) (2.3.0)\n",
      "Requirement already satisfied: torchaudio>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from torch-audiomentations==0.11.1) (2.3.0)\n",
      "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations==0.11.1)\n",
      "  Using cached torch_pitch_shift-1.2.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (1.4.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (0.59.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (1.8.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations==0.11.1) (1.0.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (3.13.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->torch-audiomentations==0.11.1) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->torch-audiomentations==0.11.1) (12.4.127)\n",
      "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations==0.11.1)\n",
      "  Using cached primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations==0.11.1) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations==0.11.1) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa>=0.6.0->torch-audiomentations==0.11.1) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.0->torch-audiomentations==0.11.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.0->torch-audiomentations==0.11.1) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.6.0->torch-audiomentations==0.11.1) (2024.2.2)\n",
      "Using cached torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
      "Using cached primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Installing collected packages: primePy, julius, torch-pitch-shift, torch-audiomentations\n",
      "Successfully installed julius-0.2.7 primePy-1.3 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4\n",
      "Processing ./packages/torchinfo-1.8.0-py3-none-any.whl\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q packages/torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl\n",
    "!pip install packages/lightning-2.2.0-py3-none-any.whl\n",
    "!pip install packages/colorednoise-2.2.0-py3-none-any.whl\n",
    "!pip install packages/librosa-0.10.2-py3-none-any.whl\n",
    "!pip install packages/torch_audiomentations-0.11.1-py3-none-any.whl\n",
    "!pip install packages/torchinfo-1.8.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75392a3-8fe8-4412-b90d-155c6b613215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.8)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.1.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.5.1)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Using cached wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
      "Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Installing collected packages: appdirs, setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed appdirs-1.4.4 docker-pycreds-0.4.0 sentry-sdk-2.1.1 setproctitle-1.3.3 wandb-0.16.6\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b15717c-531e-499d-9c6b-dcedc83e762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-05-07 07:52:13.934234: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-07 07:52:13.983470: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793cc2a9-2de1-4a6d-b1df-48d3c74c0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df79c82c-21d3-4d17-b997-ca391c90bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Number of CUDA devices: 1\n",
      "CUDA device 0 cores: 40\n"
     ]
    }
   ],
   "source": [
    "# check cuda and select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "if device == \"cuda\":\n",
    "    # get the num of devices\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_devices)\n",
    "\n",
    "    # Iterate over each CUDA device and print its core count\n",
    "    for i in range(num_devices):\n",
    "        print(\"CUDA device\", i, \"cores:\", torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2704fa8-515c-4864-9062-f1b4c1c96516",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407c4a2b-23a1-40c7-95d8-97130b17587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# initilize tran and val sets\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# random select a sample into val set and other part into train set.\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# split dataset based off index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55955f5-f743-4d3b-abe5-8aefc1430da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217556, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43a15bc-f80d-4236-acba-8526418efbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53744b0a-9323-4c07-a578-1f2c0402e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 20000 pieces data from trainset\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# add these selected data into valset\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# drop these data out of trainset\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab923c4-2e9c-49aa-a856-b40952b2524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197556, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fcbc8f-7c84-49f7-b802-0f6bc72f3be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20182, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ccfceae-58f8-46b7-99a5-5cdd3c4ecefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to interpolate missing values ​​for ratings in metadata csv files\n",
    "\n",
    "def rating_value_interplote(df:pd.DataFrame):\n",
    "    '''\n",
    "    interplote Nan values for rating col in metadata csv \n",
    "\n",
    "    parameters:\n",
    "        df: the df of the metadata csv file\n",
    "\n",
    "    rating col means the quality of the corresponding audio file\n",
    "        5 is high quality\n",
    "        1 is low quality\n",
    "        0 is without defined quality level\n",
    "    '''\n",
    "\n",
    "    if df['rating'].isna().sum()>0: # having missing values\n",
    "        df['rating'].fillna(0, inplace=True)\n",
    "\n",
    "    # Randomly assign a value to all places where the value is 0, choosing from the specified choices\n",
    "    mask = df['rating'] == 0  # create a bool mask, indicate which positions are 0\n",
    "\n",
    "    choices=np.arange(0.5,5.1,0.5).tolist() # [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "    random_values = np.random.choice(choices, size=mask.sum())  # Generate random numbers for these 0 values \n",
    "    df.loc[mask, 'rating'] = random_values  # Fill the generated random numbers back into the corresponding positions of the original DataFrame\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e313e4c-79fc-41d9-b53f-e298192d6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the weight of each audio file by rating helps model training\n",
    "def audio_weight(df):\n",
    "    '''\n",
    "    calculate the weight corresponding to each audio file through the rating value\n",
    "\n",
    "    Because each audio has different quality level, we use weight to affect the inportance of each audio in models,\n",
    "    the lower the quality of the audio, the lower the weight\n",
    "    '''\n",
    "    # Through rating, we calculate the credibility of each audio and express it through weight.\n",
    "    # The purpose of this is to improve the model by increasing the weight of high-quality audio and reducing the weight of low-quality audio.\n",
    "    df[\"audio_weight\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23358165-7e83-41f2-9204-b7bd7a43f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this is an unbalanced dataset, the amount of data in each category is very different\n",
    "# So I will calculate the weight of each category here\n",
    "# **(-0.5) The purpose is to reduce the relative influence of high-frequency categories and increase the influence of low-frequency categories, so as to help the model better learn those uncommon categories\n",
    "# The purpose of calculating this is to build a WeightedRandomSampler, so that each time a batch is extracted using dataloader, it is more friendly to data of different categories.\n",
    "\n",
    "def sampling_weight(df)->torch.Tensor:\n",
    "    '''\n",
    "    calculate the sampling weight of each audio file\n",
    "\n",
    "    because this is imbalanced dataset\n",
    "    we hope the category with less data has large probability to be picked.\n",
    "    '''\n",
    "    sample_weights = (df['primary_label'].value_counts() / df['primary_label'].value_counts().sum()) ** (-0.5)\n",
    "\n",
    "    # Map weights to each row of the original data\n",
    "    sample_weights_map = df['primary_label'].map(sample_weights)\n",
    "\n",
    "    # Convert a pandas Series to a NumPy array\n",
    "    sample_weights_np = sample_weights_map.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors using torch.from_numpy\n",
    "    sample_weights_tensor = torch.from_numpy(sample_weights_np)\n",
    "\n",
    "    return sample_weights_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dba5e15-d9c0-409a-bde6-f97f2318fd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x7f8c015979d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_csv(metadata_path,header=0)\n",
    "sample_weights_tensor=sampling_weight(df=train_df)\n",
    "# Here we will build an argument sampler that dataloader will use\n",
    "# It should be noted that the order of weights in the constructed sampler needs to be consistent with the order of data passed into the dataloader, otherwise the weights will not match\n",
    "\n",
    "#Create a sampler based on the newly obtained weight list\n",
    "sampler = WeightedRandomSampler(sample_weights_tensor.type('torch.DoubleTensor'), len(sample_weights_tensor),replacement=True)\n",
    "\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fecb5cb-9dc9-4e4f-85b2-6ba0e43108f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_from_s3(bucket_name: str, key: str):\n",
    "    \"\"\"\n",
    "    Read an OGG file from an S3 bucket using torchaudio and return the waveform tensor and sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name: Name of the S3 bucket\n",
    "        key: Key of the file in the S3 bucket\n",
    "\n",
    "    Returns:\n",
    "        waveform: Tensor representing the waveform\n",
    "        sample_rate: Sample rate of the audio file\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        audio_data = obj['Body'].read()\n",
    "        audio_stream = BytesIO(audio_data)\n",
    "        audio, sample_rate = torchaudio.load(audio_stream)\n",
    "        return audio, sample_rate\n",
    "    except ClientError as e:\n",
    "        print(\"Error reading audio file from S3:\", e)\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d21c134-f19e-401b-b72a-88c04e9dfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CustomCompose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class CustomOneOf:\n",
    "    def __init__(self, transforms: list, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if np.random.rand() < self.p:\n",
    "            n_trns = len(self.transforms)\n",
    "            trns_idx = np.random.choice(n_trns)\n",
    "            trns = self.transforms[trns_idx]\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GaussianNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=40.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class VolumeControl(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode=\"uniform\"):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        assert mode in [\n",
    "            \"uniform\",\n",
    "            \"fade\",\n",
    "            \"fade\",\n",
    "            \"cosine\",\n",
    "            \"sine\",\n",
    "        ], \"`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'\"\n",
    "\n",
    "        self.db_limit = db_limit\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.db_limit, self.db_limit)\n",
    "        if self.mode == \"uniform\":\n",
    "            db_translated = 10 ** (db / 20)\n",
    "        elif self.mode == \"fade\":\n",
    "            lin = np.arange(len(y))[::-1] / (len(y) - 1)\n",
    "            db_translated = 10 ** (db * lin / 20)\n",
    "        elif self.mode == \"cosine\":\n",
    "            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * cosine / 20)\n",
    "        else:\n",
    "            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * sine / 20)\n",
    "        augmented = y * db_translated\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class NoiseInjection(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_level = (0.0, max_noise_level)\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_level = np.random.uniform(*self.noise_level)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_level).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class GaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise**2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y**2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise**2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_rate = max_rate\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "def _db2float(db: float, amplitude=True):\n",
    "    if amplitude:\n",
    "        return 10 ** (db / 20)\n",
    "    else:\n",
    "        return 10 ** (db / 10)\n",
    "\n",
    "\n",
    "def volume_down(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for decreasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to decrease\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with decreased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(-db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "def volume_up(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for increasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to increase\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with increased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "class RandomVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        if db >= 0:\n",
    "            return volume_up(y, db)\n",
    "        else:\n",
    "            return volume_down(y, db)\n",
    "\n",
    "\n",
    "class CosineVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "        dbs = _db2float(cosine * db)\n",
    "        return y * dbs\n",
    "\n",
    "\n",
    "class AddGaussianNoise(AudioTransform):\n",
    "    \"\"\"Add gaussian noise to the samples\"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self, always_apply=False, min_amplitude=0.001, max_amplitude=0.015, p=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_amplitude: Minimum noise amplification factor\n",
    "        :param max_amplitude: Maximum noise amplification factor\n",
    "        :param p:\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        assert min_amplitude > 0.0\n",
    "        assert max_amplitude > 0.0\n",
    "        assert max_amplitude >= min_amplitude\n",
    "        self.min_amplitude = min_amplitude\n",
    "        self.max_amplitude = max_amplitude\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        amplitude = np.random.uniform(self.min_amplitude, self.max_amplitude)\n",
    "        noise = np.random.randn(*samples.shape).astype(np.float32)\n",
    "        samples = samples + amplitude * noise\n",
    "        return samples\n",
    "\n",
    "\n",
    "class AddGaussianSNR(AudioTransform):\n",
    "    \"\"\"\n",
    "    Add gaussian noise to the input. A random Signal to Noise Ratio (SNR) will be picked\n",
    "    uniformly in the decibel scale. This aligns with human hearing, which is more\n",
    "    logarithmic than linear.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        always_apply=False,\n",
    "        min_snr_in_db: float = 5.0,\n",
    "        max_snr_in_db: float = 40.0,\n",
    "        p: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param min_snr_in_db: Minimum signal-to-noise ratio in dB. A lower number means more noise.\n",
    "        :param max_snr_in_db: Maximum signal-to-noise ratio in dB. A greater number means less noise.\n",
    "        :param p: The probability of applying this transform\n",
    "        \"\"\"\n",
    "        super().__init__(always_apply, p)\n",
    "        self.min_snr_in_db = min_snr_in_db\n",
    "        self.max_snr_in_db = max_snr_in_db\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        snr = np.random.uniform(self.min_snr_in_db, self.max_snr_in_db)\n",
    "\n",
    "        clean_rms = np.sqrt(np.mean(np.square(samples)))\n",
    "\n",
    "        a = float(snr) / 20\n",
    "        noise_rms = clean_rms / (10**a)\n",
    "\n",
    "        noise = np.random.normal(0.0, noise_rms, size=samples.shape).astype(np.float32)\n",
    "        return samples + noise\n",
    "\n",
    "\n",
    "class Normalize(AudioTransform):\n",
    "    \"\"\"\n",
    "    Apply a constant amount of gain, so that highest signal level present in the sound becomes\n",
    "    0 dBFS, i.e. the loudest level allowed if all samples must be between -1 and 1. Also known\n",
    "    as peak normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    supports_multichannel = True\n",
    "\n",
    "    def __init__(self, always_apply=False, apply_to: str = \"all\", p: float = 0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        assert apply_to in (\"all\", \"only_too_loud_sounds\")\n",
    "        self.apply_to = apply_to\n",
    "\n",
    "    def apply(self, samples: np.ndarray, sample_rate=32000):\n",
    "        max_amplitude = np.amax(np.abs(samples))\n",
    "        if self.apply_to == \"only_too_loud_sounds\" and max_amplitude < 1.0:\n",
    "            return samples\n",
    "\n",
    "        if max_amplitude > 0:\n",
    "            return samples / max_amplitude\n",
    "        else:\n",
    "            return samples\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "656ed1a9-8806-445a-8713-2e1875c4801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order of this is very important and needs to be matched one by one in subsequent training, I will save these categories here\n",
    "# save as .npy file\n",
    "np.save(\"external_files/3-bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5edb8a2e-0e84-4eec-a9b4-c8bcbfc6d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdclefDataset(Dataset):\n",
    "    def __init__(self,df:pd.DataFrame,bird_category_dir:str,audio_prefix:str='train_audio',bucket:str='birdclef',train:bool=True):\n",
    "        '''\n",
    "        parameters:\n",
    "            df: the dataframe of metadata (train/val)\n",
    "            bird_category_dir: the directory of the bird category array file (npy)\n",
    "            audio_dir: the parent path where all audio files stored\n",
    "            train: If the Datset for train set or val set\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # if the Dataset for training or validation\n",
    "        self.train=train\n",
    "        self.raw_df=df\n",
    "\n",
    "        # inperplote nan or 0 value of rating col\n",
    "        self.raw_df=rating_value_interplote(df=self.raw_df)\n",
    "        # calculate each audio file's weight through feature: rating\n",
    "        self.raw_df=audio_weight(self.raw_df)\n",
    "\n",
    "        self.audio_prefix=audio_prefix\n",
    "\n",
    "        self.bucket=bucket\n",
    "\n",
    "        self.bird_cate_array=np.load(bird_category_dir,allow_pickle=True)\n",
    "\n",
    "    def get_audio_path(self,file_name:str) -> str:\n",
    "        '''\n",
    "        grab the audio file path corresponding to the variable: index in the provided train metadata csv file\n",
    "        this func would only return one path, because only provide one index\n",
    "\n",
    "        Parameters:\n",
    "            file_name: in format category_type/XC-ID.ogg (asbfly/XC134896.ogg)\n",
    "\n",
    "        Return:\n",
    "            the single audio path string\n",
    "        '''\n",
    "\n",
    "        # concatenate parent path and child path\n",
    "        return os.path.join(self.audio_prefix,file_name)\n",
    "\n",
    "\n",
    "    def target_clip(self,index:int,audio:torch.Tensor,sample_rate:int)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the index corresponding audio clip \n",
    "\n",
    "        information from the train metadata csv\n",
    "\n",
    "        Parameters:\n",
    "            audio: the raw audio in tensor [num_channels,length]\n",
    "            sample_rate: audio sampling rate\n",
    "        \"\"\"\n",
    "        # get the strat time of audio clip based off index\n",
    "        clip_start_time=self.raw_df['clip_start_time'].iloc[index]\n",
    "        duration_seconds=self.raw_df['duration'].iloc[index]\n",
    "\n",
    "        # define clip length\n",
    "        segment_duration = 5 * sample_rate\n",
    "\n",
    "        # Total number of samples in the waveform\n",
    "        total_samples = audio.shape[1]\n",
    "\n",
    "        if clip_start_time<=duration_seconds:\n",
    "            clip_start_point=clip_start_time*sample_rate\n",
    "            # For the last clip, the original audio may not be long enough, so we need to use a mask to fill the sequence.\n",
    "            # The first step is to confirm whether the clip duration is sufficient\n",
    "            # do not need add mask if the clip duration is enough\n",
    "            if clip_start_point+segment_duration<=total_samples:\n",
    "                clip=audio[:, clip_start_point:clip_start_point + segment_duration]\n",
    "\n",
    "            # add mask if not enough\n",
    "            else:\n",
    "                padding_length = clip_start_point+segment_duration - total_samples\n",
    "                silence = torch.zeros(audio.shape[0], padding_length)\n",
    "                # concat the last part of the raw audio with silence clip\n",
    "                clip=torch.cat((audio[:,clip_start_point:],silence),dim=1)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError('The clip start time is out of raw audio length')\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "    def random_audio_augmentation(self,audio:torch.Tensor):\n",
    "        '''\n",
    "        audio (torch.Tensor): A 2D tensor of audio samples with shape (1, N), where N is the number of samples.\n",
    "        '''\n",
    "        np_audio_transforms = CustomCompose(\n",
    "            [\n",
    "                CustomOneOf(\n",
    "                    [\n",
    "                        NoiseInjection(p=1, max_noise_level=0.04),\n",
    "                        GaussianNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        PinkNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        AddGaussianNoise(min_amplitude=0.0001, max_amplitude=0.03, p=0.5),\n",
    "                        AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=15, p=0.5),\n",
    "                    ],\n",
    "                    p=0.3,  \n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        audio_aug=np_audio_transforms(audio[0].numpy())\n",
    "\n",
    "        # tranfer the array to 2D tensor and keep the num channel is 1\n",
    "        # this step is to keep the input and output shape adn type are the same\n",
    "\n",
    "        audio_aug_tensor=torch.from_numpy(audio_aug)\n",
    "        audio_aug_tensor=audio_aug_tensor.unsqueeze(0)\n",
    "\n",
    "        return audio_aug_tensor\n",
    "    \n",
    "\n",
    "    def audio_label_tensor_generator(self,true_label:str)-> torch.Tensor:\n",
    "        '''\n",
    "        Generate a tensor containing all categories based on the given real audio label\n",
    "\n",
    "        Parameters:\n",
    "            true lable: a label string\n",
    "\n",
    "        Return:\n",
    "            If have 10 class, and give a true lable\n",
    "            the return should be tensor([0,1,0,0,0,0,0,0,0,0])\n",
    "        '''\n",
    "        # find the target index in the array\n",
    "        idx = np.where(self.bird_cate_array == true_label)[0][0]\n",
    "        \n",
    "        # create a zero tensor, the tensor length equals to the array\n",
    "        audio_label_tensor = torch.zeros(len(self.bird_cate_array))\n",
    "\n",
    "        # set the corresponding index value as 1\n",
    "        audio_label_tensor[idx] = 1\n",
    "\n",
    "        return audio_label_tensor\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.raw_df.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        row=self.raw_df.iloc[index]\n",
    "\n",
    "        audio_label=row['primary_label']\n",
    "        audio_weight=row['audio_weight']\n",
    "\n",
    "        # grab the single audio file path\n",
    "        single_audio_dir=self.get_audio_path(row['filename'])\n",
    "\n",
    "        # read audio array based off the path\n",
    "        audio, sr=read_audio_from_s3(bucket_name=self.bucket, key=single_audio_dir)\n",
    "\n",
    "        # augmentation\n",
    "        # only used for train df\n",
    "        # do not do augmentation if for validation set\n",
    "        if self.train:\n",
    "            audio_augmentation=self.random_audio_augmentation(audio=audio)\n",
    "            # get the corresponding audio clip based off the index\n",
    "            clip=self.target_clip(index,audio=audio_augmentation,sample_rate=sr)\n",
    "        else:\n",
    "            clip=self.target_clip(index,audio=audio,sample_rate=sr)\n",
    "\n",
    "        # change audio label to one-hot tensor\n",
    "        audio_label_tensor=self.audio_label_tensor_generator(true_label=audio_label)\n",
    "\n",
    "        audio_label_tensor=torch.tensor(audio_label_tensor, dtype=torch.float32)\n",
    "        clip=torch.tensor(clip, dtype=torch.float32)\n",
    "        audio_weight=torch.tensor(audio_weight, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        return audio_label_tensor.to(device),clip.to(device),audio_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "338efa57-0fb6-4010-935d-19709fb8c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self,sampler,train_df:pd.DataFrame,val_df:pd.DataFrame,bird_category_dir:str,audio_prefix: str = 'train_audio',batch_size:int=128):\n",
    "        super().__init__()\n",
    "        self.train_df=train_df\n",
    "        self.val_df=val_df\n",
    "        self.bird_category_dir=bird_category_dir\n",
    "        self.audio_prefix=audio_prefix\n",
    "        self.batch_size=batch_size\n",
    "        self.sampler=sampler\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.train_df,bird_category_dir=self.bird_category_dir,audio_prefix=self.audio_prefix,train=True)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, sampler=self.sampler, pin_memory=False)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD=BirdclefDataset(df=self.val_df,bird_category_dir=self.bird_category_dir,audio_prefix=self.audio_prefix,train=False)\n",
    "        loader = DataLoader(dataset=BD, batch_size=self.batch_size, pin_memory=False)\n",
    "\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68c5c802-2897-4add-ad79-d3893bc98904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup_prob, mixup_double):\n",
    "        super(Mixup, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.mixup_double = mixup_double\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0] # Generate a random number p and compare it with mixup_prob to decide whether to mix.\n",
    "        if p < self.mixup_prob:\n",
    "            bs = X.shape[0] # batch size\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs) # generate a random permutation, for selecting samples from the current batch to mix randomly.\n",
    "\n",
    "            p1 = torch.rand((1,))[0] # If the random number p1 (determines whether to perform double mixing) is less than mixup_double, perform a single mix. Otherwise, perform double mixing:\n",
    "            if p1 < self.mixup_double:\n",
    "                X = X + X[perm]\n",
    "                Y = Y + Y[perm]\n",
    "                Y = torch.clamp(Y, 0, 1) \n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = 0.5 * weight + 0.5 * weight[perm]\n",
    "                    return X, Y, weight\n",
    "            else:\n",
    "                perm2 = torch.randperm(bs)\n",
    "                X = X + X[perm] + X[perm2]\n",
    "                Y = Y + Y[perm] + Y[perm2]\n",
    "                Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "                if weight is None:\n",
    "                    return X, Y\n",
    "                else:\n",
    "                    weight = (\n",
    "                        1 / 3 * weight + 1 / 3 * weight[perm] + 1 / 3 * weight[perm2]\n",
    "                    )\n",
    "                    return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bef0938-3ed1-4c91-9841-80ac45f69ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_transform(sample_rate:float,audio:torch.Tensor,window_size: float=0.04,hop_size:float=0.02,n_mels:int=40)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    transform audio data into mel sepctrogram\n",
    "    \"\"\"\n",
    "    # Determine window size and frame shift\n",
    "    # window_size = 0.04 # 40 milliseconds\n",
    "    # hop_size = 0.02 # 20 milliseconds, usually half the window size\n",
    "    n_fft = int(window_size * sample_rate)  # Convert window size to number of sampling points\n",
    "    hop_length = int(hop_size * sample_rate)  # Convert frame shift to sampling point number\n",
    "\n",
    "    # Calculate Mel spectrum\n",
    "    # n_mels = 40 # The number of Mel filters\n",
    "\n",
    "    # Set up the Mel Spectrogram converter\n",
    "    mel_transformer = MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=0,\n",
    "        f_max=16000\n",
    "    ).to(device)\n",
    "\n",
    "    melspec=mel_transformer(audio)\n",
    "\n",
    "    return melspec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f9ba7f0-b369-4f4f-bc25-882ce7b3e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltas(specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\") -> torch.Tensor:\n",
    "    \"\"\"Compute delta coefficients of a tensor, usually a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        specgram (Tensor): Tensor of audio of dimension (..., freq, time)\n",
    "        win_length (int, optional): The window length used for computing delta (Default: 5)\n",
    "        mode (str, optional): Mode parameter passed to padding (Default: \"replicate\")\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of deltas of dimension (..., freq, time)\n",
    "    \"\"\"\n",
    "    device = specgram.device\n",
    "    dtype = specgram.dtype\n",
    "\n",
    "    # pack batch\n",
    "    shape = specgram.size()\n",
    "    specgram = specgram.reshape(1, -1, shape[-1])\n",
    "\n",
    "    assert win_length >= 3\n",
    "    n = (win_length - 1) // 2\n",
    "    denom = n * (n + 1) * (2 * n + 1) / 3\n",
    "\n",
    "    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n",
    "\n",
    "    # Create the kernel tensor, making sure it is on the same device as the input tensor\n",
    "    kernel = torch.arange(-n, n + 1, 1, dtype=dtype,device=device).repeat(specgram.shape[1], 1, 1)\n",
    "\n",
    "    output = (\n",
    "        torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n",
    "    )\n",
    "\n",
    "    # unpack batch\n",
    "    output = output.reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def make_delta(input_tensor: torch.Tensor):\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    input_tensor = compute_deltas(input_tensor)\n",
    "    input_tensor = input_tensor.transpose(3, 2)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "def image_delta(x):\n",
    "    delta_1 = make_delta(x)\n",
    "    delta_2 = make_delta(delta_1)\n",
    "    x = torch.cat([x, delta_1, delta_2], dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4408462c-4cf6-4953-a6e5-3d4c3883629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup2(nn.Module):\n",
    "    def __init__(self, mix_beta, mixup2_prob):\n",
    "        super(Mixup2, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "        self.mixup2_prob = mixup2_prob\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "        p = torch.rand((1,))[0]\n",
    "        if p < self.mixup2_prob:\n",
    "            bs = X.shape[0]\n",
    "            n_dims = len(X.shape)\n",
    "            perm = torch.randperm(bs)\n",
    "            coeffs = self.beta_distribution.rsample(torch.Size((bs,))).to(device)\n",
    "\n",
    "            if n_dims == 2:\n",
    "                X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n",
    "            elif n_dims == 3:\n",
    "                X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n",
    "            else:\n",
    "                X = (\n",
    "                    coeffs.view(-1, 1, 1, 1) * X\n",
    "                    + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n",
    "                )\n",
    "            Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n",
    "            # Y = Y + Y[perm]\n",
    "            # Y = torch.clamp(Y, 0, 1)\n",
    "\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n",
    "                return X, Y, weight\n",
    "        else:\n",
    "            if weight is None:\n",
    "                return X, Y\n",
    "            else:\n",
    "                return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32dde0d6-436a-40f2-b14d-5f7677f47286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    '''\n",
    "    initilize dense layer parameters\n",
    "    '''\n",
    "    nn.init.xavier_uniform_(layer.weight) # initilize net layers weight and bias\n",
    "\n",
    "    if hasattr(layer, \"bias\"): # check if layer has bias value\n",
    "        if layer.bias is not None: # and bias is not none\n",
    "            layer.bias.data.fill_(0.0) # if existing bias, set as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2adbbf42-c32b-4558-b723-f4c0cc811532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we want to put the high dimentional fetures grabbed into a attention block\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "    \n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c439dcb-e15e-4d17-9738-48a61484341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(self,sample_rate:int=32000,pretrained_model_name:str='tf_efficientnetv2_s_in21k',class_num:int=182):\n",
    "        super().__init__()\n",
    "        self.sample_rate=sample_rate\n",
    "        self.class_num=class_num\n",
    "\n",
    "        self.audio_transforms = Compose(\n",
    "            [\n",
    "                # AddColoredNoise(p=0.5),\n",
    "                PitchShift(\n",
    "                    min_transpose_semitones=-4,\n",
    "                    max_transpose_semitones=4,\n",
    "                    sample_rate=32000,\n",
    "                    p=0.4,\n",
    "                ),\n",
    "                Shift(min_shift=-0.5, max_shift=0.5, p=0.4),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # load pretrained model\n",
    "        pretrained_model = timm.create_model(pretrained_model_name, pretrained=True,in_chans=3)\n",
    "\n",
    "        # The last two layers are an adaptive pooling layer and a fully connected layer.\n",
    "        # Here I choose to replace these two layers. First remove these two layers\n",
    "        layers = list(pretrained_model.children())[:-2]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers).to(device) \n",
    "\n",
    "        self.in_features=pretrained_model.classifier.in_features # classifier is the last fully connected layer of the model, out_features represents the number of categories\n",
    "\n",
    "        # create a fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=self.in_features, out_features=self.in_features, bias=True).to(device)\n",
    "\n",
    "        # add attention block\n",
    "        self.att_block=AttBlockV2(in_features=self.in_features, out_features=self.class_num, activation=\"sigmoid\").to(device)\n",
    "\n",
    "        # Initialize the weights and biases of the fully connected layer\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "        # freeze part parameters\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.encoder.eval()\n",
    "        # self.fc1.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for param in self.fc1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,clip):\n",
    "\n",
    "        # use pre-trained model (exclude the last two layers) for computation\n",
    "        clip=self.encoder(clip.to(device)) # feature extractor\n",
    "\n",
    "        # Calculate the mean of each frequency band and merge them Dimensionality compression\n",
    "        clip = torch.mean(clip, dim=2)\n",
    "\n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(clip, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.relu_(self.fc1(x))\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=True)\n",
    "\n",
    "        target_pred, norm_att, segmentwise_output = self.att_block(x)\n",
    "\n",
    "        \n",
    "        return target_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "\n",
    "\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "    \n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # mixup audio\n",
    "        mixup = Mixup(mix_beta=5,mixup_prob=0.7,mixup_double=0.5)\n",
    "\n",
    "        clip, audio_label,audio_weights=mixup(X=clip,Y=audio_label,weight=audio_weights)\n",
    "\n",
    "        # Use Compose to combine multiple audio transformation operations. \n",
    "        # These operations are applied to the input audio data to improve the generalization and robustness of the model.\n",
    "        clip=self.audio_transforms(clip,sample_rate=self.sample_rate)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        #Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        # normalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Randomly masking part of the spectrogram helps the model learn to be robust to missing information in certain time periods.\n",
    "\n",
    "        time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=20, iid_masks=True, p=0.3)\n",
    "\n",
    "        clip = time_mask_transform(clip)\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # audio mix up\n",
    "        mixup2 = Mixup2(mix_beta=2, mixup2_prob=0.15)\n",
    "\n",
    "        clip, audio_label,audio_weights = mixup2(clip, audio_label, audio_weights)\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        audio_label=batch[0]\n",
    "        clip=batch[1]\n",
    "        audio_weights=batch[2]\n",
    "\n",
    "        audio_label=audio_label.to(device)\n",
    "        clip=clip.to(device)\n",
    "        audio_weights=audio_weights.to(device)\n",
    "\n",
    "        # Convert audio data into mel spectrogram\n",
    "        clip=mel_transform(sample_rate=self.sample_rate,audio=clip).to(device)\n",
    "\n",
    "        ##Convert the amplitude of Mel Spectrogram to decibel (dB)\n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "        clip=db_transform(clip).to(device)\n",
    "\n",
    "        # normalization\n",
    "        clip=(clip+80)/80\n",
    "\n",
    "        # Calculate the first and second order differences of audio or other time series data, usually called delta and delta-delta (also called acceleration) features.\n",
    "        clip= image_delta(clip.to(device))\n",
    "\n",
    "        # predictions\n",
    "        target_pred=self(clip.to(device))\n",
    "\n",
    "        loss = self.loss_function(torch.logit(target_pred), audio_label)\n",
    "\n",
    "        loss = loss.sum(dim=1) * audio_weights\n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a576888d-703b-42ec-a185-637e6752ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpoint（ModelCheckpoint）\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # monitor val loss\n",
    "    dirpath='models/checkpoints',\n",
    "    filename='sed_s21-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,  # Only save the best model, the one with the lowest validation loss\n",
    "    mode='min',  # Specifying ‘min’ means the smaller the better, e.g. the smaller the loss the better\n",
    "    auto_insert_metric_name=False  # Prevent automatic insertion of index names into path names\n",
    ")\n",
    "\n",
    "# EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    min_delta=0.00,\n",
    "    patience=5,   # If the validation set loss does not improve within 5 epochs, stop training early\n",
    "    verbose=True,\n",
    "    mode='min'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a1f4a33-d9ad-413a-8fb9-f86b159378bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(project='BirdClef-2024', name='sef_s21_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "243d915f-d84f-4834-8711-22a41b1b74da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdydifferent\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240507_075231-anliameg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dydifferent/BirdClef-2024/runs/anliameg' target=\"_blank\">sef_s21_v1</a></strong> to <a href='https://wandb.ai/dydifferent/BirdClef-2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dydifferent/BirdClef-2024' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dydifferent/BirdClef-2024/runs/anliameg' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-2024/runs/anliameg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type              | Params\n",
      "-------------------------------------------------------\n",
      "0 | audio_transforms | Compose           | 0     \n",
      "1 | encoder          | Sequential        | 20.2 M\n",
      "2 | fc1              | Linear            | 1.6 M \n",
      "3 | att_block        | AttBlockV2        | 466 K \n",
      "4 | loss_function    | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "20.2 M    Non-trainable params\n",
      "22.3 M    Total params\n",
      "89.134    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe0da637b541859e86b1ea74666a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Previously we used a separate dataloader to feed the model\n",
    "# Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "bdm=BirdclefDatasetModule(sampler=sampler,train_df=train_df,val_df=val_df,bird_category_dir='external_files/3-bird-cates.npy',batch_size=1024)\n",
    "\n",
    "\n",
    "class_num=len(np.load('external_files/3-bird-cates.npy',allow_pickle=True))\n",
    "BirdModelModule=BirdModelModule(class_num=class_num).to(device)\n",
    "\n",
    "\n",
    "trainer=L.Trainer(\n",
    "    max_epochs=45,\n",
    "    accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "    devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "    default_root_dir='models/model_training',\n",
    "    # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],  \n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    model=BirdModelModule,\n",
    "    datamodule=bdm \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b6b6b-e843-4888-a575-dedac374e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327477a-9ad0-40b1-a380-a46cca841835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfdd24-ce0c-4570-a865-7b0b6d59d5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc608e-7453-4191-ac93-86348b7aee32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
