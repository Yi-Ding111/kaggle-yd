{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q packages/torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl\n",
    "!pip install -q packages/lightning-2.2.0-py3-none-any.whl\n",
    "!pip install -q packages/colorednoise-2.2.0-py3-none-any.whl\n",
    "!pip install -q packages/librosa-0.10.2-py3-none-any.whl\n",
    "!pip install -q packages/torch_audiomentations-0.11.1-py3-none-any.whl\n",
    "!pip install -q packages/torchinfo-1.8.0-py3-none-any.whl\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-05-15 06:11:29.374098: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-15 06:11:29.423499: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import colorednoise as cn\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from torch.distributions import Beta\n",
    "from torch_audiomentations import Compose, PitchShift, Shift, OneOf, AddColoredNoise\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    ")\n",
    "from lightning.pytorch.callbacks  import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.audioprocess import rating_value_interplote, audio_weight, sampling_weight, dataloader_sampler_generate,class_weight_generate\n",
    "from common.audiotransform import read_audio, Mixup, mel_transform,image_delta, Mixup2\n",
    "from common.audiotransform import CustomCompose,CustomOneOf,NoiseInjection,GaussianNoise,PinkNoise,AddGaussianNoise,AddGaussianSNR\n",
    "from common.audiodatasets import BirdclefDataset\n",
    "from common.audiodatasets import trainloader_collate,valloader_collate\n",
    "from common.modelmeasurements import FocalLoss,compute_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Number of CUDA devices: 1\n",
      "CUDA device 0 cores: 40\n"
     ]
    }
   ],
   "source": [
    "# check cuda and select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "if device == \"cuda\":\n",
    "    # get the num of devices\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_devices)\n",
    "\n",
    "    # Iterate over each CUDA device and print its core count\n",
    "    for i in range(num_devices):\n",
    "        print(\"CUDA device\", i, \"cores:\", torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path='./data/train_metadata_new_add_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a train test split on the data first\n",
    "# Because this dataset is unbalanced\n",
    "# Randomly select a sample from each category to add to the validation set, and the rest to the training set\n",
    "\n",
    "raw_df=pd.read_csv(metadata_path,header=0)\n",
    "\n",
    "# find the index of each category\n",
    "class_indices = raw_df.groupby('primary_label').apply(lambda x: x.index.tolist())\n",
    "\n",
    "# initilize tran and val sets\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "\n",
    "# random select a sample into val set and other part into train set.\n",
    "for indices in class_indices:\n",
    "    val_sample = pd.Series(indices).sample(n=1, random_state=42).tolist()\n",
    "    val_indices.extend(val_sample)\n",
    "    train_indices.extend(set(indices) - set(val_sample))\n",
    "\n",
    "\n",
    "# split dataset based off index\n",
    "train_df = raw_df.loc[train_indices]\n",
    "val_df = raw_df.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 20000 pieces data from trainset\n",
    "additional_val_samples = train_df.sample(n=20000, random_state=42)\n",
    "\n",
    "# add these selected data into valset\n",
    "val_df = pd.concat([val_df, additional_val_samples])\n",
    "\n",
    "# drop these data out of trainset\n",
    "train_df = train_df.drop(additional_val_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataloader sampler\n",
    "\n",
    "train_sampler=dataloader_sampler_generate(df=train_df)\n",
    "val_sampler=dataloader_sampler_generate(df=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get all the types\n",
    "meta_df=pd.read_csv(metadata_path,header=0)\n",
    "bird_cates=meta_df.primary_label.unique()\n",
    "\n",
    "#Because the order of this is very important and needs to be matched one by one in subsequent training, \n",
    "# I will save these categories here\n",
    "# save as .npy file\n",
    "np.save(\"./external_files/bird-cates.npy\", bird_cates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .npy file\n",
    "loaded_array = np.load(\"./external_files/bird-cates.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_class_weights=class_weight_generate(df=train_df,loaded_array=loaded_array)\n",
    "loss_val_class_weights=class_weight_generate(df=val_df,loaded_array=loaded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DatasetModule\n",
    "\n",
    "\n",
    "class BirdclefDatasetModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_sampler,\n",
    "        val_sampler,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        bird_category_dir: str,\n",
    "        audio_dir: str = \"data/audio\",\n",
    "        batch_size: int = 128,\n",
    "        workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.bird_category_dir = bird_category_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_sampler = train_sampler\n",
    "        self.val_sampler = val_sampler\n",
    "        self.workers = workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            df=self.train_df,\n",
    "            bird_category_dir=self.bird_category_dir,\n",
    "            audio_dir=self.audio_dir,\n",
    "            train=True,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.train_sampler,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=trainloader_collate\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        BD = BirdclefDataset(\n",
    "            df=self.val_df,\n",
    "            bird_category_dir=self.bird_category_dir,\n",
    "            audio_dir=self.audio_dir,\n",
    "            train=False,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset=BD,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.val_sampler,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=valloader_collate\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self,class_nums:int=182):\n",
    "        super().__init__()\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=1280, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=128, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru3 = nn.GRU(\n",
    "            input_size=256, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.gru4 = nn.GRU(\n",
    "            input_size=384, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=class_nums)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Because the input shape required by gru is (batch_size, sequence length, feature_size)\n",
    "        # But the result of the previous conversion calculation is (batchsize, feature_size, sequence length)\n",
    "        # I need to change the shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        gru_out1, _ = self.gru1(x)\n",
    "        x1 = self.bn1(gru_out1)\n",
    "        gru_out2, _ = self.gru2(x1)\n",
    "        x2 = self.bn2(gru_out2)\n",
    "        # According to the chrononet architecture, \n",
    "        # we need to connect the calculations of the two layers of GRU according to the feature-size dimension\n",
    "        x3 = torch.cat((x1, x2), dim=2)\n",
    "        gru_out3, _ = self.gru3(x3)\n",
    "        x4 = self.bn3(gru_out3)\n",
    "        x5 = torch.cat((x1, x2, x4), dim=2)\n",
    "        gru_out4, _ = self.gru4(x5)\n",
    "        x6 = self.dropout1(gru_out4[:, -1, :])  #Usually take the final output of GRU\n",
    "        out = self.fc1(x6)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModelModule(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_class_weight: torch.Tensor,\n",
    "        val_class_weight: torch.Tensor,\n",
    "        sample_rate: int = 32000,\n",
    "        class_num: int = 182,\n",
    "        lr: float = 0.001\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model: the defined model module\n",
    "            train_class_weight: the argument is used for Focal Loss Function, focal loss needs a sequence of class weights to calculate the loss\n",
    "            val_class_weight: the argument is also used for Focal loss function, for validation step\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "        self.train_class_weight = train_class_weight.to(device)\n",
    "        self.val_class_weight = val_class_weight.to(device)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.class_num = class_num\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, clips):\n",
    "\n",
    "        return self.model(clips)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # predictions\n",
    "        # target_pred=self(clip.to(device))\n",
    "        target_pred = self(clips)\n",
    "        # print(\"train\", weights.shape)\n",
    "        # initialize loss fn\n",
    "        loss_fn = FocalLoss(weight=self.train_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        # roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"train_roc_auc\",\n",
    "        #     roc_auc,\n",
    "        #     on_step=True,\n",
    "        #     on_epoch=True,\n",
    "        #     prog_bar=True,\n",
    "        #     logger=True,\n",
    "        # )\n",
    "\n",
    "        # # clean up memory\n",
    "        # del labels, clips, weights, target_pred\n",
    "        # if torch.cuda.is_available():\n",
    "        #     torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        clips = batch[0]\n",
    "        labels = batch[1]\n",
    "        weights = batch[2]\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        clips = clips.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        # Use flatten to combine the last two dimensions\n",
    "        clips = torch.flatten(clips, start_dim=2)\n",
    "\n",
    "        # predictions\n",
    "        target_pred = self(clips).detach()\n",
    "\n",
    "        # initialize loss fn\n",
    "        print(\"val\", weights.shape)\n",
    "        loss_fn = FocalLoss(weight=self.val_class_weight, sample_weight=weights)\n",
    "\n",
    "        loss = loss_fn(inputs=target_pred, targets=labels)\n",
    "\n",
    "        # Compute ROC-AUC and log it\n",
    "        roc_auc = compute_roc_auc(preds=target_pred, targets=labels)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"val_roc_auc\",\n",
    "            roc_auc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        # # clean up memory\n",
    "        # del labels, clips, weights, target_pred\n",
    "        # if torch.cuda.is_available():\n",
    "        #     torch.cuda.empty_cache()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.lr,\n",
    "            weight_decay=0.001,\n",
    "        )\n",
    "        interval = \"epoch\"\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": interval,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdydifferent\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240515_061136-wuwjd8z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dydifferent/BirdClef-2024/runs/wuwjd8z3' target=\"_blank\">sef_s21_v2</a></strong> to <a href='https://wandb.ai/dydifferent/BirdClef-2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dydifferent/BirdClef-2024' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dydifferent/BirdClef-2024/runs/wuwjd8z3' target=\"_blank\">https://wandb.ai/dydifferent/BirdClef-2024/runs/wuwjd8z3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | ChronoNet | 1.0 M \n",
      "------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val torch.Size([64])\n",
      "val torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22bafb3c988462daa43d258c19661d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "    # # initilize collate_fn\n",
    "    # valloader_collate=valloader_collate()\n",
    "    # trainloader_collate=trainloader_collate()\n",
    "\n",
    "    logger = WandbLogger(project='BirdClef-2024', name='sef_s21_v2')\n",
    "\n",
    "    # setup checkpoint（ModelCheckpoint）\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",  # monitor val set loss\n",
    "        dirpath=\"models/check\",\n",
    "        filename=\"sed_s21k_v2-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,  # Only save the best model, the one with the lowest validation loss\n",
    "        mode=\"min\",  \n",
    "        auto_insert_metric_name=False, \n",
    "    )\n",
    "\n",
    "    # EarlyStopping\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",  \n",
    "        min_delta=0.00,\n",
    "        patience=3, \n",
    "        verbose=True,\n",
    "        mode=\"min\", \n",
    "    )\n",
    "    # we used a separate dataloader to feed the model Previously\n",
    "    # Here we encapsulate the dataloader and use this class to read data for training\n",
    "\n",
    "    bdm = BirdclefDatasetModule(\n",
    "        train_sampler=train_sampler,\n",
    "        val_sampler=val_sampler,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        bird_category_dir=\"./external_files/bird-cates.npy\",\n",
    "        audio_dir=\"./data/audio\",\n",
    "        batch_size=64,\n",
    "        workers=8,\n",
    "    )\n",
    "\n",
    "    class_num = len(np.load(\"external_files/bird-cates.npy\", allow_pickle=True))\n",
    "    # initilize model\n",
    "    chrononet = ChronoNet(class_nums=class_num)\n",
    "\n",
    "    BirdModelModule = BirdModelModule(\n",
    "        model=chrononet,\n",
    "        train_class_weight=loss_train_class_weights,\n",
    "        val_class_weight=loss_val_class_weights,\n",
    "        class_num=class_num,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        # 设置 Trainer，enable mixed precision\n",
    "        precision=16,\n",
    "        # Set up Trainer, use gradient accumulation, \n",
    "        # and update parameters after accumulating gradients every 512 batches\n",
    "        accumulate_grad_batches=512,\n",
    "        max_epochs=45,\n",
    "        # accelerator=\"auto\", # set to 'auto' or 'gpu' to use gpu if possible\n",
    "        # devices='auto', # use all gpus if applicable like value=1 or \"auto\"\n",
    "        default_root_dir=\"models/model_training\",\n",
    "        # logger=CSVLogger(save_dir='/Users/yiding/personal_projects/ML/github_repo/birdcief/code/model-training/log/',name='chrononet')\n",
    "        logger=logger,  # logger\n",
    "        callbacks=[checkpoint_callback, early_stop_callback], \n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(\n",
    "        model=BirdModelModule,\n",
    "        datamodule=bdm,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
